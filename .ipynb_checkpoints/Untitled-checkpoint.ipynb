{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a2640b-ef37-4379-992c-5396029304a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.readers.file import PDFReader\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3507bbec-353e-497a-af23-b25f433e0e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='21c9d65a-f1b6-4ee0-a588-c0e4ad33775b', embedding=None, metadata={'page_label': '1', 'file_name': 'resume.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Jasdeep Singh Chhabra\\nhttps://jasdeep06.github.io/\\njasdeepchhabr a94@gmail.c om | +91 7 7 54941699\\nEDUCATION\\nIIT VARANASI\\nB TECH(2013- 201 7)\\nCHEMICAL ENGINEERING AND\\nTECHNOLOGY\\nCGP A: 8.40\\nLINKS\\nBlog:// Jasdeep’s Blog\\nGithub:// jasdeep06\\nLinkedIn:// Jasdeep Singh Chhabra\\nCERTIFICATIONS\\nCOURSERA’S DEEP LEARNING\\nSPECIALIZATION\\n•Neural Networks andDeep\\nLearning (100 %grade)\\n•Improving Deep Neural\\nNetworks: Hyperparameter\\ntuning, Regularization and\\nOptimization (100 %grade)\\n•Structuring Machine Learning\\nProjects (100 %grade)\\nOTHER\\n•Coursera’s Machine Learning\\n(Stanford University/Andrew Ng)\\nSKILLS\\nPROGRAMMING LANGUAGES\\nPython •Java•C•\\nAREAS OF INTEREST\\nDeep Learning •Natural Language\\nProcessing •Computer Vision •\\nLIBRARIES AND DATABASES\\nTensorﬂow •Numpy •Pandas\\n•Django •scikit-learn •Neo4j •\\nMongoDB• ﬂask•pymongo •\\nSpacy •MATLAB •EXPERIENCE\\nUDACITY | SELF DRIVING CAR NANODEGREE\\nDec 201 7 – Presen t\\n•Pursuing selfdriving carnanodegree.\\nTEKTORCH | MACHINE LEARNING DEVELOPER\\nMa y 201 7 – Presen t | Remo te\\n•Worked onNatural Language models using Recurrent neural\\nnetworks inTensorﬂow.\\n•Developed Chatbots(for banking andtravel industries) using in\\nhouse NLPunitandMongoDB andﬂaskforREST API.\\n•Predictive analysis inKaggle competitions.\\nCONSTALYTICS | DEEP L EARNING INTERN\\nOct 201 7 – Presen t | Chandigarh\\n•Worked onderiving insights from unstructured datausing\\nadvanced deep learning techniques.\\n•Worked withgraph database(Neo4j) andgraph analytics to\\nmonetize unstructured data.\\n•Major contributions from building annotation toolfordata\\nlabelling topresenting theproduct tomultinational clients.\\nPROJECTS\\nMACHINE LEARNING BLOG\\nJ an 201 7 – Presen t\\nApopular machine learning blogconsisting ofposts ranging from explaining\\nbackpropagation tocomplex tensorﬂow implementations accompanied with\\nimplementation ofresearch papers.\\nTEXT GENERATOR\\nJ ul y 201 7\\nBuilt acharacter leveltextgenerator using Recurrent Neural\\nNetworks(RNNs,LSTMs),Embeddings,Language modelling inTensorﬂow.When fed\\nwithasample textﬁle,the model generates similar styled text.Features suchas\\ntraining resumption,intermediate sampling implemented.Effect ofdifferent learning\\nalgorithms andregularization techniques alsoexplored.\\nKAGGLE COMPETITIONS\\nJ un 201 7 -presen t\\nStood top10(At thetimeofsubmission) inKaggle’s House Prices Prediction\\ncompetition byimplementing bagging,boosting andstacking of8advanced\\nregression models including Extreme Gradient Boosting(xgboost) andGradient\\nBoosted Trees(GBT) using sklearn,numpy,pandas andscipy.\\nRESEARCH PROJECT\\nOct 2016-Dec 2016\\nAlab(research) project onprediction ofpolymer product quality inanindustrial\\nreactor using recurrent neural networks based onthepaper ”Online prediction of\\npolymer product quality inanindustrial reactor using recurrent neural networks” by\\nRandall S.Barton andDavid M.Himmelblau.\\n1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
     ]
    }
   ],
   "source": [
    "reader= PDFReader()\n",
    "docs = reader.load_data(Path(\"resume.pdf\"))\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53a60d8-912f-43e1-9725-6055bbd79e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_text = \"IIT VARANASI BTECH(2013-2017) CHEMICAL ENGINEERING AND TECHNOLOGY CGPA: 8.40\"\n",
    "github_link = \"https://github.com/jasdeep06\"\n",
    "linkedin_link = \"https://www.linkedin.com/in/jasdeepchhabra/\"\n",
    "certification_text = \"\"\"\n",
    "COURSERA’S DEEP LEARNING SPECIALIZATION\n",
    "• Neural Networks and Deep\n",
    "Learning (100 %grade)(https://www.coursera.org/account/accomplishments/certificate/WGQFQ48YQWU9)\n",
    "• Improving Deep Neural\n",
    "Networks: Hyperparameter\n",
    "tuning, Regularization and\n",
    "Optimization (100 %grade)(https://www.coursera.org/account/accomplishments/certificate/ZQGLNKLAHME6)\n",
    "• Structuring Machine Learning\n",
    "Projects (100 %grade)(https://www.coursera.org/account/accomplishments/certificate/D5YZJFTCN3KL)\n",
    "OTHER\n",
    "• Coursera’s Machine Learning\n",
    "(Stanford University/Andrew Ng)(https://www.coursera.org/account/accomplishments/certificate/K3FHF5GAVB7N)\n",
    "\"\"\"\n",
    "\n",
    "skills_text = \"\"\"\n",
    "PROGRAMMING LANGUAGES\n",
    "Python • Java • C •\n",
    "AREAS OF INTEREST\n",
    "Deep Learning • Natural Language\n",
    "Processing • Computer Vision •\n",
    "LIBRARIES AND DATABASES\n",
    "Tensorflow • Numpy • Pandas\n",
    "•Django •scikit-learn • Neo4j •\n",
    "MongoDB• flask • pymongo •\n",
    "Spacy •MATLAB •\n",
    "\"\"\"\n",
    "\n",
    "experience_text = \"\"\"\n",
    "UDACITY | SELF DRIVING CAR NANODEGREE\n",
    "Dec 2017 – Present\n",
    "• Pursuing self driving car nanodegree.\n",
    "TEKTORCH(https://tektorch.com.au/) | MACHINE LEARNING DEVELOPER\n",
    "May 2017 – Present | Remote\n",
    "• Worked on Natural Language models using Recurrent neural\n",
    "networks in Tensorflow.\n",
    "• Developed Chatbots(for banking and travel industries) using in\n",
    "house NLP unit and MongoDB and flask for REST API.\n",
    "• Predictive analysis in Kaggle competitions.\n",
    "CONSTALYTICS(http://www.constalytics.com/) | DEEP LEARNING INTERN\n",
    "Oct 2017 – Present | Chandigarh\n",
    "• Worked on deriving insights from unstructured data using\n",
    "advanced deep learning techniques.\n",
    "• Worked with graph database(Neo4j) and graph analytics to\n",
    "monetize unstructured data.\n",
    "• Major contributions from building annotation tool for data\n",
    "labelling to presenting the product to multinational clients.\n",
    "\"\"\"\n",
    "\n",
    "project_text = \"\"\"\n",
    "MACHINE LEARNING BLOG(https://jasdeep06.github.io/)\n",
    "Jan 2017 – Present\n",
    "A popular machine learning blog consisting of posts ranging from explaining\n",
    "backpropagation to complex tensorflow implementations accompanied with\n",
    "implementation ofresearch papers.\n",
    "TEXT GENERATOR\n",
    "July 2017\n",
    "Built a characterlevel text generator using Recurrent Neural\n",
    "Networks(RNNs,LSTMs),Embeddings,Language modelling in Tensorflow.When fed\n",
    "with a sample text file,the model generates similar styled text.Features such as\n",
    "training resumption,intermediate sampling implemented.Effect of different learning\n",
    "algorithms and regularization techniques also explored.\n",
    "KAGGLE COMPETITIONS\n",
    "Jun 2017-present\n",
    "Stood top 10(At the time of submission) in Kaggle’s House Prices Prediction\n",
    "competition by implementing bagging,boosting and stacking of 8 advanced\n",
    "regression models including Extreme Gradient Boosting(xgboost) and Gradient\n",
    "Boosted Trees(GBT) using sklearn,numpy,pandas and scipy.\n",
    "RESEARCH PROJECT\n",
    "Oct 2016-Dec 2016\n",
    "A lab(research) project on prediction of polymer product quality in an industrial\n",
    "reactor using recurrent neural networks based on the paper”Online prediction of\n",
    "polymer product quality in an industrialreactor using recurrent neural networks” by\n",
    "Randall S. Barton and David M. Himmelblau.\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6b4ce7-de0e-4615-93bc-24d8ead899a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='ed5c8559-6504-4963-850e-f0bdb8963a16', embedding=None, metadata={'page_label': '1', 'file_name': 'towards-backpropagation.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='4/2/24, 1:23 AM towards-backpropagation\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 1/9Backpropagation\\nHOME\\nTowards-Backpropagation\\nBackpropagation is by far the most important algorithm for training a neural\\nnetwork.Although alternatives such as Genetic Algorithm or Exhaustive search exist but\\ntheir performance is vastly inferior as compared to backpropagation.Many resources\\nare scattered across web that explain backpropagation but they can be pretty\\nintimidating for a beginner due to their immensely mathematical nature.This series of\\nblogposts is aimed to develop an intuition for backpropagation algorithm which would\\nenable the reader to implement backpropagation on the go. In this post we will try to\\ndevelop an intuition how change in input of a simple node system will a\\x00ect its output.\\nWhy understand it when Tensorflow/Theano are there?\\nLibraries like Tensorflow and Theano give us a ready-made implementation of\\nbackpropagation algorithm and do everything for us in few lines of code.Why to go\\nthrough all this fuss?Well,for me personally, it feels good to know the intricacies of\\nwhat I am working with but the “feel-good” factor for some folks is not a good enough\\nreason.For them,A lot of times while implementing research papers,you would come\\nacross structures other than networks to implement backpropagation through.Also\\nproblems like Vanishing gradients on sigmoids,Dying ReLUs,Exploding gradients in\\nRNNs can be better understood and prevented if you know intricacies of\\nbackpropagation. So enough of motivation!Bottom line is that “You should understand\\nbackpropagation.”\\nA lot of times while implementing research papers,you would come across\\nstructures other than networks to implement backpropagation through.Also\\nproblems like Vanishing gradients on sigmoids,Dying ReLUs,Exploding\\ngradients in RNNs can be better understood and prevented if you know\\nintricacies of backpropagation.\\nSo lets get started!', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ebed9762-7267-4c62-90f7-ed4213578d72', embedding=None, metadata={'page_label': '2', 'file_name': 'towards-backpropagation.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='4/2/24, 1:23 AM towards-backpropagation\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 2/9Before understanding backpropagation,let us go through the basic case on which\\nmajority of this post will be based. It consists of a node accepting two inputs a,b  and\\nproducing an output. It will be referred to as default case for rest of this post.\\nThe node accepts two inputs a,b  and does a product operation on them and gives a*b\\nas output.\\nAim-Our aim is to increase the output by tweaking values of a and b.\\nMethod#1\\nThe first method is the most obvious one.Let us randomly increase the values of a and\\nb by a small quantity h times a random number:\\ndef product (a,b):\\nreturn a*b\\na=4\\nb=3\\nh=0.01\\na=a+h*(random.random())\\nb=b+h*(random.random())\\nprint(product(a,b))\\nThe output for above program is 12.042  which is greater than 12 as was our\\naim.Although our aim is achieved but there are problems:\\nThis is a good strategy for small problems with few nodes but with millions of\\ninputs and thousands of nodes which is easily possible in modern day networks', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9cf048d8-33b9-4c8c-9800-04ad601265e0', embedding=None, metadata={'page_label': '3', 'file_name': 'towards-backpropagation.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='4/2/24, 1:23 AM towards-backpropagation\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 3/9this strategy of exhaustive search would be too time consuming.\\nThis is an unreliable method to increase value of the function.When we randomly\\nincrease the values of a and b,it might result in decrease in value of function.Have\\na look at the code below:\\ndef product (a,b):\\nreturn a*b\\na=-4\\nb=-3\\nh=0.01\\na=a+h*(random.random())\\nb=b+h*(random.random())\\nprint(product(a,b))\\nThe above code produces output 11.94  which is less than 12.The only di\\x00erence was\\nin initial values of a and b.Thus randomly increasing the values of a and b wonʼt su\\x00ice\\nour aim.\\nThus we can conclude that:\\nWe need more control over increasing the values of input.Essentially it means that\\nthere should be more controlled coe\\x00icient of h.\\nThe coe\\x00icient of h should be function of input or inputs as changing the initial\\nvalues of input changed the behaviour of output.\\nMethod#2\\nThis control that we desire in the coe\\x00icient of h is given by derivative.Derivative of a\\nfunction with respect to a variable is a pretty easy and straightforward concept.Let us\\nunderstand by modifying our default case-\\nLet us increase the value of a by a small quantity h(same as in method 1) and let us give\\na name to our product function say f(a,b).Thus:\\nf(a,b)=a∗b', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='66b22e3b-7e74-4743-98a1-42c7c8f9b293', embedding=None, metadata={'page_label': '4', 'file_name': 'towards-backpropagation.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='4/2/24, 1:23 AM towards-backpropagation\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 4/9\\nThe new output can be calculated easily as (a+h)*b  which can be expanded as\\na*b+h*b .Thus the output increases by a value of h*b  as compared to default case.We\\ncan say that with increase of h in a the output increases by h*b ,thus with a unit\\nincrease in a the output would have increased by b.This normalised e\\x00ect of increase in\\nvalue of one of the input on output is expressed as derivative of output with respect to\\nthat input.Note that when we take out derivative of function with respect to a variable\\nthen all other variables are kept constant. The mathematical interpretation of the\\nderivative of function f with respect to a is defined as-\\nThe above formula is nothing but mathematical interpretation of definition of\\nderivative as mentioned above.(For those familiar with multivariate calculus,this is the\\npartial derivative of  with respect to )\\nSimilarly,the derivative of function f with respect to b can be found by increasing b by a\\nsmall quantity h and normalising the di\\x00erence between the final output(a*b+a*h )\\nand initial output(a*b ) that gives us a.\\nThis is the numeric method of finding gradients.It has a significant drawback.Although\\nit is less error prone but it takes lot of time to calculate derivatives this way.Time to\\nintroduce a new method of finding derivatives:Analytical method.\\nMethod#3=∂f\\n∂af((a+h),b)−f(a,b)\\nh\\nf a', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2e2f559b-ce8a-4cfa-ab9c-e58e4d54ed7f', embedding=None, metadata={'page_label': '5', 'file_name': 'towards-backpropagation.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='4/2/24, 1:23 AM towards-backpropagation\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 5/9Analytical gradient method:In order to calculate gradient using analytical gradient\\nmethod we need to remember few basic rules of calculus(Refer Derivative-rules).These\\nrules are derived from the numerical gradient method and are committed to memory\\nso that they can be used directly.This saves us the computation time and space\\nrequired for calculating derivatives.For f=a*b ,the following can be directly stated-\\nThis can easily be derived from the mathematical interpretation of derivative as stated\\nabove.Putting values in interpretation-\\nSimilarly one can derive:\\nWith these two derivatives in hand we have our coe\\x00icients of h in a-update and b-\\nupdate.So our modified update rules will be-=b∂f\\n∂a\\n=∂f\\n∂a(a+h)∗b−a∗b\\nh\\n=∂f\\n∂a(a∗b+h∗b)−a∗b\\nh\\n=b∂f\\n∂a\\n=a∂f\\n∂b\\na=a+h∗ =a+h∗b∂f\\n∂a\\nb=b+h∗ =b+h∗a∂f\\n∂b', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e03fdd03-b9e7-4be5-8034-f1df022a0d4e', embedding=None, metadata={'page_label': '6', 'file_name': 'towards-backpropagation.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='4/2/24, 1:23 AM towards-backpropagation\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 6/9The modified code will be:\\ndef product (a,b):\\nreturn a*b\\na=-4\\nb=-3\\nh=0.01\\na=a+h*b\\nb=b+h*a\\nprint(product(a,b))\\nThe output of above code is 12.252 which is greater than 12 as was our aim.\\nWhy does this approach work?\\nTo appreciate the beauty of derivative in updates of inputs a and b we have to dive into\\ngeometrical interpretation of derivative.Geometrically,derivative of a function with\\nrespect to a variable tells us the rate at which that function changes with respect to that\\nvariable.While reading ahead keep in mind those update rules given by-\\nDerivative of a function with respect to a variable tells us the rate at which\\nthat function changes with respect to that variable keeping all other variables\\nconstant.\\nLet us understand the e\\x00ect of derivative in update rules in a single-variable system to\\nget an intuition of what is happenning-a=a+h∗∂f\\n∂a\\nb=b+h∗∂f\\n∂b', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='49d0473a-f584-4b20-8f72-8ea30d84f4eb', embedding=None, metadata={'page_label': '7', 'file_name': 'towards-backpropagation.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='4/2/24, 1:23 AM towards-backpropagation\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 7/9\\nAbove figure represents a function in single variable x.As x increases(from le\\x00 to right)\\nthe function increases till point B and then decreases as we furthur increase x.Let us\\nimagine two cases-\\nCase#1\\nImagine we are at point A as shown in figure and we want to change the value of x in\\nsuch a way that the value of funcion increases.From the figure it is clear that if we\\nincrease the value of x,the value of function increases.Now let us take a look at our\\nupdate rule and see how it comes to this conclusion-\\nWe know that our h>0  so whether x increases or decreases depends upon derivative of\\nfunction with respect to x.Recall that derivative is nothing but the rate of change of\\nfunction.At point A we can see that the function increases as x increases so the rate of\\nchange of function with respect to x at A is positive.This means that at point A the\\nderivative of function with respect to x is positive.A positive derivative will make x\\nincrease through the update rule.\\nCase#2\\nImagine that we are at point C as shown in figure.Everything remains same as case#1\\nbut now we can see from figure that the value of function increases with decrease in\\nvalue of x.At this time the derivative of funcion with respect to x will be negative asx=x+h∗dy\\ndx', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='59a8256c-61f3-40fd-9404-f994a082497d', embedding=None, metadata={'page_label': '8', 'file_name': 'towards-backpropagation.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='4/2/24, 1:23 AM towards-backpropagation\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 8/9value of function decreases as x increases(i.e. the rate of change of function with x is\\nnegative).When we put a negative derivative in our update rule the value of x decreases\\nas expected.\\nThus the derivative captures the nature of our function and gives our update\\nrule a sense of direction to obtain a higher value.\\nCase#3\\nImagine that we are somewhere pretty close to point B(say we are on the le\\x00 of it but\\npretty close to it).The value of derivative will be positive and the update rule would\\nwant to increase the value of xin order to increase the value of the function.But a\\ncondition could arise when value of h (stepsize) is su\\x00iciently large that it overshoots\\nthe point B and thus decreasing the value of our function.Then we would have to adjust\\nvalue of our stepsize(make it small of course!) in order to increase value of our\\nfunction.Thus this approach of increasing value of inputs in direction of derivative\\nwould give us desired result for almost all functions you would encounter.(It fails for\\nsome poorly defined convex functions, but for now you do not need to worry about\\nthem.)\\nI will stop this post here.I hope that this post helped you develop an intuition about\\nderivatives and their involvement in update rule.In the next post we will apply these\\nprincipals in nested nodes and will finally see backpropagation.\\nPosted on 12 January,2017\\nALSO ON JASDEEP06\\n7 years ago  2 comments\\nBackpr opagation : F urther \\ninto Backpr opagation\\nFurther-int o- Further-int o-\\nbackpr opagation backpr opagation\\n• 7 years ago  32 comments\\nCNNs in Tensor\\x00ow(cifar-\\n10)\\nUnderstanding LSTM inUnderstanding LSTM in\\nTensor\\x00owTensor\\x00ow\\n• 7 years ago  14 comments\\nTensor\\x00ow: V ariable sharing  \\nin Tensor\\x00ow\\nVariable-sharing-in-Variable-sharing-in-\\nTensor\\x00owTensor\\x00ow\\n• 7 years ago  \\nTensor\\x00ow :\\nwith Tensor\\x00\\nGetting staGetting sta\\nTensor\\x00owTensor\\x00ow\\n•', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='bdef3380-154d-4095-ad7d-2790b4966d08', embedding=None, metadata={'page_label': '9', 'file_name': 'towards-backpropagation.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='4/2/24, 1:23 AM towards-backpropagation\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 9/9Shar e Best Newest Oldest0 Comments \\ue603Jasdeep Singh Chhabr a\\nStart the discussion…\\nBe the \\x00rst t o comment.\\nSubscribe Privacy Do Not Sell My Data\\n\\uf109\\nBackpropagation maintained by jasdeep06', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(reader.load_data(Path(\"towards-backpropagation.pdf\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e0b83b-fc91-4de6-b521-aed8e5cb0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unstructured.partition.api import partition_via_api\n",
    "\n",
    "# filename = \"towards-backpropagation.pdf\"\n",
    "# elements = partition_via_api(filename=filename, api_key=\"MJeckXx5WG6ZL5vHrKwHuDgJQIK5PJ\", content_type=\"message/rfc822\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f5231c58-6b73-4d70-a428-399fbadbb252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 3cbbc98e-0ce1-4635-b102-1451102c91af\n",
      "[{'pages': [{'page': 1, 'text': '4/2/24, 1:23 AM                                                              towards-backpropagation\\n  Backpropagation\\n                                                                                                                    HOME\\n  Towards-Backpropagation\\n  Backpropagation is by far the most important algorithm for training a neural\\n  network.Although alternatives such as Genetic Algorithm or Exhaustive search exist but\\n  their performance is vastly inferior as compared to backpropagation.Many resources\\n  are scattered across web that explain backpropagation but they can be pretty\\n  intimidating for a beginner due to their immensely mathematical nature.This series of\\n  blogposts is aimed to develop an intuition for backpropagation algorithm which would\\n  enable the reader to implement backpropagation on the go. In this post we will try to\\n  develop an intuition how change in input of a simple node system will aff                           ect its output.\\n  Why understand it when Tensorflow/Theano are there?\\n  Libraries like Tensorflow and Theano give us a ready-made implementation of\\n  backpropagation algorithm and do everything for us in few lines of code.Why to go\\n  through all this fuss?Well,for me personally, it feels good to know the intricacies of\\n  what I am working with but the “feel-good” factor for some folks is not a good enough\\n  reason.For them,A lot of times while implementing research papers,you would come\\n  across structures other than networks to implement backpropagation through.Also\\n  problems like Vanishing gradients on sigmoids,Dying ReLUs,Exploding gradients in\\n  RNNs can be better understood and prevented if you know intricacies of\\n  backpropagation. So enough of motivation!Bottom line is that “You should understand\\n  backpropagation.”\\n          A lot of times while implementing research papers,you would come across\\n          structures other than networks to implement backpropagation through.Also\\n          problems like Vanishing gradients on sigmoids,Dying ReLUs,Exploding\\n          gradients in RNNs can be better understood and prevented if you know\\n          intricacies of backpropagation.\\n  So lets get started!\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                                1/9', 'md': \"## Backpropagation\\n\\nBackpropagation is by far the most important algorithm for training a neural network. Although alternatives such as Genetic Algorithm or Exhaustive search exist, their performance is vastly inferior as compared to backpropagation. Many resources are scattered across the web that explain backpropagation, but they can be pretty intimidating for a beginner due to their immensely mathematical nature. This series of blog posts is aimed to develop an intuition for the backpropagation algorithm, which would enable the reader to implement backpropagation on the go. In this post, we will try to develop an intuition on how a change in input of a simple node system will affect its output.\\n\\nWhy understand it when Tensorflow/Theano are there? Libraries like Tensorflow and Theano give us a ready-made implementation of the backpropagation algorithm and do everything for us in a few lines of code. Why go through all this fuss? Well, for me personally, it feels good to know the intricacies of what I am working with, but the “feel-good” factor for some folks is not a good enough reason. For them, a lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation. So enough of motivation! Bottom line is that “You should understand backpropagation.”\\n\\nA lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation.\\n\\nSo let's get started!\\n\\nLink to the blog post: Towards-Backpropagation\", 'images': [], 'items': [{'type': 'heading', 'lvl': 2, 'value': 'Backpropagation', 'md': '## Backpropagation'}, {'type': 'text', 'value': \"Backpropagation is by far the most important algorithm for training a neural network. Although alternatives such as Genetic Algorithm or Exhaustive search exist, their performance is vastly inferior as compared to backpropagation. Many resources are scattered across the web that explain backpropagation, but they can be pretty intimidating for a beginner due to their immensely mathematical nature. This series of blog posts is aimed to develop an intuition for the backpropagation algorithm, which would enable the reader to implement backpropagation on the go. In this post, we will try to develop an intuition on how a change in input of a simple node system will affect its output.\\n\\nWhy understand it when Tensorflow/Theano are there? Libraries like Tensorflow and Theano give us a ready-made implementation of the backpropagation algorithm and do everything for us in a few lines of code. Why go through all this fuss? Well, for me personally, it feels good to know the intricacies of what I am working with, but the “feel-good” factor for some folks is not a good enough reason. For them, a lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation. So enough of motivation! Bottom line is that “You should understand backpropagation.”\\n\\nA lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation.\\n\\nSo let's get started!\\n\\nLink to the blog post: Towards-Backpropagation\", 'md': \"Backpropagation is by far the most important algorithm for training a neural network. Although alternatives such as Genetic Algorithm or Exhaustive search exist, their performance is vastly inferior as compared to backpropagation. Many resources are scattered across the web that explain backpropagation, but they can be pretty intimidating for a beginner due to their immensely mathematical nature. This series of blog posts is aimed to develop an intuition for the backpropagation algorithm, which would enable the reader to implement backpropagation on the go. In this post, we will try to develop an intuition on how a change in input of a simple node system will affect its output.\\n\\nWhy understand it when Tensorflow/Theano are there? Libraries like Tensorflow and Theano give us a ready-made implementation of the backpropagation algorithm and do everything for us in a few lines of code. Why go through all this fuss? Well, for me personally, it feels good to know the intricacies of what I am working with, but the “feel-good” factor for some folks is not a good enough reason. For them, a lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation. So enough of motivation! Bottom line is that “You should understand backpropagation.”\\n\\nA lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation.\\n\\nSo let's get started!\\n\\nLink to the blog post: Towards-Backpropagation\"}]}, {'page': 2, 'text': '4/2/24, 1:23 AM                                                              towards-backpropagation\\n  Before understanding backpropagation,let us go through the basic case on which\\n  majority of this post will be based. It consists of a node accepting two inputs a,b and\\n  producing an output. It will be referred to as default case for rest of this post.\\n                                                                           a*b\\n  The node accepts two inputs a,b and does a product operation on them and gives a*b\\n  as output.\\n  Aim-Our aim is to increase the output by tweaking values of a and b.\\n  Method#1\\n  The first method is the most obvious one.Let us randomly increase the values of a and\\n  b by a small quantity h times a random number:\\n  def product(a,b):\\n                     return a*b\\n  a=4\\n  b=3\\n  h=0.01\\n  a=a+h*(random.random())\\n  b=b+h*(random.random())\\n  print(product(a,b))\\n  The output for above program is 12.042 which is greater than 12 as was our\\n  aim.Although our aim is achieved but there are problems:\\n          This is a good strategy for small problems with few nodes but with millions of\\n          inputs and thousands of nodes which is easily possible in modern day networks\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                            2/9', 'md': 'Before understanding backpropagation, let us go through the basic case on which the majority of this post will be based. It consists of a node accepting two inputs a, b and producing an output. It will be referred to as the default case for the rest of this post.\\n\\nThe node accepts two inputs a, b and does a product operation on them and gives a*b as output.\\n\\nAim: Our aim is to increase the output by tweaking values of a and b.\\n\\nMethod#1\\n\\nThe first method is the most obvious one. Let us randomly increase the values of a and b by a small quantity h times a random number:\\n\\ndef product(a, b):\\nreturn a*b\\n\\na = 4\\nb = 3\\nh = 0.01\\na = a + h*(random.random())\\nb = b + h*(random.random())\\nprint(product(a, b))\\n\\nThe output for the above program is 12.042 which is greater than 12 as was our aim. Although our aim is achieved, there are problems:\\n\\n- This is a good strategy for small problems with few nodes but with millions of inputs and thousands of nodes which is easily possible in modern-day networks', 'images': [{'name': 'page-2-0.jpg', 'height': 213, 'width': 341, 'x': 33, 'y': 99}], 'items': [{'type': 'text', 'value': 'Before understanding backpropagation, let us go through the basic case on which the majority of this post will be based. It consists of a node accepting two inputs a, b and producing an output. It will be referred to as the default case for the rest of this post.\\n\\nThe node accepts two inputs a, b and does a product operation on them and gives a*b as output.\\n\\nAim: Our aim is to increase the output by tweaking values of a and b.\\n\\nMethod#1\\n\\nThe first method is the most obvious one. Let us randomly increase the values of a and b by a small quantity h times a random number:\\n\\ndef product(a, b):\\nreturn a*b\\n\\na = 4\\nb = 3\\nh = 0.01\\na = a + h*(random.random())\\nb = b + h*(random.random())\\nprint(product(a, b))\\n\\nThe output for the above program is 12.042 which is greater than 12 as was our aim. Although our aim is achieved, there are problems:\\n\\n- This is a good strategy for small problems with few nodes but with millions of inputs and thousands of nodes which is easily possible in modern-day networks', 'md': 'Before understanding backpropagation, let us go through the basic case on which the majority of this post will be based. It consists of a node accepting two inputs a, b and producing an output. It will be referred to as the default case for the rest of this post.\\n\\nThe node accepts two inputs a, b and does a product operation on them and gives a*b as output.\\n\\nAim: Our aim is to increase the output by tweaking values of a and b.\\n\\nMethod#1\\n\\nThe first method is the most obvious one. Let us randomly increase the values of a and b by a small quantity h times a random number:\\n\\ndef product(a, b):\\nreturn a*b\\n\\na = 4\\nb = 3\\nh = 0.01\\na = a + h*(random.random())\\nb = b + h*(random.random())\\nprint(product(a, b))\\n\\nThe output for the above program is 12.042 which is greater than 12 as was our aim. Although our aim is achieved, there are problems:\\n\\n- This is a good strategy for small problems with few nodes but with millions of inputs and thousands of nodes which is easily possible in modern-day networks'}]}, {'page': 3, 'text': '4/2/24, 1:23 AM                                                              towards-backpropagation\\n          this strategy of exhaustive search would be too time consuming.\\n          This is an unreliable method to increase value of the function.When we randomly\\n          increase the values of a and b,it might result in decrease in value of function.Have\\n          a look at the code below:\\n           def product(a,b):\\n                             return a*b\\n           a=-4\\n           b=-3\\n           h=0.01\\n           a=a+h*(random.random())\\n           b=b+h*(random.random())\\n           print(product(a,b))\\n  The above code produces output 11.94 which is less than 12.The only diff                                              erence was\\n  in initial values of a and b.Thus randomly increasing the values of a and b won’t suff                                            ice\\n  our aim.\\n  Thus we can conclude that:\\n          We need more control over increasing the values of input.Essentially it means that\\n          there should be more controlled coeff                            icient of h.\\n          The coefficient of h should be function of input or inputs as changing the initial\\n          values of input changed the behaviour of output.\\n  Method#2\\n  This control that we desire in the coeff                          icient of h is given by derivative.Derivative of a\\n  function with respect to a variable is a pretty easy and straightforward concept.Let us\\n  understand by modifying our default case-\\n  Let us increase the value of a by a small quantity h(same as in method 1) and let us give\\n  a name to our product function say f(a,b).Thus:\\n                                                          f(a, b) = a ∗                       b\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                                               3/9', 'md': 'towards-backpropagation\\n\\nthis strategy of exhaustive search would be too time consuming. This is an unreliable method to increase value of the function. When we randomly increase the values of a and b, it might result in decrease in value of function. Have a look at the code below:\\n\\ndef product(a,b):\\nreturn a*b\\na=-4\\nb=-3\\nh=0.01\\na=a+h*(random.random())\\nb=b+h*(random.random())\\nprint(product(a,b))\\n\\nThe above code produces output 11.94 which is less than 12. The only difference was in initial values of a and b. Thus randomly increasing the values of a and b won’t suffice our aim.\\n\\nThus we can conclude that: We need more control over increasing the values of input. Essentially it means that there should be more controlled coefficient of h. The coefficient of h should be function of input or inputs as changing the initial values of input changed the behaviour of output.\\n\\nMethod#2 This control that we desire in the coefficient of h is given by derivative. Derivative of a function with respect to a variable is a pretty easy and straightforward concept. Let us understand by modifying our default case- Let us increase the value of a by a small quantity h (same as in method 1) and let us give a name to our product function say f(a,b). Thus: f(a, b) = a * b\\n\\nSource: https://jasdeep06.github.io/posts/towards-backpropagation/', 'images': [], 'items': [{'type': 'text', 'value': 'towards-backpropagation\\n\\nthis strategy of exhaustive search would be too time consuming. This is an unreliable method to increase value of the function. When we randomly increase the values of a and b, it might result in decrease in value of function. Have a look at the code below:\\n\\ndef product(a,b):\\nreturn a*b\\na=-4\\nb=-3\\nh=0.01\\na=a+h*(random.random())\\nb=b+h*(random.random())\\nprint(product(a,b))\\n\\nThe above code produces output 11.94 which is less than 12. The only difference was in initial values of a and b. Thus randomly increasing the values of a and b won’t suffice our aim.\\n\\nThus we can conclude that: We need more control over increasing the values of input. Essentially it means that there should be more controlled coefficient of h. The coefficient of h should be function of input or inputs as changing the initial values of input changed the behaviour of output.\\n\\nMethod#2 This control that we desire in the coefficient of h is given by derivative. Derivative of a function with respect to a variable is a pretty easy and straightforward concept. Let us understand by modifying our default case- Let us increase the value of a by a small quantity h (same as in method 1) and let us give a name to our product function say f(a,b). Thus: f(a, b) = a * b\\n\\nSource: https://jasdeep06.github.io/posts/towards-backpropagation/', 'md': 'towards-backpropagation\\n\\nthis strategy of exhaustive search would be too time consuming. This is an unreliable method to increase value of the function. When we randomly increase the values of a and b, it might result in decrease in value of function. Have a look at the code below:\\n\\ndef product(a,b):\\nreturn a*b\\na=-4\\nb=-3\\nh=0.01\\na=a+h*(random.random())\\nb=b+h*(random.random())\\nprint(product(a,b))\\n\\nThe above code produces output 11.94 which is less than 12. The only difference was in initial values of a and b. Thus randomly increasing the values of a and b won’t suffice our aim.\\n\\nThus we can conclude that: We need more control over increasing the values of input. Essentially it means that there should be more controlled coefficient of h. The coefficient of h should be function of input or inputs as changing the initial values of input changed the behaviour of output.\\n\\nMethod#2 This control that we desire in the coefficient of h is given by derivative. Derivative of a function with respect to a variable is a pretty easy and straightforward concept. Let us understand by modifying our default case- Let us increase the value of a by a small quantity h (same as in method 1) and let us give a name to our product function say f(a,b). Thus: f(a, b) = a * b\\n\\nSource: https://jasdeep06.github.io/posts/towards-backpropagation/'}]}, {'page': 4, 'text': '4/2/24, 1:23 AM                                                              towards-backpropagation\\n                       a+h                                           (a+h)tb\\n  The new output can be calculated easily as (a+h)*b which can be expanded as\\n  a*b+h*b.Thus the output increases by a value of h*b as compared to default case.We\\n  can say that with increase of h in a the output increases by h*b,thus with a unit\\n  increase in a the output would have increased by b.This normalised eff                                          ect of increase in\\n  value of one of the input on output is expressed as derivative of output with respect to\\n  that input.Note that when we take out derivative of function with respect to a variable\\n  then all other variables are kept constant. The mathematical interpretation of the\\n  derivative of function f with respect to a is defined as-\\n                                      ∂f        =        f((a + h), b) −                           f(a, b)\\n                                      ∂a                                             h\\n  The above formula is nothing but mathematical interpretation of definition of\\n  derivative as mentioned above.(For those familiar with multivariate calculus,this is the\\n  partial derivative of             f   with respect to           a  )\\n  Similarly,the derivative of function f with respect to b can be found by increasing b by a\\n  small quantity h and normalising the diff                             erence between the final output(a*b+a*h)\\n  and initial output(a*b) that gives us a.\\n  This is the numeric method of finding gradients.It has a significant drawback.Although\\n  it is less error prone but it takes lot of time to calculate derivatives this way.Time to\\n  introduce a new method of finding derivatives:Analytical method.\\n  Method#3\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                                            4/9', 'md': 'The new output can be calculated easily as (a+h)*b which can be expanded as a*b+h*b. Thus the output increases by a value of h*b as compared to the default case. We can say that with an increase of h in a, the output increases by h*b, thus with a unit increase in a the output would have increased by b. This normalized effect of an increase in the value of one of the input on output is expressed as the derivative of output with respect to that input. Note that when we take the derivative of a function with respect to a variable, then all other variables are kept constant. The mathematical interpretation of the derivative of function f with respect to a is defined as:\\n\\n∂f / ∂a = f((a + h), b) - f(a, b) / h\\n\\nThe above formula is nothing but the mathematical interpretation of the definition of the derivative as mentioned above. (For those familiar with multivariate calculus, this is the partial derivative of f with respect to a.)\\n\\nSimilarly, the derivative of function f with respect to b can be found by increasing b by a small quantity h and normalizing the difference between the final output (a*b+a*h) and the initial output (a*b) that gives us a.\\n\\nThis is the numeric method of finding gradients. It has a significant drawback. Although it is less error-prone, it takes a lot of time to calculate derivatives this way. Time to introduce a new method of finding derivatives: Analytical method.', 'images': [{'name': 'page-4-0.jpg', 'height': 201, 'width': 357, 'x': 33, 'y': 27}], 'items': [{'type': 'text', 'value': 'The new output can be calculated easily as (a+h)*b which can be expanded as a*b+h*b. Thus the output increases by a value of h*b as compared to the default case. We can say that with an increase of h in a, the output increases by h*b, thus with a unit increase in a the output would have increased by b. This normalized effect of an increase in the value of one of the input on output is expressed as the derivative of output with respect to that input. Note that when we take the derivative of a function with respect to a variable, then all other variables are kept constant. The mathematical interpretation of the derivative of function f with respect to a is defined as:\\n\\n∂f / ∂a = f((a + h), b) - f(a, b) / h\\n\\nThe above formula is nothing but the mathematical interpretation of the definition of the derivative as mentioned above. (For those familiar with multivariate calculus, this is the partial derivative of f with respect to a.)\\n\\nSimilarly, the derivative of function f with respect to b can be found by increasing b by a small quantity h and normalizing the difference between the final output (a*b+a*h) and the initial output (a*b) that gives us a.\\n\\nThis is the numeric method of finding gradients. It has a significant drawback. Although it is less error-prone, it takes a lot of time to calculate derivatives this way. Time to introduce a new method of finding derivatives: Analytical method.', 'md': 'The new output can be calculated easily as (a+h)*b which can be expanded as a*b+h*b. Thus the output increases by a value of h*b as compared to the default case. We can say that with an increase of h in a, the output increases by h*b, thus with a unit increase in a the output would have increased by b. This normalized effect of an increase in the value of one of the input on output is expressed as the derivative of output with respect to that input. Note that when we take the derivative of a function with respect to a variable, then all other variables are kept constant. The mathematical interpretation of the derivative of function f with respect to a is defined as:\\n\\n∂f / ∂a = f((a + h), b) - f(a, b) / h\\n\\nThe above formula is nothing but the mathematical interpretation of the definition of the derivative as mentioned above. (For those familiar with multivariate calculus, this is the partial derivative of f with respect to a.)\\n\\nSimilarly, the derivative of function f with respect to b can be found by increasing b by a small quantity h and normalizing the difference between the final output (a*b+a*h) and the initial output (a*b) that gives us a.\\n\\nThis is the numeric method of finding gradients. It has a significant drawback. Although it is less error-prone, it takes a lot of time to calculate derivatives this way. Time to introduce a new method of finding derivatives: Analytical method.'}]}, {'page': 5, 'text': '4/2/24, 1:23 AM                                                              towards-backpropagation\\n  Analytical gradient method:In order to calculate gradient using analytical gradient\\n  method we need to remember few basic rules of calculus(Refer Derivative-rules).These\\n  rules are derived from the numerical gradient method and are committed to memory\\n  so that they can be used directly.This saves us the computation time and space\\n  required for calculating derivatives.For f=a*b,the following can be directly stated-\\n                                                                    ∂f        = b\\n                                                                    ∂a\\n  This can easily be derived from the mathematical interpretation of derivative as stated\\n  above.Putting values in interpretation-\\n                                           ∂f         =       (a + h) ∗                 b −        a ∗       b\\n                                           ∂a                                        h\\n                                       ∂f        =       (a ∗         b + h ∗             b) −          a ∗      b\\n                                       ∂a                                            h\\n                                                                    ∂f        = b\\n                                                                    ∂a\\n  Similarly one can derive:\\n                                                                    ∂f        = a\\n                                                                    ∂b\\n  With these two derivatives in hand we have our coeff                                       icients of h in a-update and b-\\n  update.So our modified update rules will be-\\n                                       a = a + h ∗                        ∂f         = a + h ∗                   b\\n                                                                          ∂a\\n                                        b = b + h ∗                      ∂f         = b + h ∗                   a\\n                                                                          ∂b\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                                    5/9', 'md': '4/2/24, 1:23 AM towards-backpropagation\\n\\n|Analytical gradient method:|In order to calculate gradient using analytical gradient method we need to remember few basic rules of calculus(Refer Derivative-rules).These rules are derived from the numerical gradient method and are committed to memory so that they can be used directly.This saves us the computation time and space required for calculating derivatives.For f=a*b,the following can be directly stated-|\\n|---|---|\\n|∂f|= b|\\n| |∂a|\\n|This can easily be derived from the mathematical interpretation of derivative as stated above.Putting values in interpretation-| |\\n|∂f|= (a + h) * b - a * b|\\n|∂a|h|\\n|∂f|= (a * b + h * b) - a * b|\\n|∂a|h|\\n|∂f|= b|\\n| |∂a|\\n|Similarly one can derive:| |\\n|∂f|= a|\\n| |∂b|\\n|With these two derivatives in hand we have our coefficients of h in a-update and b-update.So our modified update rules will be-| |\\n|a = a + h * ∂f|a = a + h * b|\\n| |∂a|\\n|b = b + h * ∂f|b = b + h * a|\\n| |∂b|\\n\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 5/9', 'images': [], 'items': [{'type': 'text', 'value': '4/2/24, 1:23 AM towards-backpropagation', 'md': '4/2/24, 1:23 AM towards-backpropagation'}, {'type': 'table', 'rows': [['Analytical gradient method:', 'In order to calculate gradient using analytical gradient method we need to remember few basic rules of calculus(Refer Derivative-rules).These rules are derived from the numerical gradient method and are committed to memory so that they can be used directly.This saves us the computation time and space required for calculating derivatives.For f=a*b,the following can be directly stated-'], ['∂f', '= b'], ['', '∂a'], ['This can easily be derived from the mathematical interpretation of derivative as stated above.Putting values in interpretation-', ''], ['∂f', '= (a + h) * b - a * b'], ['∂a', 'h'], ['∂f', '= (a * b + h * b) - a * b'], ['∂a', 'h'], ['∂f', '= b'], ['', '∂a'], ['Similarly one can derive:', ''], ['∂f', '= a'], ['', '∂b'], ['With these two derivatives in hand we have our coefficients of h in a-update and b-update.So our modified update rules will be-', ''], ['a = a + h * ∂f', 'a = a + h * b'], ['', '∂a'], ['b = b + h * ∂f', 'b = b + h * a'], ['', '∂b']], 'md': '|Analytical gradient method:|In order to calculate gradient using analytical gradient method we need to remember few basic rules of calculus(Refer Derivative-rules).These rules are derived from the numerical gradient method and are committed to memory so that they can be used directly.This saves us the computation time and space required for calculating derivatives.For f=a*b,the following can be directly stated-|\\n|---|---|\\n|∂f|= b|\\n| |∂a|\\n|This can easily be derived from the mathematical interpretation of derivative as stated above.Putting values in interpretation-| |\\n|∂f|= (a + h) * b - a * b|\\n|∂a|h|\\n|∂f|= (a * b + h * b) - a * b|\\n|∂a|h|\\n|∂f|= b|\\n| |∂a|\\n|Similarly one can derive:| |\\n|∂f|= a|\\n| |∂b|\\n|With these two derivatives in hand we have our coefficients of h in a-update and b-update.So our modified update rules will be-| |\\n|a = a + h * ∂f|a = a + h * b|\\n| |∂a|\\n|b = b + h * ∂f|b = b + h * a|\\n| |∂b|', 'isPerfectTable': True, 'csv': '\"Analytical gradient method:\",\"In order to calculate gradient using analytical gradient method we need to remember few basic rules of calculus(Refer Derivative-rules).These rules are derived from the numerical gradient method and are committed to memory so that they can be used directly.This saves us the computation time and space required for calculating derivatives.For f=a*b,the following can be directly stated-\"\\n\"∂f\",\"= b\"\\n\"\",\"∂a\"\\n\"This can easily be derived from the mathematical interpretation of derivative as stated above.Putting values in interpretation-\",\"\"\\n\"∂f\",\"= (a + h) * b - a * b\"\\n\"∂a\",\"h\"\\n\"∂f\",\"= (a * b + h * b) - a * b\"\\n\"∂a\",\"h\"\\n\"∂f\",\"= b\"\\n\"\",\"∂a\"\\n\"Similarly one can derive:\",\"\"\\n\"∂f\",\"= a\"\\n\"\",\"∂b\"\\n\"With these two derivatives in hand we have our coefficients of h in a-update and b-update.So our modified update rules will be-\",\"\"\\n\"a = a + h * ∂f\",\"a = a + h * b\"\\n\"\",\"∂a\"\\n\"b = b + h * ∂f\",\"b = b + h * a\"\\n\"\",\"∂b\"'}, {'type': 'text', 'value': 'https://jasdeep06.github.io/posts/towards-backpropagation/ 5/9', 'md': 'https://jasdeep06.github.io/posts/towards-backpropagation/ 5/9'}]}, {'page': 6, 'text': '4/2/24, 1:23 AM                                                              towards-backpropagation\\n  The modified code will be:\\n  def product(a,b):\\n                     return a*b\\n  a=-4\\n  b=-3\\n  h=0.01\\n  a=a+h*b\\n  b=b+h*a\\n  print(product(a,b))\\n  The output of above code is 12.252 which is greater than 12 as was our aim.\\n  Why does this approach work?\\n  To appreciate the beauty of derivative in updates of inputs a and b we have to dive into\\n  geometrical interpretation of derivative.Geometrically,derivative of a function with\\n  respect to a variable tells us the rate at which that function changes with respect to that\\n  variable.While reading ahead keep in mind those update rules given by-\\n                                                       a = a + h ∗                        ∂f\\n                                                                                           ∂a\\n                                                        b = b + h ∗                       ∂f\\n                                                                                          ∂b\\n          Derivative of a function with respect to a variable tells us the rate at which\\n          that function changes with respect to that variable keeping all other variables\\n          constant.\\n  Let us understand the eff                  ect of derivative in update rules in a single-variable system to\\n  get an intuition of what is happenning-\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                     6/9', 'md': '4/2/24, 1:23 AM towards-backpropagation\\n\\nThe modified code will be:\\n\\ndef product(a,b):\\n\\nreturn a*b\\n\\na=-4\\n\\nb=-3\\n\\nh=0.01\\n\\na=a+h*b\\n\\nb=b+h*a\\n\\nprint(product(a,b))\\n\\nThe output of above code is 12.252 which is greater than 12 as was our aim.\\n\\nWhy does this approach work?\\n\\nTo appreciate the beauty of derivative in updates of inputs a and b we have to dive into geometrical interpretation of derivative. Geometrically, derivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable. While reading ahead keep in mind those update rules given by-\\n\\na = a + h ∗ ∂f\\n\\n∂a\\n\\nb = b + h ∗ ∂f\\n\\n∂b\\n\\nDerivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable keeping all other variables constant.\\n\\nLet us understand the effect of derivative in update rules in a single-variable system to get an intuition of what is happening- Link to Source', 'images': [], 'items': [{'type': 'text', 'value': '4/2/24, 1:23 AM towards-backpropagation\\n\\nThe modified code will be:\\n\\ndef product(a,b):\\n\\nreturn a*b\\n\\na=-4\\n\\nb=-3\\n\\nh=0.01\\n\\na=a+h*b\\n\\nb=b+h*a\\n\\nprint(product(a,b))\\n\\nThe output of above code is 12.252 which is greater than 12 as was our aim.\\n\\nWhy does this approach work?\\n\\nTo appreciate the beauty of derivative in updates of inputs a and b we have to dive into geometrical interpretation of derivative. Geometrically, derivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable. While reading ahead keep in mind those update rules given by-\\n\\na = a + h ∗ ∂f\\n\\n∂a\\n\\nb = b + h ∗ ∂f\\n\\n∂b\\n\\nDerivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable keeping all other variables constant.\\n\\nLet us understand the effect of derivative in update rules in a single-variable system to get an intuition of what is happening- Link to Source', 'md': '4/2/24, 1:23 AM towards-backpropagation\\n\\nThe modified code will be:\\n\\ndef product(a,b):\\n\\nreturn a*b\\n\\na=-4\\n\\nb=-3\\n\\nh=0.01\\n\\na=a+h*b\\n\\nb=b+h*a\\n\\nprint(product(a,b))\\n\\nThe output of above code is 12.252 which is greater than 12 as was our aim.\\n\\nWhy does this approach work?\\n\\nTo appreciate the beauty of derivative in updates of inputs a and b we have to dive into geometrical interpretation of derivative. Geometrically, derivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable. While reading ahead keep in mind those update rules given by-\\n\\na = a + h ∗ ∂f\\n\\n∂a\\n\\nb = b + h ∗ ∂f\\n\\n∂b\\n\\nDerivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable keeping all other variables constant.\\n\\nLet us understand the effect of derivative in update rules in a single-variable system to get an intuition of what is happening- Link to Source'}]}, {'page': 7, 'text': '4/2/24, 1:23 AM                                                              towards-backpropagation\\n                                                                                   y-f(x)\\n  Above figure represents a function in single variable x.As x increases(from left to right)\\n  the function increases till point B and then decreases as we furthur increase x.Let us\\n  imagine two cases-\\n  Case#1\\n  Imagine we are at point A as shown in figure and we want to change the value of x in\\n  such a way that the value of funcion increases.From the figure it is clear that if we\\n  increase the value of x,the value of function increases.Now let us take a look at our\\n  update rule and see how it comes to this conclusion-\\n                                                       x = x + h ∗                         dy\\n                                                                                           dx\\n  We know that our h>0 so whether x increases or decreases depends upon derivative of\\n  function with respect to x.Recall that derivative is nothing but the rate of change of\\n  function.At point A we can see that the function increases as x increases so the rate of\\n  change of function with respect to x at A is positive.This means that at point A the\\n  derivative of function with respect to x is positive.A positive derivative will make x\\n  increase through the update rule.\\n  Case#2\\n  Imagine that we are at point C as shown in figure.Everything remains same as case#1\\n  but now we can see from figure that the value of function increases with decrease in\\n  value of x.At this time the derivative of funcion with respect to x will be negative as\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                            7/9', 'md': 'Above figure represents a function in single variable x. As x increases (from left to right) the function increases till point B and then decreases as we further increase x. Let us imagine two cases-\\n\\nCase#1\\n\\nImagine we are at point A as shown in the figure and we want to change the value of x in such a way that the value of the function increases. From the figure, it is clear that if we increase the value of x, the value of the function increases. Now let us take a look at our update rule and see how it comes to this conclusion-\\n\\nx = x + h * dy/dx\\n\\nWe know that our h > 0 so whether x increases or decreases depends upon the derivative of the function with respect to x. Recall that the derivative is nothing but the rate of change of the function. At point A, we can see that the function increases as x increases so the rate of change of the function with respect to x at A is positive. This means that at point A the derivative of the function with respect to x is positive. A positive derivative will make x increase through the update rule.\\n\\nCase#2\\n\\nImagine that we are at point C as shown in the figure. Everything remains the same as case#1 but now we can see from the figure that the value of the function increases with a decrease in the value of x. At this time the derivative of the function with respect to x will be negative as\\n\\nSource', 'images': [{'name': 'page-7-0.jpg', 'height': 211, 'width': 357, 'x': 33, 'y': 27}], 'items': [{'type': 'text', 'value': 'Above figure represents a function in single variable x. As x increases (from left to right) the function increases till point B and then decreases as we further increase x. Let us imagine two cases-\\n\\nCase#1\\n\\nImagine we are at point A as shown in the figure and we want to change the value of x in such a way that the value of the function increases. From the figure, it is clear that if we increase the value of x, the value of the function increases. Now let us take a look at our update rule and see how it comes to this conclusion-\\n\\nx = x + h * dy/dx\\n\\nWe know that our h > 0 so whether x increases or decreases depends upon the derivative of the function with respect to x. Recall that the derivative is nothing but the rate of change of the function. At point A, we can see that the function increases as x increases so the rate of change of the function with respect to x at A is positive. This means that at point A the derivative of the function with respect to x is positive. A positive derivative will make x increase through the update rule.\\n\\nCase#2\\n\\nImagine that we are at point C as shown in the figure. Everything remains the same as case#1 but now we can see from the figure that the value of the function increases with a decrease in the value of x. At this time the derivative of the function with respect to x will be negative as\\n\\nSource', 'md': 'Above figure represents a function in single variable x. As x increases (from left to right) the function increases till point B and then decreases as we further increase x. Let us imagine two cases-\\n\\nCase#1\\n\\nImagine we are at point A as shown in the figure and we want to change the value of x in such a way that the value of the function increases. From the figure, it is clear that if we increase the value of x, the value of the function increases. Now let us take a look at our update rule and see how it comes to this conclusion-\\n\\nx = x + h * dy/dx\\n\\nWe know that our h > 0 so whether x increases or decreases depends upon the derivative of the function with respect to x. Recall that the derivative is nothing but the rate of change of the function. At point A, we can see that the function increases as x increases so the rate of change of the function with respect to x at A is positive. This means that at point A the derivative of the function with respect to x is positive. A positive derivative will make x increase through the update rule.\\n\\nCase#2\\n\\nImagine that we are at point C as shown in the figure. Everything remains the same as case#1 but now we can see from the figure that the value of the function increases with a decrease in the value of x. At this time the derivative of the function with respect to x will be negative as\\n\\nSource'}]}, {'page': 8, 'text': '4/2/24, 1:23 AM                                                                                        towards-backpropagation\\n   value of function decreases as x increases(i.e. the rate of change of function with x is\\n   negative).When we put a negative derivative in our update rule the value of x decreases\\n   as expected.\\n               Thus the derivative captures the nature of our function and gives our update\\n               rule a sense of direction to obtain a higher value.\\n   Case#3\\n   Imagine that we are somewhere pretty close to point B(say we are on the left of it but\\n   pretty close to it).The value of derivative will be positive and the update rule would\\n   want to increase the value of xin order to increase the value of the function.But a\\n   condition could arise when value of h (stepsize) is sufficiently large that it overshoots\\n   the point B and thus decreasing the value of our function.Then we would have to adjust\\n   value of our stepsize(make it small of course!) in order to increase value of our\\n   function.Thus this approach of increasing value of inputs in direction of derivative\\n   would give us desired result for almost all functions you would encounter.(It fails for\\n   some poorly defined convex functions, but for now you do not need to worry about\\n   them.)\\n   I will stop this post here.I hope that this post helped you develop an intuition about\\n   derivatives and their involvement in update rule.In the next post we will apply these\\n   principals in nested nodes and will finally see backpropagation.\\n                                                                           Posted on 12 January,2017\\n      ALSO ON JASDEEP06\\n        Further-Into-                                              Understanding LSTA In                                   Variable sharing-Ins                Getting st4\\n         Further-into-                                              Understanding LSTM in                                   Variable-sharing-in-                 Getting sta\\n         Further-into-                                              Understanding LSTM in                                   Variable-sharing-in-                 Getting sta\\n       backpropaqation                                              Tensorflov                                              Tensorflom                          Tensorflov\\n         backpropagation                                            Tensorflow                                              Tensorflow                           Tensorflow\\n         backpropagation                                            Tensorflow                                              Tensorflow                           Tensorflow\\n         7 years ago        • 2 comments                            7 years ago         • 32 comments                       7 years ago        •  14 comments    7 years ago    •\\n         Backpropagation : Further                                  CNNs in Tensorflow(cifar-                               Tensorflow: Variable sharing         Tensorflow :\\n         into Backpropagation                                       10)                                                     in Tensorflow                        with Tensorfl\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                                                                                         8/9', 'md': '4/2/24, 1:23 AM towards-backpropagation value of function decreases as x increases(i.e. the rate of change of function with x is negative).When we put a negative derivative in our update rule the value of x decreases as expected. Thus the derivative captures the nature of our function and gives our update rule a sense of direction to obtain a higher value. Case#3 Imagine that we are somewhere pretty close to point B(say we are on the left of it but pretty close to it).The value of derivative will be positive and the update rule would want to increase the value of xin order to increase the value of the function.But a condition could arise when value of h (stepsize) is sufficiently large that it overshoots the point B and thus decreasing the value of our function.Then we would have to adjust value of our stepsize(make it small of course!) in order to increase value of our function.Thus this approach of increasing value of inputs in direction of derivative would give us desired result for almost all functions you would encounter.(It fails for some poorly defined convex functions, but for now you do not need to worry about them.) I will stop this post here.I hope that this post helped you develop an intuition about derivatives and their involvement in update rule.In the next post we will apply these principals in nested nodes and will finally see backpropagation. Posted on 12 January,2017 ALSO ON JASDEEP06 Further-Into- Understanding LSTA In Variable sharing-Ins Getting st4 Further-into- Understanding LSTM in Variable-sharing-in- Getting sta Further-into- Understanding LSTM in Variable-sharing-in- Getting sta backpropaqation Tensorflov Tensorflom Tensorflov backpropagation Tensorflow Tensorflow Tensorflow backpropagation Tensorflow Tensorflow Tensorflow 7 years ago • 2 comments 7 years ago • 32 comments 7 years ago • 14 comments 7 years ago • Backpropagation : Further CNNs in Tensorflow(cifar- 10) Tensorflow: Variable sharing Tensorflow : with Tensorfl\\n\\nhttps://jasdeep06.gipub.io/posts/towards-backpropagation/\\n8/9', 'images': [{'name': 'page-8-0.jpg', 'height': 76, 'width': 149, 'x': 40, 'y': 559}, {'name': 'page-8-2.jpg', 'height': 76, 'width': 149, 'x': 352, 'y': 559}, {'name': 'page-8-3.jpg', 'height': 76, 'width': 149, 'x': 508, 'y': 559}, {'name': 'page-8-1.jpg', 'height': 76, 'width': 149, 'x': 196, 'y': 559}], 'items': [{'type': 'text', 'value': '4/2/24, 1:23 AM towards-backpropagation value of function decreases as x increases(i.e. the rate of change of function with x is negative).When we put a negative derivative in our update rule the value of x decreases as expected. Thus the derivative captures the nature of our function and gives our update rule a sense of direction to obtain a higher value. Case#3 Imagine that we are somewhere pretty close to point B(say we are on the left of it but pretty close to it).The value of derivative will be positive and the update rule would want to increase the value of xin order to increase the value of the function.But a condition could arise when value of h (stepsize) is sufficiently large that it overshoots the point B and thus decreasing the value of our function.Then we would have to adjust value of our stepsize(make it small of course!) in order to increase value of our function.Thus this approach of increasing value of inputs in direction of derivative would give us desired result for almost all functions you would encounter.(It fails for some poorly defined convex functions, but for now you do not need to worry about them.) I will stop this post here.I hope that this post helped you develop an intuition about derivatives and their involvement in update rule.In the next post we will apply these principals in nested nodes and will finally see backpropagation. Posted on 12 January,2017 ALSO ON JASDEEP06 Further-Into- Understanding LSTA In Variable sharing-Ins Getting st4 Further-into- Understanding LSTM in Variable-sharing-in- Getting sta Further-into- Understanding LSTM in Variable-sharing-in- Getting sta backpropaqation Tensorflov Tensorflom Tensorflov backpropagation Tensorflow Tensorflow Tensorflow backpropagation Tensorflow Tensorflow Tensorflow 7 years ago • 2 comments 7 years ago • 32 comments 7 years ago • 14 comments 7 years ago • Backpropagation : Further CNNs in Tensorflow(cifar- 10) Tensorflow: Variable sharing Tensorflow : with Tensorfl\\n\\nhttps://jasdeep06.gipub.io/posts/towards-backpropagation/\\n8/9', 'md': '4/2/24, 1:23 AM towards-backpropagation value of function decreases as x increases(i.e. the rate of change of function with x is negative).When we put a negative derivative in our update rule the value of x decreases as expected. Thus the derivative captures the nature of our function and gives our update rule a sense of direction to obtain a higher value. Case#3 Imagine that we are somewhere pretty close to point B(say we are on the left of it but pretty close to it).The value of derivative will be positive and the update rule would want to increase the value of xin order to increase the value of the function.But a condition could arise when value of h (stepsize) is sufficiently large that it overshoots the point B and thus decreasing the value of our function.Then we would have to adjust value of our stepsize(make it small of course!) in order to increase value of our function.Thus this approach of increasing value of inputs in direction of derivative would give us desired result for almost all functions you would encounter.(It fails for some poorly defined convex functions, but for now you do not need to worry about them.) I will stop this post here.I hope that this post helped you develop an intuition about derivatives and their involvement in update rule.In the next post we will apply these principals in nested nodes and will finally see backpropagation. Posted on 12 January,2017 ALSO ON JASDEEP06 Further-Into- Understanding LSTA In Variable sharing-Ins Getting st4 Further-into- Understanding LSTM in Variable-sharing-in- Getting sta Further-into- Understanding LSTM in Variable-sharing-in- Getting sta backpropaqation Tensorflov Tensorflom Tensorflov backpropagation Tensorflow Tensorflow Tensorflow backpropagation Tensorflow Tensorflow Tensorflow 7 years ago • 2 comments 7 years ago • 32 comments 7 years ago • 14 comments 7 years ago • Backpropagation : Further CNNs in Tensorflow(cifar- 10) Tensorflow: Variable sharing Tensorflow : with Tensorfl\\n\\nhttps://jasdeep06.gipub.io/posts/towards-backpropagation/\\n8/9'}]}, {'page': 9, 'text': '4/2/24, 1:23 AM                                                              towards-backpropagation\\n  0 Comments                                                                                          \\ue603  Jasdeep Singh Chhabra\\n                   Start the discussion…\\n     \\uf109          Share                                                                                            Best     Newest  Oldest\\n                                                             Be the first to comment.\\n       Subscribe              Privacy             Do Not Sell My Data\\n  Backpropagation maintained by jasdeep06\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                                                9/9', 'md': '# 4/2/24, 1:23 AM\\n\\n# towards-backpropagation\\n\\n0 Comments\\n\\nJasdeep Singh Chhabra\\n\\nStart the discussion...\\n\\nShare\\n\\nBest | Newest | Oldest\\n\\nBe the first to comment.\\n\\nSubscribe | Privacy | Do Not Sell My Data\\n\\nBackpropagation maintained by jasdeep06\\n\\nLink to the article', 'images': [{'name': 'page-9-0.jpg', 'height': 39, 'width': 39, 'x': 33, 'y': 83}], 'items': [{'type': 'heading', 'lvl': 1, 'value': '4/2/24, 1:23 AM', 'md': '# 4/2/24, 1:23 AM'}, {'type': 'heading', 'lvl': 1, 'value': 'towards-backpropagation', 'md': '# towards-backpropagation'}, {'type': 'text', 'value': '0 Comments\\n\\nJasdeep Singh Chhabra\\n\\nStart the discussion...\\n\\nShare\\n\\nBest | Newest | Oldest\\n\\nBe the first to comment.\\n\\nSubscribe | Privacy | Do Not Sell My Data\\n\\nBackpropagation maintained by jasdeep06\\n\\nLink to the article', 'md': '0 Comments\\n\\nJasdeep Singh Chhabra\\n\\nStart the discussion...\\n\\nShare\\n\\nBest | Newest | Oldest\\n\\nBe the first to comment.\\n\\nSubscribe | Privacy | Do Not Sell My Data\\n\\nBackpropagation maintained by jasdeep06\\n\\nLink to the article'}]}], 'job_id': '3cbbc98e-0ce1-4635-b102-1451102c91af', 'file_path': 'towards-backpropagation.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "\n",
    "llamaparser = LlamaParse(\n",
    "    api_key=\"llx-nO6BMv1gWY9CqX0eUqTF8sKhez2TEs7UD2zG3aFNY1fVPaBD\",  # can also be set in your env as LLAMA_CLOUD_API_KEY\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# sync\n",
    "# documents = parser.load_data(\"/content/towards-backpropagation.pdf\")\n",
    "json_objs = llamaparser.get_json_result(\"towards-backpropagation.pdf\")\n",
    "\n",
    "\n",
    "print(json_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "892368a1-857f-4bec-9702-039b2bf709ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## Backpropagation\\n\\nBackpropagation is by far the most important algorithm for training a neural network. Although alternatives such as Genetic Algorithm or Exhaustive search exist, their performance is vastly inferior as compared to backpropagation. Many resources are scattered across the web that explain backpropagation, but they can be pretty intimidating for a beginner due to their immensely mathematical nature. This series of blog posts is aimed to develop an intuition for the backpropagation algorithm, which would enable the reader to implement backpropagation on the go. In this post, we will try to develop an intuition on how a change in input of a simple node system will affect its output.\\n\\nWhy understand it when Tensorflow/Theano are there? Libraries like Tensorflow and Theano give us a ready-made implementation of the backpropagation algorithm and do everything for us in a few lines of code. Why go through all this fuss? Well, for me personally, it feels good to know the intricacies of what I am working with, but the “feel-good” factor for some folks is not a good enough reason. For them, a lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation. So enough of motivation! Bottom line is that “You should understand backpropagation.”\\n\\nA lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation.\\n\\nSo let's get started!\\n\\nLink to the blog post: Towards-Backpropagation\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_objs[0]['pages'][0][\"md\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b7268309-d7f5-4d92-aca9-585f1e723518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## Backpropagation\\n\\nBackpropagation is by far the most important algorithm for training a neural network. Although alternatives such as Genetic Algorithm or Exhaustive search exist, their performance is vastly inferior as compared to backpropagation. Many resources are scattered across the web that explain backpropagation, but they can be pretty intimidating for a beginner due to their immensely mathematical nature. This series of blog posts is aimed to develop an intuition for the backpropagation algorithm, which would enable the reader to implement backpropagation on the go. In this post, we will try to develop an intuition on how a change in input of a simple node system will affect its output.\\n\\nWhy understand it when Tensorflow/Theano are there? Libraries like Tensorflow and Theano give us a ready-made implementation of the backpropagation algorithm and do everything for us in a few lines of code. Why go through all this fuss? Well, for me personally, it feels good to know the intricacies of what I am working with, but the “feel-good” factor for some folks is not a good enough reason. For them, a lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation. So enough of motivation! Bottom line is that “You should understand backpropagation.”\\n\\nA lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation.\\n\\nSo let's get started!\\n\\nLink to the blog post: Towards-Backpropagation\\n\\nBefore understanding backpropagation, let us go through the basic case on which the majority of this post will be based. It consists of a node accepting two inputs a, b and producing an output. It will be referred to as the default case for the rest of this post.\\n\\nThe node accepts two inputs a, b and does a product operation on them and gives a*b as output.\\n\\nAim: Our aim is to increase the output by tweaking values of a and b.\\n\\nMethod#1\\n\\nThe first method is the most obvious one. Let us randomly increase the values of a and b by a small quantity h times a random number:\\n\\ndef product(a, b):\\nreturn a*b\\n\\na = 4\\nb = 3\\nh = 0.01\\na = a + h*(random.random())\\nb = b + h*(random.random())\\nprint(product(a, b))\\n\\nThe output for the above program is 12.042 which is greater than 12 as was our aim. Although our aim is achieved, there are problems:\\n\\n- This is a good strategy for small problems with few nodes but with millions of inputs and thousands of nodes which is easily possible in modern-day networks\\n\\ntowards-backpropagation\\n\\nthis strategy of exhaustive search would be too time consuming. This is an unreliable method to increase value of the function. When we randomly increase the values of a and b, it might result in decrease in value of function. Have a look at the code below:\\n\\ndef product(a,b):\\nreturn a*b\\na=-4\\nb=-3\\nh=0.01\\na=a+h*(random.random())\\nb=b+h*(random.random())\\nprint(product(a,b))\\n\\nThe above code produces output 11.94 which is less than 12. The only difference was in initial values of a and b. Thus randomly increasing the values of a and b won’t suffice our aim.\\n\\nThus we can conclude that: We need more control over increasing the values of input. Essentially it means that there should be more controlled coefficient of h. The coefficient of h should be function of input or inputs as changing the initial values of input changed the behaviour of output.\\n\\nMethod#2 This control that we desire in the coefficient of h is given by derivative. Derivative of a function with respect to a variable is a pretty easy and straightforward concept. Let us understand by modifying our default case- Let us increase the value of a by a small quantity h (same as in method 1) and let us give a name to our product function say f(a,b). Thus: f(a, b) = a * b\\n\\nSource: https://jasdeep06.github.io/posts/towards-backpropagation/\\n\\nThe new output can be calculated easily as (a+h)*b which can be expanded as a*b+h*b. Thus the output increases by a value of h*b as compared to the default case. We can say that with an increase of h in a, the output increases by h*b, thus with a unit increase in a the output would have increased by b. This normalized effect of an increase in the value of one of the input on output is expressed as the derivative of output with respect to that input. Note that when we take the derivative of a function with respect to a variable, then all other variables are kept constant. The mathematical interpretation of the derivative of function f with respect to a is defined as:\\n\\n∂f / ∂a = f((a + h), b) - f(a, b) / h\\n\\nThe above formula is nothing but the mathematical interpretation of the definition of the derivative as mentioned above. (For those familiar with multivariate calculus, this is the partial derivative of f with respect to a.)\\n\\nSimilarly, the derivative of function f with respect to b can be found by increasing b by a small quantity h and normalizing the difference between the final output (a*b+a*h) and the initial output (a*b) that gives us a.\\n\\nThis is the numeric method of finding gradients. It has a significant drawback. Although it is less error-prone, it takes a lot of time to calculate derivatives this way. Time to introduce a new method of finding derivatives: Analytical method.\\n\\n4/2/24, 1:23 AM towards-backpropagation\\n\\n|Analytical gradient method:|In order to calculate gradient using analytical gradient method we need to remember few basic rules of calculus(Refer Derivative-rules).These rules are derived from the numerical gradient method and are committed to memory so that they can be used directly.This saves us the computation time and space required for calculating derivatives.For f=a*b,the following can be directly stated-|\\n|---|---|\\n|∂f|= b|\\n| |∂a|\\n|This can easily be derived from the mathematical interpretation of derivative as stated above.Putting values in interpretation-| |\\n|∂f|= (a + h) * b - a * b|\\n|∂a|h|\\n|∂f|= (a * b + h * b) - a * b|\\n|∂a|h|\\n|∂f|= b|\\n| |∂a|\\n|Similarly one can derive:| |\\n|∂f|= a|\\n| |∂b|\\n|With these two derivatives in hand we have our coefficients of h in a-update and b-update.So our modified update rules will be-| |\\n|a = a + h * ∂f|a = a + h * b|\\n| |∂a|\\n|b = b + h * ∂f|b = b + h * a|\\n| |∂b|\\n\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 5/9\\n\\n4/2/24, 1:23 AM towards-backpropagation\\n\\nThe modified code will be:\\n\\ndef product(a,b):\\n\\nreturn a*b\\n\\na=-4\\n\\nb=-3\\n\\nh=0.01\\n\\na=a+h*b\\n\\nb=b+h*a\\n\\nprint(product(a,b))\\n\\nThe output of above code is 12.252 which is greater than 12 as was our aim.\\n\\nWhy does this approach work?\\n\\nTo appreciate the beauty of derivative in updates of inputs a and b we have to dive into geometrical interpretation of derivative. Geometrically, derivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable. While reading ahead keep in mind those update rules given by-\\n\\na = a + h ∗ ∂f\\n\\n∂a\\n\\nb = b + h ∗ ∂f\\n\\n∂b\\n\\nDerivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable keeping all other variables constant.\\n\\nLet us understand the effect of derivative in update rules in a single-variable system to get an intuition of what is happening- Link to Source\\n\\nAbove figure represents a function in single variable x. As x increases (from left to right) the function increases till point B and then decreases as we further increase x. Let us imagine two cases-\\n\\nCase#1\\n\\nImagine we are at point A as shown in the figure and we want to change the value of x in such a way that the value of the function increases. From the figure, it is clear that if we increase the value of x, the value of the function increases. Now let us take a look at our update rule and see how it comes to this conclusion-\\n\\nx = x + h * dy/dx\\n\\nWe know that our h > 0 so whether x increases or decreases depends upon the derivative of the function with respect to x. Recall that the derivative is nothing but the rate of change of the function. At point A, we can see that the function increases as x increases so the rate of change of the function with respect to x at A is positive. This means that at point A the derivative of the function with respect to x is positive. A positive derivative will make x increase through the update rule.\\n\\nCase#2\\n\\nImagine that we are at point C as shown in the figure. Everything remains the same as case#1 but now we can see from the figure that the value of the function increases with a decrease in the value of x. At this time the derivative of the function with respect to x will be negative as\\n\\nSource\\n\\n4/2/24, 1:23 AM towards-backpropagation value of function decreases as x increases(i.e. the rate of change of function with x is negative).When we put a negative derivative in our update rule the value of x decreases as expected. Thus the derivative captures the nature of our function and gives our update rule a sense of direction to obtain a higher value. Case#3 Imagine that we are somewhere pretty close to point B(say we are on the left of it but pretty close to it).The value of derivative will be positive and the update rule would want to increase the value of xin order to increase the value of the function.But a condition could arise when value of h (stepsize) is sufficiently large that it overshoots the point B and thus decreasing the value of our function.Then we would have to adjust value of our stepsize(make it small of course!) in order to increase value of our function.Thus this approach of increasing value of inputs in direction of derivative would give us desired result for almost all functions you would encounter.(It fails for some poorly defined convex functions, but for now you do not need to worry about them.) I will stop this post here.I hope that this post helped you develop an intuition about derivatives and their involvement in update rule.In the next post we will apply these principals in nested nodes and will finally see backpropagation. Posted on 12 January,2017 ALSO ON JASDEEP06 Further-Into- Understanding LSTA In Variable sharing-Ins Getting st4 Further-into- Understanding LSTM in Variable-sharing-in- Getting sta Further-into- Understanding LSTM in Variable-sharing-in- Getting sta backpropaqation Tensorflov Tensorflom Tensorflov backpropagation Tensorflow Tensorflow Tensorflow backpropagation Tensorflow Tensorflow Tensorflow 7 years ago • 2 comments 7 years ago • 32 comments 7 years ago • 14 comments 7 years ago • Backpropagation : Further CNNs in Tensorflow(cifar- 10) Tensorflow: Variable sharing Tensorflow : with Tensorfl\\n\\nhttps://jasdeep06.gipub.io/posts/towards-backpropagation/\\n8/9\\n\\n# 4/2/24, 1:23 AM\\n\\n# towards-backpropagation\\n\\n0 Comments\\n\\nJasdeep Singh Chhabra\\n\\nStart the discussion...\\n\\nShare\\n\\nBest | Newest | Oldest\\n\\nBe the first to comment.\\n\\nSubscribe | Privacy | Do Not Sell My Data\\n\\nBackpropagation maintained by jasdeep06\\n\\nLink to the article\\n\\n\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_text = \"\"\n",
    "for obj in json_objs[0]['pages']:\n",
    "    # print(obj['md'])\n",
    "    # print(obj['images'])\n",
    "    blog_text += obj['md'] + \"\\n\\n\"\n",
    "\n",
    "blog_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aa84b8ae-a6b0-43db-9687-abb887567656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='89e518c6-c82c-4bca-8f31-51eab31cfff4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"## Backpropagation\\n\\nBackpropagation is by far the most important algorithm for training a neural network. Although alternatives such as Genetic Algorithm or Exhaustive search exist, their performance is vastly inferior as compared to backpropagation. Many resources are scattered across the web that explain backpropagation, but they can be pretty intimidating for a beginner due to their immensely mathematical nature. This series of blog posts is aimed to develop an intuition for the backpropagation algorithm, which would enable the reader to implement backpropagation on the go. In this post, we will try to develop an intuition on how a change in input of a simple node system will affect its output.\\n\\nWhy understand it when Tensorflow/Theano are there? Libraries like Tensorflow and Theano give us a ready-made implementation of the backpropagation algorithm and do everything for us in a few lines of code. Why go through all this fuss? Well, for me personally, it feels good to know the intricacies of what I am working with, but the “feel-good” factor for some folks is not a good enough reason. For them, a lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation. So enough of motivation! Bottom line is that “You should understand backpropagation.”\\n\\nA lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation.\\n\\nSo let's get started!\\n\\nLink to the blog post: Towards-Backpropagation\\n\\nBefore understanding backpropagation, let us go through the basic case on which the majority of this post will be based. It consists of a node accepting two inputs a, b and producing an output. It will be referred to as the default case for the rest of this post.\\n\\nThe node accepts two inputs a, b and does a product operation on them and gives a*b as output.\\n\\nAim: Our aim is to increase the output by tweaking values of a and b.\\n\\nMethod#1\\n\\nThe first method is the most obvious one. Let us randomly increase the values of a and b by a small quantity h times a random number:\\n\\ndef product(a, b):\\nreturn a*b\\n\\na = 4\\nb = 3\\nh = 0.01\\na = a + h*(random.random())\\nb = b + h*(random.random())\\nprint(product(a, b))\\n\\nThe output for the above program is 12.042 which is greater than 12 as was our aim. Although our aim is achieved, there are problems:\\n\\n- This is a good strategy for small problems with few nodes but with millions of inputs and thousands of nodes which is easily possible in modern-day networks\\n\\ntowards-backpropagation\\n\\nthis strategy of exhaustive search would be too time consuming. This is an unreliable method to increase value of the function. When we randomly increase the values of a and b, it might result in decrease in value of function. Have a look at the code below:\\n\\ndef product(a,b):\\nreturn a*b\\na=-4\\nb=-3\\nh=0.01\\na=a+h*(random.random())\\nb=b+h*(random.random())\\nprint(product(a,b))\\n\\nThe above code produces output 11.94 which is less than 12. The only difference was in initial values of a and b. Thus randomly increasing the values of a and b won’t suffice our aim.\\n\\nThus we can conclude that: We need more control over increasing the values of input. Essentially it means that there should be more controlled coefficient of h. The coefficient of h should be function of input or inputs as changing the initial values of input changed the behaviour of output.\\n\\nMethod#2 This control that we desire in the coefficient of h is given by derivative. Derivative of a function with respect to a variable is a pretty easy and straightforward concept. Let us understand by modifying our default case- Let us increase the value of a by a small quantity h (same as in method 1) and let us give a name to our product function say f(a,b). Thus: f(a, b) = a * b\\n\\nSource: https://jasdeep06.github.io/posts/towards-backpropagation/\\n\\nThe new output can be calculated easily as (a+h)*b which can be expanded as a*b+h*b. Thus the output increases by a value of h*b as compared to the default case. We can say that with an increase of h in a, the output increases by h*b, thus with a unit increase in a the output would have increased by b. This normalized effect of an increase in the value of one of the input on output is expressed as the derivative of output with respect to that input. Note that when we take the derivative of a function with respect to a variable, then all other variables are kept constant. The mathematical interpretation of the derivative of function f with respect to a is defined as:\\n\\n∂f / ∂a = f((a + h), b) - f(a, b) / h\\n\\nThe above formula is nothing but the mathematical interpretation of the definition of the derivative as mentioned above. (For those familiar with multivariate calculus, this is the partial derivative of f with respect to a.)\\n\\nSimilarly, the derivative of function f with respect to b can be found by increasing b by a small quantity h and normalizing the difference between the final output (a*b+a*h) and the initial output (a*b) that gives us a.\\n\\nThis is the numeric method of finding gradients. It has a significant drawback. Although it is less error-prone, it takes a lot of time to calculate derivatives this way. Time to introduce a new method of finding derivatives: Analytical method.\\n\\n4/2/24, 1:23 AM towards-backpropagation\\n\\n|Analytical gradient method:|In order to calculate gradient using analytical gradient method we need to remember few basic rules of calculus(Refer Derivative-rules).These rules are derived from the numerical gradient method and are committed to memory so that they can be used directly.This saves us the computation time and space required for calculating derivatives.For f=a*b,the following can be directly stated-|\\n|---|---|\\n|∂f|= b|\\n| |∂a|\\n|This can easily be derived from the mathematical interpretation of derivative as stated above.Putting values in interpretation-| |\\n|∂f|= (a + h) * b - a * b|\\n|∂a|h|\\n|∂f|= (a * b + h * b) - a * b|\\n|∂a|h|\\n|∂f|= b|\\n| |∂a|\\n|Similarly one can derive:| |\\n|∂f|= a|\\n| |∂b|\\n|With these two derivatives in hand we have our coefficients of h in a-update and b-update.So our modified update rules will be-| |\\n|a = a + h * ∂f|a = a + h * b|\\n| |∂a|\\n|b = b + h * ∂f|b = b + h * a|\\n| |∂b|\\n\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 5/9\\n\\n4/2/24, 1:23 AM towards-backpropagation\\n\\nThe modified code will be:\\n\\ndef product(a,b):\\n\\nreturn a*b\\n\\na=-4\\n\\nb=-3\\n\\nh=0.01\\n\\na=a+h*b\\n\\nb=b+h*a\\n\\nprint(product(a,b))\\n\\nThe output of above code is 12.252 which is greater than 12 as was our aim.\\n\\nWhy does this approach work?\\n\\nTo appreciate the beauty of derivative in updates of inputs a and b we have to dive into geometrical interpretation of derivative. Geometrically, derivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable. While reading ahead keep in mind those update rules given by-\\n\\na = a + h ∗ ∂f\\n\\n∂a\\n\\nb = b + h ∗ ∂f\\n\\n∂b\\n\\nDerivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable keeping all other variables constant.\\n\\nLet us understand the effect of derivative in update rules in a single-variable system to get an intuition of what is happening- Link to Source\\n\\nAbove figure represents a function in single variable x. As x increases (from left to right) the function increases till point B and then decreases as we further increase x. Let us imagine two cases-\\n\\nCase#1\\n\\nImagine we are at point A as shown in the figure and we want to change the value of x in such a way that the value of the function increases. From the figure, it is clear that if we increase the value of x, the value of the function increases. Now let us take a look at our update rule and see how it comes to this conclusion-\\n\\nx = x + h * dy/dx\\n\\nWe know that our h > 0 so whether x increases or decreases depends upon the derivative of the function with respect to x. Recall that the derivative is nothing but the rate of change of the function. At point A, we can see that the function increases as x increases so the rate of change of the function with respect to x at A is positive. This means that at point A the derivative of the function with respect to x is positive. A positive derivative will make x increase through the update rule.\\n\\nCase#2\\n\\nImagine that we are at point C as shown in the figure. Everything remains the same as case#1 but now we can see from the figure that the value of the function increases with a decrease in the value of x. At this time the derivative of the function with respect to x will be negative as\\n\\nSource\\n\\n4/2/24, 1:23 AM towards-backpropagation value of function decreases as x increases(i.e. the rate of change of function with x is negative).When we put a negative derivative in our update rule the value of x decreases as expected. Thus the derivative captures the nature of our function and gives our update rule a sense of direction to obtain a higher value. Case#3 Imagine that we are somewhere pretty close to point B(say we are on the left of it but pretty close to it).The value of derivative will be positive and the update rule would want to increase the value of xin order to increase the value of the function.But a condition could arise when value of h (stepsize) is sufficiently large that it overshoots the point B and thus decreasing the value of our function.Then we would have to adjust value of our stepsize(make it small of course!) in order to increase value of our function.Thus this approach of increasing value of inputs in direction of derivative would give us desired result for almost all functions you would encounter.(It fails for some poorly defined convex functions, but for now you do not need to worry about them.) I will stop this post here.I hope that this post helped you develop an intuition about derivatives and their involvement in update rule.In the next post we will apply these principals in nested nodes and will finally see backpropagation. Posted on 12 January,2017 ALSO ON JASDEEP06 Further-Into- Understanding LSTA In Variable sharing-Ins Getting st4 Further-into- Understanding LSTM in Variable-sharing-in- Getting sta Further-into- Understanding LSTM in Variable-sharing-in- Getting sta backpropaqation Tensorflov Tensorflom Tensorflov backpropagation Tensorflow Tensorflow Tensorflow backpropagation Tensorflow Tensorflow Tensorflow 7 years ago • 2 comments 7 years ago • 32 comments 7 years ago • 14 comments 7 years ago • Backpropagation : Further CNNs in Tensorflow(cifar- 10) Tensorflow: Variable sharing Tensorflow : with Tensorfl\\n\\nhttps://jasdeep06.gipub.io/posts/towards-backpropagation/\\n8/9\\n\\n# 4/2/24, 1:23 AM\\n\\n# towards-backpropagation\\n\\n0 Comments\\n\\nJasdeep Singh Chhabra\\n\\nStart the discussion...\\n\\nShare\\n\\nBest | Newest | Oldest\\n\\nBe the first to comment.\\n\\nSubscribe | Privacy | Do Not Sell My Data\\n\\nBackpropagation maintained by jasdeep06\\n\\nLink to the article\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "documents = [Document(text=blog_text)]\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dc893978-e61a-4000-a01c-c34238fe689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].metadata['author'] = \"Jasdeep Singh Chhabra\"\n",
    "documents[0].metadata['blog_title'] = \"Towards Backpropagation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cfe78160-3838-4897-9622-7ef004a0ab5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e3386197-11e6-4b73-9e77-a3e43a043a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001F7AC609400>, id_func=<function default_id_func at 0x000001F7A4435AF0>, chunk_size=512, chunk_overlap=0, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "node_parser = SentenceSplitter(chunk_size=512,chunk_overlap=0)\n",
    "node_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "acc1509f-e4d5-42f9-8174-5fd4a84a4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nodes = node_parser.get_nodes_from_documents(documents)\n",
    "base_nodes\n",
    "for idx,node in enumerate(base_nodes):\n",
    "    node.id_ = \"node-{}\".format(str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d3cd23f9-d696-41c0-b335-856d5624229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node-0\n",
      "node-1\n",
      "node-2\n",
      "node-3\n",
      "node-4\n",
      "node-5\n"
     ]
    }
   ],
   "source": [
    "for node in base_nodes:\n",
    "    print(node.id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "930cdcbe-e129-45f4-9cee-1168d87776b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1346bb58-4bde-4a37-88e1-c0687d5fb02f': IndexNode(id_='1346bb58-4bde-4a37-88e1-c0687d5fb02f', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-0', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='28eb279999decb7813eaf123aa29d697d9426517f16a5856d92ab3862a9b3311'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='02bc84c2-2c5d-46f6-8a1b-099a1f3a689b', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c8b9d920cf91278aaa6e086756b3bd0964ff14d37f8e078fc26d4ac0e2bcd9ac')}, text='## Backpropagation\\n\\nBackpropagation is by far the most important algorithm for training a neural network. Although alternatives such as Genetic Algorithm or Exhaustive search exist, their performance is vastly inferior as compared to backpropagation. Many resources are scattered across the web that explain backpropagation, but they can be pretty intimidating for a beginner due to their immensely mathematical nature. This series of blog posts is aimed to develop an intuition for the backpropagation algorithm, which would enable the reader to implement backpropagation on the go.', start_char_idx=0, end_char_idx=583, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-0', obj=None),\n",
       " '02bc84c2-2c5d-46f6-8a1b-099a1f3a689b': IndexNode(id_='02bc84c2-2c5d-46f6-8a1b-099a1f3a689b', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-0', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='28eb279999decb7813eaf123aa29d697d9426517f16a5856d92ab3862a9b3311'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1346bb58-4bde-4a37-88e1-c0687d5fb02f', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='ebb6bf4d99d7f44950a36afcdd7f6e70cbefd8c5f07e487e333148daddf99974'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4ad806e6-5c58-4eff-af1c-e7c8477aaa62', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='e237fc84de4e59aa4527de3d71b5d54dd5580270fc73ebd42c23c316d4e8f9b9')}, text='In this post, we will try to develop an intuition on how a change in input of a simple node system will affect its output.\\n\\nWhy understand it when Tensorflow/Theano are there? Libraries like Tensorflow and Theano give us a ready-made implementation of the backpropagation algorithm and do everything for us in a few lines of code. Why go through all this fuss?', start_char_idx=584, end_char_idx=944, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-0', obj=None),\n",
       " '4ad806e6-5c58-4eff-af1c-e7c8477aaa62': IndexNode(id_='4ad806e6-5c58-4eff-af1c-e7c8477aaa62', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-0', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='28eb279999decb7813eaf123aa29d697d9426517f16a5856d92ab3862a9b3311'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='02bc84c2-2c5d-46f6-8a1b-099a1f3a689b', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='112f22bfb20e0e11f9b765188c0c53ad7e13dfd088b8f1da71f03aad87f23b25'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='65d8f653-3080-4ca6-bf73-90f66cd1836e', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='cf3d987a0f6712b2e64cf8f5b2f988f0d8cf630f753e4f96877b49e6c3993c80')}, text='Well, for me personally, it feels good to know the intricacies of what I am working with, but the “feel-good” factor for some folks is not a good enough reason. For them, a lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation.', start_char_idx=945, end_char_idx=1438, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-0', obj=None),\n",
       " '65d8f653-3080-4ca6-bf73-90f66cd1836e': IndexNode(id_='65d8f653-3080-4ca6-bf73-90f66cd1836e', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-0', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='28eb279999decb7813eaf123aa29d697d9426517f16a5856d92ab3862a9b3311'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4ad806e6-5c58-4eff-af1c-e7c8477aaa62', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='d4433820e214df8ed75f7adace1d66bdfc09cb56cc65de9a8493c0f0a00813fb'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='bd0b6312-7f90-41a8-887b-002b72c6b184', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b4d84e6ea17606bb83141a553d1b44752275de2cfb56c6132afe397c6f6b52e3')}, text=\"So enough of motivation! Bottom line is that “You should understand backpropagation.”\\n\\nA lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation.\\n\\nSo let's get started!\", start_char_idx=1439, end_char_idx=1871, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-0', obj=None),\n",
       " 'bd0b6312-7f90-41a8-887b-002b72c6b184': IndexNode(id_='bd0b6312-7f90-41a8-887b-002b72c6b184', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-0', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='28eb279999decb7813eaf123aa29d697d9426517f16a5856d92ab3862a9b3311'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='65d8f653-3080-4ca6-bf73-90f66cd1836e', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='a2b9a6890ce334db69c15bcad996284ad5d632772b9e52d44b31075df5d840ae')}, text='Link to the blog post: Towards-Backpropagation\\n\\nBefore understanding backpropagation, let us go through the basic case on which the majority of this post will be based. It consists of a node accepting two inputs a, b and producing an output. It will be referred to as the default case for the rest of this post.\\n\\nThe node accepts two inputs a, b and does a product operation on them and gives a*b as output.', start_char_idx=1873, end_char_idx=2280, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-0', obj=None),\n",
       " '58cccae5-3576-4522-8553-0644306af8df': IndexNode(id_='58cccae5-3576-4522-8553-0644306af8df', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-0', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='28eb279999decb7813eaf123aa29d697d9426517f16a5856d92ab3862a9b3311'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='1e49b818-5b7c-47e3-82b7-e8ddd1878a74', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='fd54aa9424fa7a55fd5a2b4e773ceae3b82913a7307c9d7028385b9f54eed8e9')}, text='## Backpropagation\\n\\nBackpropagation is by far the most important algorithm for training a neural network. Although alternatives such as Genetic Algorithm or Exhaustive search exist, their performance is vastly inferior as compared to backpropagation. Many resources are scattered across the web that explain backpropagation, but they can be pretty intimidating for a beginner due to their immensely mathematical nature. This series of blog posts is aimed to develop an intuition for the backpropagation algorithm, which would enable the reader to implement backpropagation on the go. In this post, we will try to develop an intuition on how a change in input of a simple node system will affect its output.\\n\\nWhy understand it when Tensorflow/Theano are there? Libraries like Tensorflow and Theano give us a ready-made implementation of the backpropagation algorithm and do everything for us in a few lines of code. Why go through all this fuss? Well, for me personally, it feels good to know the intricacies of what I am working with, but the “feel-good” factor for some folks is not a good enough reason.', start_char_idx=0, end_char_idx=1105, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-0', obj=None),\n",
       " '1e49b818-5b7c-47e3-82b7-e8ddd1878a74': IndexNode(id_='1e49b818-5b7c-47e3-82b7-e8ddd1878a74', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-0', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='28eb279999decb7813eaf123aa29d697d9426517f16a5856d92ab3862a9b3311'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='58cccae5-3576-4522-8553-0644306af8df', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='71c7b9e260dcce5761f40713d0c6fc7b25248e0bbad5cce42a4bce335d7460f3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d43af22b-e1d8-403e-8700-1660a9030aaa', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='67a9270c3cf00f792c65f44f2207619ba44c282a3da9762cac783ace59c43f71')}, text=\"For them, a lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation. So enough of motivation! Bottom line is that “You should understand backpropagation.”\\n\\nA lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation.\\n\\nSo let's get started!\\n\\nLink to the blog post: Towards-Backpropagation\\n\\nBefore understanding backpropagation, let us go through the basic case on which the majority of this post will be based. It consists of a node accepting two inputs a, b and producing an output. It will be referred to as the default case for the rest of this post.\", start_char_idx=1106, end_char_idx=2184, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-0', obj=None),\n",
       " 'd43af22b-e1d8-403e-8700-1660a9030aaa': IndexNode(id_='d43af22b-e1d8-403e-8700-1660a9030aaa', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-0', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='28eb279999decb7813eaf123aa29d697d9426517f16a5856d92ab3862a9b3311'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1e49b818-5b7c-47e3-82b7-e8ddd1878a74', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='1748d4cdcd22c9d22b7b54239f35e4131ba7fac2b48b78497bcfc5644bfbb4a1')}, text='The node accepts two inputs a, b and does a product operation on them and gives a*b as output.', start_char_idx=2186, end_char_idx=2280, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-0', obj=None),\n",
       " 'node-0': IndexNode(id_='node-0', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='89e518c6-c82c-4bca-8f31-51eab31cfff4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='981c221281d663434f9057c4fc300009f52f547887ce66bd384136c12e48527a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a133e47c-8e65-434e-bea2-3cca761c8fe0', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='9dfede0b57c3501342deac429b5bd893fb918dfab2d8b73a85a207f0f73845d5')}, text=\"## Backpropagation\\n\\nBackpropagation is by far the most important algorithm for training a neural network. Although alternatives such as Genetic Algorithm or Exhaustive search exist, their performance is vastly inferior as compared to backpropagation. Many resources are scattered across the web that explain backpropagation, but they can be pretty intimidating for a beginner due to their immensely mathematical nature. This series of blog posts is aimed to develop an intuition for the backpropagation algorithm, which would enable the reader to implement backpropagation on the go. In this post, we will try to develop an intuition on how a change in input of a simple node system will affect its output.\\n\\nWhy understand it when Tensorflow/Theano are there? Libraries like Tensorflow and Theano give us a ready-made implementation of the backpropagation algorithm and do everything for us in a few lines of code. Why go through all this fuss? Well, for me personally, it feels good to know the intricacies of what I am working with, but the “feel-good” factor for some folks is not a good enough reason. For them, a lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation. So enough of motivation! Bottom line is that “You should understand backpropagation.”\\n\\nA lot of times while implementing research papers, you would come across structures other than networks to implement backpropagation through. Also, problems like Vanishing gradients on sigmoids, Dying ReLUs, Exploding gradients in RNNs can be better understood and prevented if you know the intricacies of backpropagation.\\n\\nSo let's get started!\\n\\nLink to the blog post: Towards-Backpropagation\\n\\nBefore understanding backpropagation, let us go through the basic case on which the majority of this post will be based. It consists of a node accepting two inputs a, b and producing an output. It will be referred to as the default case for the rest of this post.\\n\\nThe node accepts two inputs a, b and does a product operation on them and gives a*b as output.\", start_char_idx=0, end_char_idx=2280, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-0', obj=None),\n",
       " 'e76591eb-782b-4f59-8217-c1774023b4b5': IndexNode(id_='e76591eb-782b-4f59-8217-c1774023b4b5', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-1', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='86968118853346f5ba3ebb5c892c16926984a3ab5b84bc0804d132d953d72445'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='48736dc1-9cd3-4813-ac32-96c58f27747e', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='6c40fa7ca9873ddfa10e44b751b49a05a1a62860cf4d7fecf958f1a3e8bc55be')}, text='Aim: Our aim is to increase the output by tweaking values of a and b.\\n\\nMethod#1\\n\\nThe first method is the most obvious one.', start_char_idx=0, end_char_idx=122, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-1', obj=None),\n",
       " '48736dc1-9cd3-4813-ac32-96c58f27747e': IndexNode(id_='48736dc1-9cd3-4813-ac32-96c58f27747e', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-1', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='86968118853346f5ba3ebb5c892c16926984a3ab5b84bc0804d132d953d72445'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e76591eb-782b-4f59-8217-c1774023b4b5', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='ed7738b660913297460b921e60a65a7bddb88c915834a7e182e96b8acb8d2c67'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5a16f616-f78c-4327-bd7c-8c816b1507d2', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='2f1ef5a7d639c6d42af86433a506ff61ae3b1e3906aa1352ea7346523f18d446')}, text='Let us randomly increase the values of a and b by a small quantity h times a random number:\\n\\ndef product(a, b):\\nreturn a*b\\n\\na = 4\\nb = 3\\nh = 0.01\\na = a + h*(random.random())\\nb = b + h*(random.random())\\nprint(product(a, b))\\n\\nThe output for the above program is 12.042 which is greater than 12 as was our aim.', start_char_idx=123, end_char_idx=429, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-1', obj=None),\n",
       " '5a16f616-f78c-4327-bd7c-8c816b1507d2': IndexNode(id_='5a16f616-f78c-4327-bd7c-8c816b1507d2', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-1', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='86968118853346f5ba3ebb5c892c16926984a3ab5b84bc0804d132d953d72445'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='48736dc1-9cd3-4813-ac32-96c58f27747e', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='bc3ea8ffb760612b1ed8eeec5f374be0efb5979d80e61e326b0eea22a0ea38b0'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9859b2e7-14bc-4915-b639-ef63fa5ac083', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='9ac83cfe8e638ec03163081dfaaf8b9595f08f6618d352de762852d143f06c81')}, text='Although our aim is achieved, there are problems:\\n\\n- This is a good strategy for small problems with few nodes but with millions of inputs and thousands of nodes which is easily possible in modern-day networks\\n\\ntowards-backpropagation\\n\\nthis strategy of exhaustive search would be too time consuming. This is an unreliable method to increase value of the function. When we randomly increase the values of a and b, it might result in decrease in value of function.', start_char_idx=430, end_char_idx=892, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-1', obj=None),\n",
       " '9859b2e7-14bc-4915-b639-ef63fa5ac083': IndexNode(id_='9859b2e7-14bc-4915-b639-ef63fa5ac083', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-1', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='86968118853346f5ba3ebb5c892c16926984a3ab5b84bc0804d132d953d72445'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5a16f616-f78c-4327-bd7c-8c816b1507d2', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='6ed9ad1cf2b089d287368ed0da2f603c5674ff194289c527744a0b290654add0'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9da36d79-2ea3-46f3-850d-f99a4c223e74', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='741b2bb835d24cf017c8c73d6764c9a2ea96faa557159e4d184a8485630683ba')}, text='Have a look at the code below:\\n\\ndef product(a,b):\\nreturn a*b\\na=-4\\nb=-3\\nh=0.01\\na=a+h*(random.random())\\nb=b+h*(random.random())\\nprint(product(a,b))\\n\\nThe above code produces output 11.94 which is less than 12. The only difference was in initial values of a and b. Thus randomly increasing the values of a and b won’t suffice our aim.\\n\\nThus we can conclude that: We need more control over increasing the values of input.', start_char_idx=893, end_char_idx=1309, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-1', obj=None),\n",
       " '9da36d79-2ea3-46f3-850d-f99a4c223e74': IndexNode(id_='9da36d79-2ea3-46f3-850d-f99a4c223e74', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-1', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='86968118853346f5ba3ebb5c892c16926984a3ab5b84bc0804d132d953d72445'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9859b2e7-14bc-4915-b639-ef63fa5ac083', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='fa377750a15fe43797f06071f5047aa780c16410d796cc72d07a8e4ef028f506'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b8777353-3331-464b-947a-fe243d5f74ed', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='56a554ab378a3069a540a036e03da8ec3241e8bec98efee454fea126b8d4e465')}, text='Essentially it means that there should be more controlled coefficient of h. The coefficient of h should be function of input or inputs as changing the initial values of input changed the behaviour of output.\\n\\nMethod#2 This control that we desire in the coefficient of h is given by derivative. Derivative of a function with respect to a variable is a pretty easy and straightforward concept.', start_char_idx=1310, end_char_idx=1701, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-1', obj=None),\n",
       " 'b8777353-3331-464b-947a-fe243d5f74ed': IndexNode(id_='b8777353-3331-464b-947a-fe243d5f74ed', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-1', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='86968118853346f5ba3ebb5c892c16926984a3ab5b84bc0804d132d953d72445'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9da36d79-2ea3-46f3-850d-f99a4c223e74', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='06a26936779796e0952e4abb9718fa1382ac69a99a086ee186662455e25e4758')}, text='Let us understand by modifying our default case- Let us increase the value of a by a small quantity h (same as in method 1) and let us give a name to our product function say f(a,b).', start_char_idx=1702, end_char_idx=1884, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-1', obj=None),\n",
       " 'aba5c3be-80ce-42c9-be52-17a7820dc5d2': IndexNode(id_='aba5c3be-80ce-42c9-be52-17a7820dc5d2', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-1', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='86968118853346f5ba3ebb5c892c16926984a3ab5b84bc0804d132d953d72445'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='daca435e-49c1-4da5-8f23-4f8ba0b30f0b', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='3193f2c9caf55b836b2fd1b56010ad4a4814783f31dd70be41abbc876ad17e64')}, text='Aim: Our aim is to increase the output by tweaking values of a and b.\\n\\nMethod#1\\n\\nThe first method is the most obvious one. Let us randomly increase the values of a and b by a small quantity h times a random number:\\n\\ndef product(a, b):\\nreturn a*b\\n\\na = 4\\nb = 3\\nh = 0.01\\na = a + h*(random.random())\\nb = b + h*(random.random())\\nprint(product(a, b))\\n\\nThe output for the above program is 12.042 which is greater than 12 as was our aim. Although our aim is achieved, there are problems:\\n\\n- This is a good strategy for small problems with few nodes but with millions of inputs and thousands of nodes which is easily possible in modern-day networks\\n\\ntowards-backpropagation\\n\\nthis strategy of exhaustive search would be too time consuming. This is an unreliable method to increase value of the function. When we randomly increase the values of a and b, it might result in decrease in value of function.', start_char_idx=0, end_char_idx=892, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-1', obj=None),\n",
       " 'daca435e-49c1-4da5-8f23-4f8ba0b30f0b': IndexNode(id_='daca435e-49c1-4da5-8f23-4f8ba0b30f0b', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-1', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='86968118853346f5ba3ebb5c892c16926984a3ab5b84bc0804d132d953d72445'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='aba5c3be-80ce-42c9-be52-17a7820dc5d2', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='9b59e023f4d137cbcdddabbab03604d20da8508102529a80731f3469feed1fa5')}, text='Have a look at the code below:\\n\\ndef product(a,b):\\nreturn a*b\\na=-4\\nb=-3\\nh=0.01\\na=a+h*(random.random())\\nb=b+h*(random.random())\\nprint(product(a,b))\\n\\nThe above code produces output 11.94 which is less than 12. The only difference was in initial values of a and b. Thus randomly increasing the values of a and b won’t suffice our aim.\\n\\nThus we can conclude that: We need more control over increasing the values of input. Essentially it means that there should be more controlled coefficient of h. The coefficient of h should be function of input or inputs as changing the initial values of input changed the behaviour of output.\\n\\nMethod#2 This control that we desire in the coefficient of h is given by derivative. Derivative of a function with respect to a variable is a pretty easy and straightforward concept. Let us understand by modifying our default case- Let us increase the value of a by a small quantity h (same as in method 1) and let us give a name to our product function say f(a,b).', start_char_idx=893, end_char_idx=1884, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-1', obj=None),\n",
       " 'node-1': IndexNode(id_='node-1', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='89e518c6-c82c-4bca-8f31-51eab31cfff4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='981c221281d663434f9057c4fc300009f52f547887ce66bd384136c12e48527a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ea34b2e2-8126-450c-bf9e-15e17170fe86', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='28eb279999decb7813eaf123aa29d697d9426517f16a5856d92ab3862a9b3311'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c1f7e335-94d2-4df8-8ee4-c8a87c1de10d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b1595fd57495e5ff9b495d23b3d29bb3a963e45f6773a2d492a3ad2cf88a9785')}, text='Aim: Our aim is to increase the output by tweaking values of a and b.\\n\\nMethod#1\\n\\nThe first method is the most obvious one. Let us randomly increase the values of a and b by a small quantity h times a random number:\\n\\ndef product(a, b):\\nreturn a*b\\n\\na = 4\\nb = 3\\nh = 0.01\\na = a + h*(random.random())\\nb = b + h*(random.random())\\nprint(product(a, b))\\n\\nThe output for the above program is 12.042 which is greater than 12 as was our aim. Although our aim is achieved, there are problems:\\n\\n- This is a good strategy for small problems with few nodes but with millions of inputs and thousands of nodes which is easily possible in modern-day networks\\n\\ntowards-backpropagation\\n\\nthis strategy of exhaustive search would be too time consuming. This is an unreliable method to increase value of the function. When we randomly increase the values of a and b, it might result in decrease in value of function. Have a look at the code below:\\n\\ndef product(a,b):\\nreturn a*b\\na=-4\\nb=-3\\nh=0.01\\na=a+h*(random.random())\\nb=b+h*(random.random())\\nprint(product(a,b))\\n\\nThe above code produces output 11.94 which is less than 12. The only difference was in initial values of a and b. Thus randomly increasing the values of a and b won’t suffice our aim.\\n\\nThus we can conclude that: We need more control over increasing the values of input. Essentially it means that there should be more controlled coefficient of h. The coefficient of h should be function of input or inputs as changing the initial values of input changed the behaviour of output.\\n\\nMethod#2 This control that we desire in the coefficient of h is given by derivative. Derivative of a function with respect to a variable is a pretty easy and straightforward concept. Let us understand by modifying our default case- Let us increase the value of a by a small quantity h (same as in method 1) and let us give a name to our product function say f(a,b).', start_char_idx=2282, end_char_idx=4166, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-1', obj=None),\n",
       " 'cdb83ff3-914b-4a1b-a4d3-bec8f315d0ca': IndexNode(id_='cdb83ff3-914b-4a1b-a4d3-bec8f315d0ca', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-2', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='aa888154f6ae393249fe66d30a7f4bf0a4b2d8d11617e65c7da718fa9ee34a00'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='cf60f127-466e-49c5-99c2-7c71a9db81a7', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='13a6b0fe8f8f216b0a5f5c63484425b48311d6db6c5ea115cee6a5f5b10b40cf')}, text='Thus: f(a, b) = a * b\\n\\nSource: https://jasdeep06.github.io/posts/towards-backpropagation/\\n\\nThe new output can be calculated easily as (a+h)*b which can be expanded as a*b+h*b. Thus the output increases by a value of h*b as compared to the default case.', start_char_idx=0, end_char_idx=252, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-2', obj=None),\n",
       " 'cf60f127-466e-49c5-99c2-7c71a9db81a7': IndexNode(id_='cf60f127-466e-49c5-99c2-7c71a9db81a7', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-2', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='aa888154f6ae393249fe66d30a7f4bf0a4b2d8d11617e65c7da718fa9ee34a00'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='cdb83ff3-914b-4a1b-a4d3-bec8f315d0ca', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='a08ab4819ccea2e915af2fc257509c313be22cff51d5109586fb4ed3e2ca46d6'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b945231f-913a-424b-9344-523bf14e6e72', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='28c2ba9025053cb709f8ef36ace0abc753786543ed09af47cdbdfae65f678c27')}, text='We can say that with an increase of h in a, the output increases by h*b, thus with a unit increase in a the output would have increased by b. This normalized effect of an increase in the value of one of the input on output is expressed as the derivative of output with respect to that input. Note that when we take the derivative of a function with respect to a variable, then all other variables are kept constant.', start_char_idx=253, end_char_idx=668, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-2', obj=None),\n",
       " 'b945231f-913a-424b-9344-523bf14e6e72': IndexNode(id_='b945231f-913a-424b-9344-523bf14e6e72', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-2', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='aa888154f6ae393249fe66d30a7f4bf0a4b2d8d11617e65c7da718fa9ee34a00'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='cf60f127-466e-49c5-99c2-7c71a9db81a7', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='7413d29af843ed8023d8bf81c33a8ceebeac6bf4aa449a057c80a555224d405b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='abd84962-cd48-43d0-a0d9-2411db0e757a', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='231e28abefcbe8e672769d5819a11c39204510b236def0f6f4c1b126245da124')}, text='The mathematical interpretation of the derivative of function f with respect to a is defined as:\\n\\n∂f / ∂a = f((a + h), b) - f(a, b) / h\\n\\nThe above formula is nothing but the mathematical interpretation of the definition of the derivative as mentioned above. (For those familiar with multivariate calculus, this is the partial derivative of f with respect to a.)', start_char_idx=669, end_char_idx=1030, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-2', obj=None),\n",
       " 'abd84962-cd48-43d0-a0d9-2411db0e757a': IndexNode(id_='abd84962-cd48-43d0-a0d9-2411db0e757a', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-2', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='aa888154f6ae393249fe66d30a7f4bf0a4b2d8d11617e65c7da718fa9ee34a00'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='b945231f-913a-424b-9344-523bf14e6e72', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='a97d4e56e8cc1e26c11f4d3e07b57ea171a61c7c34bc7908a32e8aa4445c4c38')}, text='Similarly, the derivative of function f with respect to b can be found by increasing b by a small quantity h and normalizing the difference between the final output (a*b+a*h) and the initial output (a*b) that gives us a.\\n\\nThis is the numeric method of finding gradients. It has a significant drawback. Although it is less error-prone, it takes a lot of time to calculate derivatives this way. Time to introduce a new method of finding derivatives: Analytical method.', start_char_idx=1032, end_char_idx=1498, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-2', obj=None),\n",
       " '0ed3231d-9bb8-492b-95aa-ecae13b138de': IndexNode(id_='0ed3231d-9bb8-492b-95aa-ecae13b138de', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-2', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='aa888154f6ae393249fe66d30a7f4bf0a4b2d8d11617e65c7da718fa9ee34a00'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='512c90a1-2526-4440-94e2-23e05c67f765', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='0cab779cc41f25a9189e4772d387db46f54c4eefb49c21b21d1fcc7071d02bc9')}, text='Thus: f(a, b) = a * b\\n\\nSource: https://jasdeep06.github.io/posts/towards-backpropagation/\\n\\nThe new output can be calculated easily as (a+h)*b which can be expanded as a*b+h*b. Thus the output increases by a value of h*b as compared to the default case. We can say that with an increase of h in a, the output increases by h*b, thus with a unit increase in a the output would have increased by b. This normalized effect of an increase in the value of one of the input on output is expressed as the derivative of output with respect to that input. Note that when we take the derivative of a function with respect to a variable, then all other variables are kept constant. The mathematical interpretation of the derivative of function f with respect to a is defined as:\\n\\n∂f / ∂a = f((a + h), b) - f(a, b) / h\\n\\nThe above formula is nothing but the mathematical interpretation of the definition of the derivative as mentioned above.', start_char_idx=0, end_char_idx=926, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-2', obj=None),\n",
       " '512c90a1-2526-4440-94e2-23e05c67f765': IndexNode(id_='512c90a1-2526-4440-94e2-23e05c67f765', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-2', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='aa888154f6ae393249fe66d30a7f4bf0a4b2d8d11617e65c7da718fa9ee34a00'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0ed3231d-9bb8-492b-95aa-ecae13b138de', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='37d314b9826ac680d4aa56971ac34d6335ba5d9cb041fda0f413d7917daa0457')}, text='(For those familiar with multivariate calculus, this is the partial derivative of f with respect to a.)\\n\\nSimilarly, the derivative of function f with respect to b can be found by increasing b by a small quantity h and normalizing the difference between the final output (a*b+a*h) and the initial output (a*b) that gives us a.\\n\\nThis is the numeric method of finding gradients. It has a significant drawback. Although it is less error-prone, it takes a lot of time to calculate derivatives this way. Time to introduce a new method of finding derivatives: Analytical method.', start_char_idx=927, end_char_idx=1498, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-2', obj=None),\n",
       " 'node-2': IndexNode(id_='node-2', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='89e518c6-c82c-4bca-8f31-51eab31cfff4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='981c221281d663434f9057c4fc300009f52f547887ce66bd384136c12e48527a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a133e47c-8e65-434e-bea2-3cca761c8fe0', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='86968118853346f5ba3ebb5c892c16926984a3ab5b84bc0804d132d953d72445'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='12e045d3-280b-49f9-b071-2ba5d60c04fd', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='7ce9228e7448f6e32acd38e63fc537c1e11b50185b5b2f13667eaef71020d967')}, text='Thus: f(a, b) = a * b\\n\\nSource: https://jasdeep06.github.io/posts/towards-backpropagation/\\n\\nThe new output can be calculated easily as (a+h)*b which can be expanded as a*b+h*b. Thus the output increases by a value of h*b as compared to the default case. We can say that with an increase of h in a, the output increases by h*b, thus with a unit increase in a the output would have increased by b. This normalized effect of an increase in the value of one of the input on output is expressed as the derivative of output with respect to that input. Note that when we take the derivative of a function with respect to a variable, then all other variables are kept constant. The mathematical interpretation of the derivative of function f with respect to a is defined as:\\n\\n∂f / ∂a = f((a + h), b) - f(a, b) / h\\n\\nThe above formula is nothing but the mathematical interpretation of the definition of the derivative as mentioned above. (For those familiar with multivariate calculus, this is the partial derivative of f with respect to a.)\\n\\nSimilarly, the derivative of function f with respect to b can be found by increasing b by a small quantity h and normalizing the difference between the final output (a*b+a*h) and the initial output (a*b) that gives us a.\\n\\nThis is the numeric method of finding gradients. It has a significant drawback. Although it is less error-prone, it takes a lot of time to calculate derivatives this way. Time to introduce a new method of finding derivatives: Analytical method.', start_char_idx=4167, end_char_idx=5665, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-2', obj=None),\n",
       " '6f9006a6-d293-4c8b-948f-82f9ba577282': IndexNode(id_='6f9006a6-d293-4c8b-948f-82f9ba577282', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-3', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='2c1e6fa6b4144a924e504f73870cc5c6565e32b58fb690e1bd4543dc40b34f56'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='709335db-2b0e-4a37-b697-984500e67695', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4e330b9556e398a77f970c0b61dc21b0714e0f6f1841feef48bc298c56d7f940')}, text='4/2/24, 1:23 AM towards-backpropagation\\n\\n|Analytical gradient method:|In order to calculate gradient using analytical gradient method we need to remember few basic rules of calculus(Refer Derivative-rules).These rules are derived from the numerical gradient method and are committed to memory so that they can be used directly.This saves us the computation time and space required for calculating derivatives.For f=a*b,', start_char_idx=0, end_char_idx=419, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-3', obj=None),\n",
       " '709335db-2b0e-4a37-b697-984500e67695': IndexNode(id_='709335db-2b0e-4a37-b697-984500e67695', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-3', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='2c1e6fa6b4144a924e504f73870cc5c6565e32b58fb690e1bd4543dc40b34f56'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6f9006a6-d293-4c8b-948f-82f9ba577282', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='96aefdc4954e2fe395d5f329ccbc88b6de8dd734fa60f876182a3d9d5baeaabf'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4455e7d6-9893-4d07-901f-2aa6ad9e62a8', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='6c5d43588f06788b6342554f9fcf78a49c7e849b33cf57e97d5b55e7e20fcf54')}, text='the following can be directly stated-|\\n|---|---|\\n|∂f|= b|\\n| |∂a|\\n|This can easily be derived from the mathematical interpretation of derivative as stated above.', start_char_idx=419, end_char_idx=579, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-3', obj=None),\n",
       " '4455e7d6-9893-4d07-901f-2aa6ad9e62a8': IndexNode(id_='4455e7d6-9893-4d07-901f-2aa6ad9e62a8', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-3', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='2c1e6fa6b4144a924e504f73870cc5c6565e32b58fb690e1bd4543dc40b34f56'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='709335db-2b0e-4a37-b697-984500e67695', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='8e8e0d1c4889f04a9f219d9ee28b627beb4092ac03a25b70b7ec59b559407c1a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b80e1c54-1dfc-4b3b-bc42-da7461b55451', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='26b475b2adc9fec1f63420d85142ae27275c17aa46fca81606abc8e0cbad0859')}, text='Putting values in interpretation-| |\\n|∂f|= (a + h) * b - a * b|\\n|∂a|h|\\n|∂f|= (a * b + h * b) - a * b|\\n|∂a|h|\\n|∂f|= b|\\n| |∂a|\\n|Similarly one can derive:| |\\n|∂f|= a|\\n| |∂b|\\n|With these two derivatives in hand we have our coefficients of h in a-update and b-update.', start_char_idx=579, end_char_idx=841, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-3', obj=None),\n",
       " 'b80e1c54-1dfc-4b3b-bc42-da7461b55451': IndexNode(id_='b80e1c54-1dfc-4b3b-bc42-da7461b55451', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-3', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='2c1e6fa6b4144a924e504f73870cc5c6565e32b58fb690e1bd4543dc40b34f56'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4455e7d6-9893-4d07-901f-2aa6ad9e62a8', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='8111e920a196c3e96fe89ad4cfc5f3f17b352e393d1ffca83d211f0f97850456'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='1c9eed80-1ef7-47ab-84e0-2d1ca1ff3464', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='efdf97d4ce510e2062718e48d60756bdba06d5684d17756aabe537a39718bedb')}, text='So our modified update rules will be-| |\\n|a = a + h * ∂f|a = a + h * b|\\n| |∂a|\\n|b = b + h * ∂f|b = b + h * a|\\n| |∂b|\\n\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 5/9\\n\\n4/2/24, 1:23 AM towards-backpropagation\\n\\nThe modified code will be:\\n\\ndef product(a,', start_char_idx=841, end_char_idx=1106, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-3', obj=None),\n",
       " '1c9eed80-1ef7-47ab-84e0-2d1ca1ff3464': IndexNode(id_='1c9eed80-1ef7-47ab-84e0-2d1ca1ff3464', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-3', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='2c1e6fa6b4144a924e504f73870cc5c6565e32b58fb690e1bd4543dc40b34f56'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='b80e1c54-1dfc-4b3b-bc42-da7461b55451', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='0b5a80053b662a35b34577ab393082e9f40818f821010dd04ad195325ef3b6ab'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9579ffde-756a-4b17-89a5-b3f3a241fda9', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='3141a4fe8728e6ed9a8c8937693491ee12940420b81ff1dc30f051eeff8a500f')}, text='b):\\n\\nreturn a*b\\n\\na=-4\\n\\nb=-3\\n\\nh=0.01\\n\\na=a+h*b\\n\\nb=b+h*a\\n\\nprint(product(a,b))\\n\\nThe output of above code is 12.252 which is greater than 12 as was our aim.\\n\\nWhy does this approach work?\\n\\nTo appreciate the beauty of derivative in updates of inputs a and b we have to dive into geometrical interpretation of derivative.', start_char_idx=1106, end_char_idx=1419, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-3', obj=None),\n",
       " '9579ffde-756a-4b17-89a5-b3f3a241fda9': IndexNode(id_='9579ffde-756a-4b17-89a5-b3f3a241fda9', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-3', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='2c1e6fa6b4144a924e504f73870cc5c6565e32b58fb690e1bd4543dc40b34f56'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1c9eed80-1ef7-47ab-84e0-2d1ca1ff3464', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='cd5fb6505de60adf446af69365c42910b439e0dba9ec2ee516da6a6111387f23')}, text='Geometrically, derivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable.', start_char_idx=1420, end_char_idx=1566, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-3', obj=None),\n",
       " '03315ea2-cf2c-470c-96d9-cbe494194496': IndexNode(id_='03315ea2-cf2c-470c-96d9-cbe494194496', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-3', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='2c1e6fa6b4144a924e504f73870cc5c6565e32b58fb690e1bd4543dc40b34f56'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6bc6de98-3aca-49a5-b659-0cf1008b4e0c', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='99d00c244bd187ac401dd605ac0fd27fc7bff9d1ed0ad06492a73e1fa461e2f8')}, text='4/2/24, 1:23 AM towards-backpropagation\\n\\n|Analytical gradient method:|In order to calculate gradient using analytical gradient method we need to remember few basic rules of calculus(Refer Derivative-rules).These rules are derived from the numerical gradient method and are committed to memory so that they can be used directly.This saves us the computation time and space required for calculating derivatives.For f=a*b,the following can be directly stated-|\\n|---|---|\\n|∂f|= b|\\n| |∂a|\\n|This can easily be derived from the mathematical interpretation of derivative as stated above.Putting values in interpretation-| |\\n|∂f|= (a + h) * b - a * b|\\n|∂a|h|\\n|∂f|= (a * b + h * b) - a * b|\\n|∂a|h|\\n|∂f|= b|\\n| |∂a|\\n|Similarly one can derive:| |\\n|∂f|= a|\\n| |∂b|\\n|With these two derivatives in hand we have our coefficients of h in a-update and b-update.', start_char_idx=0, end_char_idx=841, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-3', obj=None),\n",
       " '6bc6de98-3aca-49a5-b659-0cf1008b4e0c': IndexNode(id_='6bc6de98-3aca-49a5-b659-0cf1008b4e0c', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-3', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='2c1e6fa6b4144a924e504f73870cc5c6565e32b58fb690e1bd4543dc40b34f56'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='03315ea2-cf2c-470c-96d9-cbe494194496', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='8d11d8cd3e69c0aeaffbf9d439b43c570c140daaa8687f717a83f6cbe46b1c06')}, text='So our modified update rules will be-| |\\n|a = a + h * ∂f|a = a + h * b|\\n| |∂a|\\n|b = b + h * ∂f|b = b + h * a|\\n| |∂b|\\n\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 5/9\\n\\n4/2/24, 1:23 AM towards-backpropagation\\n\\nThe modified code will be:\\n\\ndef product(a,b):\\n\\nreturn a*b\\n\\na=-4\\n\\nb=-3\\n\\nh=0.01\\n\\na=a+h*b\\n\\nb=b+h*a\\n\\nprint(product(a,b))\\n\\nThe output of above code is 12.252 which is greater than 12 as was our aim.\\n\\nWhy does this approach work?\\n\\nTo appreciate the beauty of derivative in updates of inputs a and b we have to dive into geometrical interpretation of derivative. Geometrically, derivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable.', start_char_idx=841, end_char_idx=1566, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-3', obj=None),\n",
       " 'node-3': IndexNode(id_='node-3', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='89e518c6-c82c-4bca-8f31-51eab31cfff4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='981c221281d663434f9057c4fc300009f52f547887ce66bd384136c12e48527a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c1f7e335-94d2-4df8-8ee4-c8a87c1de10d', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='aa888154f6ae393249fe66d30a7f4bf0a4b2d8d11617e65c7da718fa9ee34a00'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ac3aacdb-954b-4a69-b2d5-637a203188a7', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b89ed3f806ed4bafb574b4ddbf50029920040db78e131570d0102f105d1a0417')}, text='4/2/24, 1:23 AM towards-backpropagation\\n\\n|Analytical gradient method:|In order to calculate gradient using analytical gradient method we need to remember few basic rules of calculus(Refer Derivative-rules).These rules are derived from the numerical gradient method and are committed to memory so that they can be used directly.This saves us the computation time and space required for calculating derivatives.For f=a*b,the following can be directly stated-|\\n|---|---|\\n|∂f|= b|\\n| |∂a|\\n|This can easily be derived from the mathematical interpretation of derivative as stated above.Putting values in interpretation-| |\\n|∂f|= (a + h) * b - a * b|\\n|∂a|h|\\n|∂f|= (a * b + h * b) - a * b|\\n|∂a|h|\\n|∂f|= b|\\n| |∂a|\\n|Similarly one can derive:| |\\n|∂f|= a|\\n| |∂b|\\n|With these two derivatives in hand we have our coefficients of h in a-update and b-update.So our modified update rules will be-| |\\n|a = a + h * ∂f|a = a + h * b|\\n| |∂a|\\n|b = b + h * ∂f|b = b + h * a|\\n| |∂b|\\n\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 5/9\\n\\n4/2/24, 1:23 AM towards-backpropagation\\n\\nThe modified code will be:\\n\\ndef product(a,b):\\n\\nreturn a*b\\n\\na=-4\\n\\nb=-3\\n\\nh=0.01\\n\\na=a+h*b\\n\\nb=b+h*a\\n\\nprint(product(a,b))\\n\\nThe output of above code is 12.252 which is greater than 12 as was our aim.\\n\\nWhy does this approach work?\\n\\nTo appreciate the beauty of derivative in updates of inputs a and b we have to dive into geometrical interpretation of derivative. Geometrically, derivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable.', start_char_idx=5667, end_char_idx=7233, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-3', obj=None),\n",
       " 'ac79626a-5c27-42c3-b97a-9a9a2900a071': IndexNode(id_='ac79626a-5c27-42c3-b97a-9a9a2900a071', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-4', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='a478f01f162652f13d45ef4992d0910bda6d5d251e9959e7e06ffa3647355471'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='45d5b893-c3df-47fa-b2a2-7454a4f00038', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='a0c92408ff118fa22398132fb4e4331cce9cad15ea901fcd1c9f90bc330149dc')}, text='While reading ahead keep in mind those update rules given by-\\n\\na = a + h ∗ ∂f\\n\\n∂a\\n\\nb = b + h ∗ ∂f\\n\\n∂b\\n\\nDerivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable keeping all other variables constant.', start_char_idx=0, end_char_idx=271, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-4', obj=None),\n",
       " '45d5b893-c3df-47fa-b2a2-7454a4f00038': IndexNode(id_='45d5b893-c3df-47fa-b2a2-7454a4f00038', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-4', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='a478f01f162652f13d45ef4992d0910bda6d5d251e9959e7e06ffa3647355471'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ac79626a-5c27-42c3-b97a-9a9a2900a071', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='1ea63db67ed14ef2e2511685feaf3b26344e9bff321915f3a9866b134d0effb8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='41026cbb-c790-4324-a19f-75896d5eab8a', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='9e3a787eb6bdd6fc8f1099c6db4281d242a80b8671827c06d818ffa1a7620542')}, text='Let us understand the effect of derivative in update rules in a single-variable system to get an intuition of what is happening- Link to Source\\n\\nAbove figure represents a function in single variable x. As x increases (from left to right) the function increases till point B and then decreases as we further increase x. Let us imagine two cases-\\n\\nCase#1\\n\\nImagine we are at point A as shown in the figure and we want to change the value of x in such a way that the value of the function increases.', start_char_idx=273, end_char_idx=768, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-4', obj=None),\n",
       " '41026cbb-c790-4324-a19f-75896d5eab8a': IndexNode(id_='41026cbb-c790-4324-a19f-75896d5eab8a', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-4', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='a478f01f162652f13d45ef4992d0910bda6d5d251e9959e7e06ffa3647355471'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='45d5b893-c3df-47fa-b2a2-7454a4f00038', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='5da2f8878edcf51f81103742a5eefe78f0778cfbe48e741f469e558ca2fd6921'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6d03965b-9f8c-42b8-994a-5b0015869658', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='04ceac0512a46ec07c2d601e7bebefd0a047e1a4f38516d38105b191fb902d00')}, text='From the figure, it is clear that if we increase the value of x, the value of the function increases. Now let us take a look at our update rule and see how it comes to this conclusion-\\n\\nx = x + h * dy/dx\\n\\nWe know that our h > 0 so whether x increases or decreases depends upon the derivative of the function with respect to x. Recall that the derivative is nothing but the rate of change of the function.', start_char_idx=769, end_char_idx=1173, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-4', obj=None),\n",
       " '6d03965b-9f8c-42b8-994a-5b0015869658': IndexNode(id_='6d03965b-9f8c-42b8-994a-5b0015869658', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-4', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='a478f01f162652f13d45ef4992d0910bda6d5d251e9959e7e06ffa3647355471'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='41026cbb-c790-4324-a19f-75896d5eab8a', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='c99c3870f83548ab3e4f12a88f3d3a1f528266a6f70391f8b12661c9f3fe41bc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c8dd3431-656a-49b9-b4d9-ee85095a424b', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='6bac41008c0f0eb3d086c7da5f7e8943e87da9d0e127e7de22398a6e8f13d40d')}, text='At point A, we can see that the function increases as x increases so the rate of change of the function with respect to x at A is positive. This means that at point A the derivative of the function with respect to x is positive. A positive derivative will make x increase through the update rule.\\n\\nCase#2\\n\\nImagine that we are at point C as shown in the figure.', start_char_idx=1174, end_char_idx=1534, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-4', obj=None),\n",
       " 'c8dd3431-656a-49b9-b4d9-ee85095a424b': IndexNode(id_='c8dd3431-656a-49b9-b4d9-ee85095a424b', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-4', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='a478f01f162652f13d45ef4992d0910bda6d5d251e9959e7e06ffa3647355471'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6d03965b-9f8c-42b8-994a-5b0015869658', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='4ed1b5fccaf89bb273e38e50d381297f23cd6b421c24af9fb580ce022870358a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2962f68d-3b5f-479e-91a5-d64136b66c29', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='a5e625cddd8561d730ba38d0da2a4a67302814f73450728db5b532299131ad94')}, text='Everything remains the same as case#1 but now we can see from the figure that the value of the function increases with a decrease in the value of x. At this time the derivative of the function with respect to x will be negative as\\n\\nSource\\n\\n4/2/24, 1:23 AM towards-backpropagation value of function decreases as x increases(i.e. the rate of change of function with x is negative).When we put a negative derivative in our update rule the value of x decreases as expected.', start_char_idx=1535, end_char_idx=2004, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-4', obj=None),\n",
       " '2962f68d-3b5f-479e-91a5-d64136b66c29': IndexNode(id_='2962f68d-3b5f-479e-91a5-d64136b66c29', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-4', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='a478f01f162652f13d45ef4992d0910bda6d5d251e9959e7e06ffa3647355471'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c8dd3431-656a-49b9-b4d9-ee85095a424b', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='90afc08565d90644456a0cf2a7de8463378d97695142c9f737394392f7d5e5a1')}, text='Thus the derivative captures the nature of our function and gives our update rule a sense of direction to obtain a higher value.', start_char_idx=2005, end_char_idx=2133, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-4', obj=None),\n",
       " '702cde99-34e6-4727-a635-fbe58155abe0': IndexNode(id_='702cde99-34e6-4727-a635-fbe58155abe0', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-4', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='a478f01f162652f13d45ef4992d0910bda6d5d251e9959e7e06ffa3647355471'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4f723251-8ee0-4373-9731-11bb5bb4fe26', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4b09a6b7cec3ffb435bb8d20dbeb6c1f19ff9bf5bc071416b29e7ca63f5cac8c')}, text='While reading ahead keep in mind those update rules given by-\\n\\na = a + h ∗ ∂f\\n\\n∂a\\n\\nb = b + h ∗ ∂f\\n\\n∂b\\n\\nDerivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable keeping all other variables constant.\\n\\nLet us understand the effect of derivative in update rules in a single-variable system to get an intuition of what is happening- Link to Source\\n\\nAbove figure represents a function in single variable x. As x increases (from left to right) the function increases till point B and then decreases as we further increase x. Let us imagine two cases-\\n\\nCase#1\\n\\nImagine we are at point A as shown in the figure and we want to change the value of x in such a way that the value of the function increases. From the figure, it is clear that if we increase the value of x, the value of the function increases.', start_char_idx=0, end_char_idx=870, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-4', obj=None),\n",
       " '4f723251-8ee0-4373-9731-11bb5bb4fe26': IndexNode(id_='4f723251-8ee0-4373-9731-11bb5bb4fe26', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-4', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='a478f01f162652f13d45ef4992d0910bda6d5d251e9959e7e06ffa3647355471'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='702cde99-34e6-4727-a635-fbe58155abe0', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='b0ac82dd301af27a39b06ae22cfa1fc0a7be9d863e23753a566f0a1600fe3164'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d94a0cdc-d188-4005-8f74-ca6f1bc98d09', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='9bd4e56e998c90e479211ba07507a4b7c4acf3096a34e80038bd64fde52c7ea6')}, text='Now let us take a look at our update rule and see how it comes to this conclusion-\\n\\nx = x + h * dy/dx\\n\\nWe know that our h > 0 so whether x increases or decreases depends upon the derivative of the function with respect to x. Recall that the derivative is nothing but the rate of change of the function. At point A, we can see that the function increases as x increases so the rate of change of the function with respect to x at A is positive. This means that at point A the derivative of the function with respect to x is positive. A positive derivative will make x increase through the update rule.\\n\\nCase#2\\n\\nImagine that we are at point C as shown in the figure. Everything remains the same as case#1 but now we can see from the figure that the value of the function increases with a decrease in the value of x. At this time the derivative of the function with respect to x will be negative as\\n\\nSource\\n\\n4/2/24, 1:23 AM towards-backpropagation value of function decreases as x increases(i.e.', start_char_idx=871, end_char_idx=1862, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-4', obj=None),\n",
       " 'd94a0cdc-d188-4005-8f74-ca6f1bc98d09': IndexNode(id_='d94a0cdc-d188-4005-8f74-ca6f1bc98d09', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-4', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='a478f01f162652f13d45ef4992d0910bda6d5d251e9959e7e06ffa3647355471'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4f723251-8ee0-4373-9731-11bb5bb4fe26', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='533bc4555bf2b1ae844aaa76add59bebae90c7db8c4c6f0d68c3c81a1ec53f85')}, text='the rate of change of function with x is negative).When we put a negative derivative in our update rule the value of x decreases as expected. Thus the derivative captures the nature of our function and gives our update rule a sense of direction to obtain a higher value.', start_char_idx=1863, end_char_idx=2133, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-4', obj=None),\n",
       " 'node-4': IndexNode(id_='node-4', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='89e518c6-c82c-4bca-8f31-51eab31cfff4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='981c221281d663434f9057c4fc300009f52f547887ce66bd384136c12e48527a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='12e045d3-280b-49f9-b071-2ba5d60c04fd', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='2c1e6fa6b4144a924e504f73870cc5c6565e32b58fb690e1bd4543dc40b34f56'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3bf9cc88-f1fd-4559-a843-539602fa66b2', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='bf911d285723f58edf2cc0182db80cb79597ef6a4ca4294cc816919381c67638')}, text='While reading ahead keep in mind those update rules given by-\\n\\na = a + h ∗ ∂f\\n\\n∂a\\n\\nb = b + h ∗ ∂f\\n\\n∂b\\n\\nDerivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable keeping all other variables constant.\\n\\nLet us understand the effect of derivative in update rules in a single-variable system to get an intuition of what is happening- Link to Source\\n\\nAbove figure represents a function in single variable x. As x increases (from left to right) the function increases till point B and then decreases as we further increase x. Let us imagine two cases-\\n\\nCase#1\\n\\nImagine we are at point A as shown in the figure and we want to change the value of x in such a way that the value of the function increases. From the figure, it is clear that if we increase the value of x, the value of the function increases. Now let us take a look at our update rule and see how it comes to this conclusion-\\n\\nx = x + h * dy/dx\\n\\nWe know that our h > 0 so whether x increases or decreases depends upon the derivative of the function with respect to x. Recall that the derivative is nothing but the rate of change of the function. At point A, we can see that the function increases as x increases so the rate of change of the function with respect to x at A is positive. This means that at point A the derivative of the function with respect to x is positive. A positive derivative will make x increase through the update rule.\\n\\nCase#2\\n\\nImagine that we are at point C as shown in the figure. Everything remains the same as case#1 but now we can see from the figure that the value of the function increases with a decrease in the value of x. At this time the derivative of the function with respect to x will be negative as\\n\\nSource\\n\\n4/2/24, 1:23 AM towards-backpropagation value of function decreases as x increases(i.e. the rate of change of function with x is negative).When we put a negative derivative in our update rule the value of x decreases as expected. Thus the derivative captures the nature of our function and gives our update rule a sense of direction to obtain a higher value.', start_char_idx=7234, end_char_idx=9367, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-4', obj=None),\n",
       " '1b82aacb-2389-48e5-8e2d-c5321af31af2': IndexNode(id_='1b82aacb-2389-48e5-8e2d-c5321af31af2', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-5', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='7925851846ffd929b8bab5dbb85e4a90918c64f729f39a58123b0ab626513500'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d7c6075e-f378-434a-bd05-c0da90c6e4d2', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='8a84ec76b6496639109900aec5e75c88d4b0bdd720b471ce000c9d647d10f0f1')}, text='Case#3 Imagine that we are somewhere pretty close to point B(say we are on the left of it but pretty close to it).The value of derivative will be positive and the update rule would want to increase the value of xin order to increase the value of the function.But a condition could arise when value of h (stepsize) is sufficiently large that it overshoots the point B and thus decreasing the value of our function.Then we would have to adjust value of our stepsize(make it small of course!)', start_char_idx=0, end_char_idx=489, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-5', obj=None),\n",
       " 'd7c6075e-f378-434a-bd05-c0da90c6e4d2': IndexNode(id_='d7c6075e-f378-434a-bd05-c0da90c6e4d2', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-5', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='7925851846ffd929b8bab5dbb85e4a90918c64f729f39a58123b0ab626513500'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1b82aacb-2389-48e5-8e2d-c5321af31af2', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='9152206323290516fd1dcb898567678457fb9725275e0fc90ffae57d8ffd0db8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e478772b-9af0-44f2-85d3-c3ea9b09daef', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='75fcc6ed0c3a072c4109a7c8fe88d10d9bbb14436a5372ecfcb50803339172fe')}, text='in order to increase value of our function.Thus this approach of increasing value of inputs in direction of derivative would give us desired result for almost all functions you would encounter.(It fails for some poorly defined convex functions, but for now you do not need to worry about them.) I will stop this post here.I hope that this post helped you develop an intuition about derivatives and their involvement in update rule.In the next post we will apply these principals in nested nodes and will finally see backpropagation. Posted on 12 January,2017 ALSO ON', start_char_idx=490, end_char_idx=1056, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-5', obj=None),\n",
       " 'e478772b-9af0-44f2-85d3-c3ea9b09daef': IndexNode(id_='e478772b-9af0-44f2-85d3-c3ea9b09daef', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-5', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='7925851846ffd929b8bab5dbb85e4a90918c64f729f39a58123b0ab626513500'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='d7c6075e-f378-434a-bd05-c0da90c6e4d2', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='0b1b4ac91204b43e52eb0be847274ab400568902b90c23e2b32f6acd2409623b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6c776ddb-3c25-4055-ae23-8e4a476e06e0', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='566e21e3add012f8785be6a159eed1b6e95cc48260fabb63634f846587b6bae5')}, text='JASDEEP06 Further-Into- Understanding LSTA In Variable sharing-Ins Getting st4 Further-into- Understanding LSTM in Variable-sharing-in- Getting sta Further-into- Understanding LSTM in Variable-sharing-in- Getting sta backpropaqation Tensorflov Tensorflom Tensorflov backpropagation Tensorflow Tensorflow Tensorflow backpropagation Tensorflow Tensorflow Tensorflow 7 years ago • 2 comments 7 years ago • 32 comments 7 years ago • 14 comments 7 years ago • Backpropagation : Further', start_char_idx=1057, end_char_idx=1537, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-5', obj=None),\n",
       " '6c776ddb-3c25-4055-ae23-8e4a476e06e0': IndexNode(id_='6c776ddb-3c25-4055-ae23-8e4a476e06e0', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-5', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='7925851846ffd929b8bab5dbb85e4a90918c64f729f39a58123b0ab626513500'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e478772b-9af0-44f2-85d3-c3ea9b09daef', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='1921202d9abbf7846bacc17c3c5c4674b70fac1f36c0ea8508ef20de2950bda8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='cf63eacd-bb9d-4a7e-a0c8-3ff1d447181b', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='25c60574908c3f09cafcb1ba892bab0393f90c799bc0ae0418adec40bd2481eb')}, text='CNNs in Tensorflow(cifar- 10) Tensorflow: Variable sharing Tensorflow : with Tensorfl\\n\\nhttps://jasdeep06.gipub.io/posts/towards-backpropagation/\\n8/9\\n\\n# 4/2/24, 1:23 AM\\n\\n# towards-backpropagation\\n\\n0 Comments\\n\\nJasdeep Singh Chhabra\\n\\nStart the discussion.\\n\\nShare\\n\\nBest | Newest | Oldest\\n\\nBe the first to comment.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-5', obj=None),\n",
       " 'cf63eacd-bb9d-4a7e-a0c8-3ff1d447181b': IndexNode(id_='cf63eacd-bb9d-4a7e-a0c8-3ff1d447181b', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-5', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='7925851846ffd929b8bab5dbb85e4a90918c64f729f39a58123b0ab626513500'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6c776ddb-3c25-4055-ae23-8e4a476e06e0', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='919c49a65aeb2555ac935110b4dcf80a6ad416d48712530c67250041527718df')}, text='Subscribe | Privacy | Do Not Sell My Data\\n\\nBackpropagation maintained by jasdeep06\\n\\nLink to the article', start_char_idx=1851, end_char_idx=1954, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-5', obj=None),\n",
       " '45520e4b-401c-45b7-8714-d4c2cdc0d99e': IndexNode(id_='45520e4b-401c-45b7-8714-d4c2cdc0d99e', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-5', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='7925851846ffd929b8bab5dbb85e4a90918c64f729f39a58123b0ab626513500'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ce00576a-1019-4023-9f10-bc62920cbf05', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='d667aa87a66601b9fe02866526a2604ec9719e121adcb140b00d3d125494d318')}, text='Case#3 Imagine that we are somewhere pretty close to point B(say we are on the left of it but pretty close to it).The value of derivative will be positive and the update rule would want to increase the value of xin order to increase the value of the function.But a condition could arise when value of h (stepsize) is sufficiently large that it overshoots the point B and thus decreasing the value of our function.Then we would have to adjust value of our stepsize(make it small of course!) in order to increase value of our function.Thus this approach of increasing value of inputs in direction of derivative would give us desired result for almost all functions you would encounter.(It fails for some poorly defined convex functions, but for now you do not need to worry about them.) I will stop this post here.I hope that this post helped you develop an intuition about derivatives and their involvement in update rule.In the next post we will apply these principals in nested nodes and will finally see backpropagation.', start_char_idx=0, end_char_idx=1022, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-5', obj=None),\n",
       " 'ce00576a-1019-4023-9f10-bc62920cbf05': IndexNode(id_='ce00576a-1019-4023-9f10-bc62920cbf05', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-5', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='7925851846ffd929b8bab5dbb85e4a90918c64f729f39a58123b0ab626513500'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='45520e4b-401c-45b7-8714-d4c2cdc0d99e', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='d80113a3fe76603bb73ea3401c770669844ea06343cb2bb551d9920e3d79b039')}, text='Posted on 12 January,2017 ALSO ON JASDEEP06 Further-Into- Understanding LSTA In Variable sharing-Ins Getting st4 Further-into- Understanding LSTM in Variable-sharing-in- Getting sta Further-into- Understanding LSTM in Variable-sharing-in- Getting sta backpropaqation Tensorflov Tensorflom Tensorflov backpropagation Tensorflow Tensorflow Tensorflow backpropagation Tensorflow Tensorflow Tensorflow 7 years ago • 2 comments 7 years ago • 32 comments 7 years ago • 14 comments 7 years ago • Backpropagation : Further CNNs in Tensorflow(cifar- 10) Tensorflow: Variable sharing Tensorflow : with Tensorfl\\n\\nhttps://jasdeep06.gipub.io/posts/towards-backpropagation/\\n8/9\\n\\n# 4/2/24, 1:23 AM\\n\\n# towards-backpropagation\\n\\n0 Comments\\n\\nJasdeep Singh Chhabra\\n\\nStart the discussion...\\n\\nShare\\n\\nBest | Newest | Oldest\\n\\nBe the first to comment.\\n\\nSubscribe | Privacy | Do Not Sell My Data\\n\\nBackpropagation maintained by jasdeep06\\n\\nLink to the article', start_char_idx=1023, end_char_idx=1954, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-5', obj=None),\n",
       " 'node-5': IndexNode(id_='node-5', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='89e518c6-c82c-4bca-8f31-51eab31cfff4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='981c221281d663434f9057c4fc300009f52f547887ce66bd384136c12e48527a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ac3aacdb-954b-4a69-b2d5-637a203188a7', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='a478f01f162652f13d45ef4992d0910bda6d5d251e9959e7e06ffa3647355471')}, text='Case#3 Imagine that we are somewhere pretty close to point B(say we are on the left of it but pretty close to it).The value of derivative will be positive and the update rule would want to increase the value of xin order to increase the value of the function.But a condition could arise when value of h (stepsize) is sufficiently large that it overshoots the point B and thus decreasing the value of our function.Then we would have to adjust value of our stepsize(make it small of course!) in order to increase value of our function.Thus this approach of increasing value of inputs in direction of derivative would give us desired result for almost all functions you would encounter.(It fails for some poorly defined convex functions, but for now you do not need to worry about them.) I will stop this post here.I hope that this post helped you develop an intuition about derivatives and their involvement in update rule.In the next post we will apply these principals in nested nodes and will finally see backpropagation. Posted on 12 January,2017 ALSO ON JASDEEP06 Further-Into- Understanding LSTA In Variable sharing-Ins Getting st4 Further-into- Understanding LSTM in Variable-sharing-in- Getting sta Further-into- Understanding LSTM in Variable-sharing-in- Getting sta backpropaqation Tensorflov Tensorflom Tensorflov backpropagation Tensorflow Tensorflow Tensorflow backpropagation Tensorflow Tensorflow Tensorflow 7 years ago • 2 comments 7 years ago • 32 comments 7 years ago • 14 comments 7 years ago • Backpropagation : Further CNNs in Tensorflow(cifar- 10) Tensorflow: Variable sharing Tensorflow : with Tensorfl\\n\\nhttps://jasdeep06.gipub.io/posts/towards-backpropagation/\\n8/9\\n\\n# 4/2/24, 1:23 AM\\n\\n# towards-backpropagation\\n\\n0 Comments\\n\\nJasdeep Singh Chhabra\\n\\nStart the discussion...\\n\\nShare\\n\\nBest | Newest | Oldest\\n\\nBe the first to comment.\\n\\nSubscribe | Privacy | Do Not Sell My Data\\n\\nBackpropagation maintained by jasdeep06\\n\\nLink to the article', start_char_idx=9368, end_char_idx=11322, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-5', obj=None)}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.schema import IndexNode\n",
    "\n",
    "\n",
    "chunk_sizes = [128,256]\n",
    "parsers = [SentenceSplitter(chunk_size = c,chunk_overlap=0) for c in chunk_sizes]\n",
    "\n",
    "all_nodes = []\n",
    "for base_node in base_nodes:\n",
    "    for parser in parsers:\n",
    "        sub_nodes = parser.get_nodes_from_documents([base_node])\n",
    "        sub_inodes = [\n",
    "            IndexNode.from_text_node(sn,index_id=base_node.node_id) for sn in sub_nodes\n",
    "        ]\n",
    "        all_nodes.extend(sub_inodes)\n",
    "\n",
    "    original_node = IndexNode.from_text_node(base_node,index_id = base_node.node_id)\n",
    "    all_nodes.append(original_node)\n",
    "\n",
    "all_nodes_dict = {n.node_id: n for n in all_nodes}\n",
    "all_nodes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5d49276c-2217-4754-859a-8ddcbef612a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1346bb58-4bde-4a37-88e1-c0687d5fb02f', '02bc84c2-2c5d-46f6-8a1b-099a1f3a689b', '4ad806e6-5c58-4eff-af1c-e7c8477aaa62', '65d8f653-3080-4ca6-bf73-90f66cd1836e', 'bd0b6312-7f90-41a8-887b-002b72c6b184', '58cccae5-3576-4522-8553-0644306af8df', '1e49b818-5b7c-47e3-82b7-e8ddd1878a74', 'd43af22b-e1d8-403e-8700-1660a9030aaa', 'node-0', 'e76591eb-782b-4f59-8217-c1774023b4b5', '48736dc1-9cd3-4813-ac32-96c58f27747e', '5a16f616-f78c-4327-bd7c-8c816b1507d2', '9859b2e7-14bc-4915-b639-ef63fa5ac083', '9da36d79-2ea3-46f3-850d-f99a4c223e74', 'b8777353-3331-464b-947a-fe243d5f74ed', 'aba5c3be-80ce-42c9-be52-17a7820dc5d2', 'daca435e-49c1-4da5-8f23-4f8ba0b30f0b', 'node-1', 'cdb83ff3-914b-4a1b-a4d3-bec8f315d0ca', 'cf60f127-466e-49c5-99c2-7c71a9db81a7', 'b945231f-913a-424b-9344-523bf14e6e72', 'abd84962-cd48-43d0-a0d9-2411db0e757a', '0ed3231d-9bb8-492b-95aa-ecae13b138de', '512c90a1-2526-4440-94e2-23e05c67f765', 'node-2', '6f9006a6-d293-4c8b-948f-82f9ba577282', '709335db-2b0e-4a37-b697-984500e67695', '4455e7d6-9893-4d07-901f-2aa6ad9e62a8', 'b80e1c54-1dfc-4b3b-bc42-da7461b55451', '1c9eed80-1ef7-47ab-84e0-2d1ca1ff3464', '9579ffde-756a-4b17-89a5-b3f3a241fda9', '03315ea2-cf2c-470c-96d9-cbe494194496', '6bc6de98-3aca-49a5-b659-0cf1008b4e0c', 'node-3', 'ac79626a-5c27-42c3-b97a-9a9a2900a071', '45d5b893-c3df-47fa-b2a2-7454a4f00038', '41026cbb-c790-4324-a19f-75896d5eab8a', '6d03965b-9f8c-42b8-994a-5b0015869658', 'c8dd3431-656a-49b9-b4d9-ee85095a424b', '2962f68d-3b5f-479e-91a5-d64136b66c29', '702cde99-34e6-4727-a635-fbe58155abe0', '4f723251-8ee0-4373-9731-11bb5bb4fe26', 'd94a0cdc-d188-4005-8f74-ca6f1bc98d09', 'node-4', '1b82aacb-2389-48e5-8e2d-c5321af31af2', 'd7c6075e-f378-434a-bd05-c0da90c6e4d2', 'e478772b-9af0-44f2-85d3-c3ea9b09daef', '6c776ddb-3c25-4055-ae23-8e4a476e06e0', 'cf63eacd-bb9d-4a7e-a0c8-3ff1d447181b', '45520e4b-401c-45b7-8714-d4c2cdc0d99e', 'ce00576a-1019-4023-9f10-bc62920cbf05', 'node-5'])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nodes_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5c8cd8e9-7f56-4fae-89f7-886f3c4bb0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Image for page 1: []\n",
      "> Image for page 2: [{'name': 'page-2-0.jpg', 'height': 213, 'width': 341, 'x': 33, 'y': 99, 'path': 'images\\\\3cbbc98e-0ce1-4635-b102-1451102c91af-page-2-0.jpg', 'job_id': '3cbbc98e-0ce1-4635-b102-1451102c91af', 'original_pdf_path': 'towards-backpropagation.pdf', 'page_number': 2}]\n",
      "> Image for page 3: []\n",
      "> Image for page 4: [{'name': 'page-4-0.jpg', 'height': 201, 'width': 357, 'x': 33, 'y': 27, 'path': 'images\\\\3cbbc98e-0ce1-4635-b102-1451102c91af-page-4-0.jpg', 'job_id': '3cbbc98e-0ce1-4635-b102-1451102c91af', 'original_pdf_path': 'towards-backpropagation.pdf', 'page_number': 4}]\n",
      "> Image for page 5: []\n",
      "> Image for page 6: []\n",
      "> Image for page 7: [{'name': 'page-7-0.jpg', 'height': 211, 'width': 357, 'x': 33, 'y': 27, 'path': 'images\\\\3cbbc98e-0ce1-4635-b102-1451102c91af-page-7-0.jpg', 'job_id': '3cbbc98e-0ce1-4635-b102-1451102c91af', 'original_pdf_path': 'towards-backpropagation.pdf', 'page_number': 7}]\n",
      "> Image for page 8: [{'name': 'page-8-0.jpg', 'height': 76, 'width': 149, 'x': 40, 'y': 559, 'path': 'images\\\\3cbbc98e-0ce1-4635-b102-1451102c91af-page-8-0.jpg', 'job_id': '3cbbc98e-0ce1-4635-b102-1451102c91af', 'original_pdf_path': 'towards-backpropagation.pdf', 'page_number': 8}, {'name': 'page-8-2.jpg', 'height': 76, 'width': 149, 'x': 352, 'y': 559, 'path': 'images\\\\3cbbc98e-0ce1-4635-b102-1451102c91af-page-8-2.jpg', 'job_id': '3cbbc98e-0ce1-4635-b102-1451102c91af', 'original_pdf_path': 'towards-backpropagation.pdf', 'page_number': 8}, {'name': 'page-8-3.jpg', 'height': 76, 'width': 149, 'x': 508, 'y': 559, 'path': 'images\\\\3cbbc98e-0ce1-4635-b102-1451102c91af-page-8-3.jpg', 'job_id': '3cbbc98e-0ce1-4635-b102-1451102c91af', 'original_pdf_path': 'towards-backpropagation.pdf', 'page_number': 8}, {'name': 'page-8-1.jpg', 'height': 76, 'width': 149, 'x': 196, 'y': 559, 'path': 'images\\\\3cbbc98e-0ce1-4635-b102-1451102c91af-page-8-1.jpg', 'job_id': '3cbbc98e-0ce1-4635-b102-1451102c91af', 'original_pdf_path': 'towards-backpropagation.pdf', 'page_number': 8}]\n",
      "> Image for page 9: [{'name': 'page-9-0.jpg', 'height': 39, 'width': 39, 'x': 33, 'y': 83, 'path': 'images\\\\3cbbc98e-0ce1-4635-b102-1451102c91af-page-9-0.jpg', 'job_id': '3cbbc98e-0ce1-4635-b102-1451102c91af', 'original_pdf_path': 'towards-backpropagation.pdf', 'page_number': 9}]\n",
      "The default case involves a node represented by a circle with an asterisk (*) inside it, symbolizing a multiplication operation. Two arrows labeled 'a' and 'b' point towards the node, indicating that these are the inputs to the node. A single arrow points away from the node, labeled 'a*b', signifying the output of the node after performing the multiplication of the inputs 'a' and 'b'.\n",
      "The representation shows a mathematical operation where two inputs, labeled as 'a+h' and 'b', are being fed into an operation denoted by an asterisk, which typically signifies multiplication. The result of this operation is shown as being outputted with the expression '(a+h)*b'. This visual aid is likely used to illustrate the concept of a function and how its output changes with respect to changes in its inputs, which is a fundamental concept in calculus, particularly when discussing derivatives and partial derivatives.\n",
      "The function depicted is a concave downward curve, resembling the shape of an inverted U. It starts at a lower point labeled A, rises to a peak at point B, and then descends to a lower point labeled C. The curve represents the function y=f(x), with the y-axis representing the function's value and the x-axis representing the variable x. At point A, the function is increasing with respect to x, indicating a positive derivative. At point C, the function is decreasing with respect to x, indicating a negative derivative. The peak at point B represents the maximum value of the function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='2bd31038-1340-4949-9ec1-68e63151be15', embedding=None, metadata={'path': 'images\\\\3cbbc98e-0ce1-4635-b102-1451102c91af-page-2-0.jpg', 'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"The default case involves a node represented by a circle with an asterisk (*) inside it, symbolizing a multiplication operation. Two arrows labeled 'a' and 'b' point towards the node, indicating that these are the inputs to the node. A single arrow points away from the node, labeled 'a*b', signifying the output of the node after performing the multiplication of the inputs 'a' and 'b'.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='513d925c-4d7a-4d41-a933-87389f6496f4', embedding=None, metadata={'path': 'images\\\\3cbbc98e-0ce1-4635-b102-1451102c91af-page-4-0.jpg', 'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"The representation shows a mathematical operation where two inputs, labeled as 'a+h' and 'b', are being fed into an operation denoted by an asterisk, which typically signifies multiplication. The result of this operation is shown as being outputted with the expression '(a+h)*b'. This visual aid is likely used to illustrate the concept of a function and how its output changes with respect to changes in its inputs, which is a fundamental concept in calculus, particularly when discussing derivatives and partial derivatives.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='ffda7c43-0f0e-4aef-8e6c-9d7a23d36a3a', embedding=None, metadata={'path': 'images\\\\3cbbc98e-0ce1-4635-b102-1451102c91af-page-7-0.jpg', 'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"The function depicted is a concave downward curve, resembling the shape of an inverted U. It starts at a lower point labeled A, rises to a peak at point B, and then descends to a lower point labeled C. The curve represents the function y=f(x), with the y-axis representing the function's value and the x-axis representing the variable x. At point A, the function is increasing with respect to x, indicating a positive derivative. At point C, the function is decreasing with respect to x, indicating a negative derivative. The peak at point B represents the maximum value of the function.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index.core.schema import ImageDocument\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "\n",
    "\n",
    "def get_image_text_nodes(json_objs):\n",
    "    \"\"\"Extract out text from images using a multimodal model.\"\"\"\n",
    "    openai_mm_llm = OpenAIMultiModal( model=\"gpt-4-vision-preview\", max_new_tokens=300)\n",
    "    image_dicts = llamaparser.get_images(json_objs, download_path=\"images\")    \n",
    "    image_documents = []\n",
    "    img_text_nodes = []\n",
    "    for image_dict in image_dicts:\n",
    "        if image_dict['width'] > 300 :\n",
    "            image_doc = ImageDocument(image_path=image_dict[\"path\"])\n",
    "            response = openai_mm_llm.complete(\n",
    "                prompt=\"\"\"\n",
    "                Describe the image given the following context. Your answer must not mention that you are describing an image. Just describe it.\n",
    "\n",
    "                {}\n",
    "                \"\"\".format(json_objs[0]['pages'][image_dict['page_number']-1][\"md\"])\n",
    "                ,\n",
    "                image_documents=[image_doc],\n",
    "            )\n",
    "            print(str(response))\n",
    "            text_node = TextNode(\n",
    "                text=str(response),\n",
    "                metadata={\"path\": image_dict[\"path\"],\"author\":\"Jasdeep Singh Chhabra\",\"blog_title\":\"Towards Backpropagation\"}\n",
    "            )\n",
    "            img_text_nodes.append(text_node)\n",
    "    return img_text_nodes\n",
    "img_text_nodes = get_image_text_nodes(json_objs)\n",
    "img_text_nodes\n",
    "# images = llamaparser.get_images(json_objs, dowinload_path=\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fa12a132-32f7-407b-b41b-788ff05f4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nodes = all_nodes + img_text_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5d556bd5-6ec7-455d-a51b-5a6aa4074e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "llm = OpenAI(model='gpt-4')\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "37fa8e63-b1bb-48d4-8004-5ed422b305b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.response.notebook_utils import display_source_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "99a4d93b-01f7-419a-9b89-4b687e24ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index_chunk = VectorStoreIndex(total_nodes,embed_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4051c197-ded8-4b22-9046-2881fa5a74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=2,embed_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "49788e98-e5a0-464c-97e1-29a8c70f82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "\n",
    "retriever_chunk = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": vector_retriever_chunk},\n",
    "    node_dict=all_nodes_dict,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ce0150a7-c2a5-4112-a836-35ccf29147ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: Explain to me fundamentals of derivative\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-3\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-3: Explain to me fundamentals of derivative\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-2\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-2: Explain to me fundamentals of derivative\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "retrievals = retriever_chunk.retrieve(\n",
    "    \"Explain to me fundamentals of derivative\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c2ef2f3c-65ec-477f-9168-a5181995ef13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** node-3<br>**Similarity:** 0.46852035495838973<br>**Text:** 4/2/24, 1:23 AM towards-backpropagation\n",
       "\n",
       "|Analytical gradient method:|In order to calculate gradient using analytical gradient method we need to remember few basic rules of calculus(Refer Derivative-rules).These rules are derived from the numerical gradient method and are committed to memory so that they can be used directly.This saves us the computation time and space required for calculating derivatives.For f=a*b,the following can be directly stated-|\n",
       "|---|---|\n",
       "|∂f|= b|\n",
       "| |∂a|\n",
       "|This can easily be derived from the mathematical interpretation of derivative as stated above.Putting values in interpretation-| |\n",
       "|∂f|= (a + h) * b - a * b|\n",
       "|∂a|h|\n",
       "|∂f|= (a * b + h * b) - a * b|\n",
       "|∂a|h|\n",
       "|∂f|= b|\n",
       "| |∂a|\n",
       "|Similarly one can derive:| |\n",
       "|∂f|= a|\n",
       "| |∂b|\n",
       "|With these two derivatives in hand we have our coefficients of h in a-update and b-update.So our modified update rules will be-| |\n",
       "|a = a + h * ∂f|a = a + h * b|\n",
       "| |∂a|\n",
       "|b = b + h * ∂f|b = b + h * a|\n",
       "| |∂b|\n",
       "\n",
       "https://jasdeep06.github.io/posts/towards-backpropagation/ 5/9\n",
       "\n",
       "4/2/24, 1:23 AM towards-backpropagation\n",
       "\n",
       "The modified code will be:\n",
       "\n",
       "def product(a,b):\n",
       "\n",
       "return a*b\n",
       "\n",
       "a=-4\n",
       "\n",
       "b=-3\n",
       "\n",
       "h=0.01\n",
       "\n",
       "a=a+h*b\n",
       "\n",
       "b=b+h*a\n",
       "\n",
       "print(product(a,b))\n",
       "\n",
       "The output of above code is 12.252 which is greater than 12 as was our aim.\n",
       "\n",
       "Why does this approach work?\n",
       "\n",
       "To appreciate the beauty of derivative in updates of inputs a and b we have to dive into geometrical interpretation of derivative. Geometrically, derivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** node-2<br>**Similarity:** 0.4507014254233799<br>**Text:** Thus: f(a, b) = a * b\n",
       "\n",
       "Source: https://jasdeep06.github.io/posts/towards-backpropagation/\n",
       "\n",
       "The new output can be calculated easily as (a+h)*b which can be expanded as a*b+h*b. Thus the output increases by a value of h*b as compared to the default case. We can say that with an increase of h in a, the output increases by h*b, thus with a unit increase in a the output would have increased by b. This normalized effect of an increase in the value of one of the input on output is expressed as the derivative of output with respect to that input. Note that when we take the derivative of a function with respect to a variable, then all other variables are kept constant. The mathematical interpretation of the derivative of function f with respect to a is defined as:\n",
       "\n",
       "∂f / ∂a = f((a + h), b) - f(a, b) / h\n",
       "\n",
       "The above formula is nothing but the mathematical interpretation of the definition of the derivative as mentioned above. (For those familiar with multivariate calculus, this is the partial derivative of f with respect to a.)\n",
       "\n",
       "Similarly, the derivative of function f with respect to b can be found by increasing b by a small quantity h and normalizing the difference between the final output (a*b+a*h) and the initial output (a*b) that gives us a.\n",
       "\n",
       "This is the numeric method of finding gradients. It has a significant drawback. Although it is less error-prone, it takes a lot of time to calculate derivatives this way. Time to introduce a new method of finding derivatives: Analytical method.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for retrieval in retrievals:\n",
    "    display_source_node(retrieval,source_length=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b007dc1e-9c28-4614-b831-744663ef3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine_chunk = RetrieverQueryEngine.from_args(\n",
    "retriever_chunk, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "36e6c29c-a6af-49e9-9e37-3de35e141126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: Explain to me fundamentals of derivative\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-3\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-3: Explain to me fundamentals of derivative\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-2\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-2: Explain to me fundamentals of derivative\n",
      "\u001b[0mThe fundamentals of derivatives are rooted in calculus. A derivative of a function with respect to a variable indicates the rate at which the function changes with respect to that variable. In other words, it measures how sensitive the function is to changes in the value of that variable. \n",
      "\n",
      "For example, if you have a function f(a, b) = a * b, the derivative of this function with respect to 'a' can be calculated as follows:\n",
      "\n",
      "∂f / ∂a = f((a + h), b) - f(a, b) / h\n",
      "\n",
      "Here, 'h' is a small quantity by which 'a' is increased. The difference between the final output and the initial output is then normalized by 'h'. This gives us the rate of change of the function with respect to 'a'. \n",
      "\n",
      "Similarly, the derivative of the function with respect to 'b' can be calculated by increasing 'b' by a small quantity 'h' and normalizing the difference between the final output and the initial output. \n",
      "\n",
      "It's important to note that when we take the derivative of a function with respect to a variable, all other variables are kept constant. This is the basic principle of partial derivatives in multivariate calculus. \n",
      "\n",
      "However, calculating derivatives this way, known as the numeric method, can be time-consuming. Therefore, the analytical method is often used, which involves using established rules of calculus to calculate derivatives more efficiently.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine_chunk.query(\n",
    "    \"Explain to me fundamentals of derivative\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5d8ebb8f-5098-47fd-9cb3-14b1effd263b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=\"The fundamentals of derivatives are rooted in calculus. A derivative of a function with respect to a variable indicates the rate at which the function changes with respect to that variable. In other words, it measures how sensitive the function is to changes in the value of that variable. \\n\\nFor example, if you have a function f(a, b) = a * b, the derivative of this function with respect to 'a' can be calculated as follows:\\n\\n∂f / ∂a = f((a + h), b) - f(a, b) / h\\n\\nHere, 'h' is a small quantity by which 'a' is increased. The difference between the final output and the initial output is then normalized by 'h'. This gives us the rate of change of the function with respect to 'a'. \\n\\nSimilarly, the derivative of the function with respect to 'b' can be calculated by increasing 'b' by a small quantity 'h' and normalizing the difference between the final output and the initial output. \\n\\nIt's important to note that when we take the derivative of a function with respect to a variable, all other variables are kept constant. This is the basic principle of partial derivatives in multivariate calculus. \\n\\nHowever, calculating derivatives this way, known as the numeric method, can be time-consuming. Therefore, the analytical method is often used, which involves using established rules of calculus to calculate derivatives more efficiently.\", source_nodes=[NodeWithScore(node=IndexNode(id_='node-3', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='89e518c6-c82c-4bca-8f31-51eab31cfff4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='981c221281d663434f9057c4fc300009f52f547887ce66bd384136c12e48527a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c1f7e335-94d2-4df8-8ee4-c8a87c1de10d', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='aa888154f6ae393249fe66d30a7f4bf0a4b2d8d11617e65c7da718fa9ee34a00'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ac3aacdb-954b-4a69-b2d5-637a203188a7', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b89ed3f806ed4bafb574b4ddbf50029920040db78e131570d0102f105d1a0417')}, text='4/2/24, 1:23 AM towards-backpropagation\\n\\n|Analytical gradient method:|In order to calculate gradient using analytical gradient method we need to remember few basic rules of calculus(Refer Derivative-rules).These rules are derived from the numerical gradient method and are committed to memory so that they can be used directly.This saves us the computation time and space required for calculating derivatives.For f=a*b,the following can be directly stated-|\\n|---|---|\\n|∂f|= b|\\n| |∂a|\\n|This can easily be derived from the mathematical interpretation of derivative as stated above.Putting values in interpretation-| |\\n|∂f|= (a + h) * b - a * b|\\n|∂a|h|\\n|∂f|= (a * b + h * b) - a * b|\\n|∂a|h|\\n|∂f|= b|\\n| |∂a|\\n|Similarly one can derive:| |\\n|∂f|= a|\\n| |∂b|\\n|With these two derivatives in hand we have our coefficients of h in a-update and b-update.So our modified update rules will be-| |\\n|a = a + h * ∂f|a = a + h * b|\\n| |∂a|\\n|b = b + h * ∂f|b = b + h * a|\\n| |∂b|\\n\\nhttps://jasdeep06.github.io/posts/towards-backpropagation/ 5/9\\n\\n4/2/24, 1:23 AM towards-backpropagation\\n\\nThe modified code will be:\\n\\ndef product(a,b):\\n\\nreturn a*b\\n\\na=-4\\n\\nb=-3\\n\\nh=0.01\\n\\na=a+h*b\\n\\nb=b+h*a\\n\\nprint(product(a,b))\\n\\nThe output of above code is 12.252 which is greater than 12 as was our aim.\\n\\nWhy does this approach work?\\n\\nTo appreciate the beauty of derivative in updates of inputs a and b we have to dive into geometrical interpretation of derivative. Geometrically, derivative of a function with respect to a variable tells us the rate at which that function changes with respect to that variable.', start_char_idx=5667, end_char_idx=7233, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-3', obj=None), score=0.46852035495838973), NodeWithScore(node=IndexNode(id_='node-2', embedding=None, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='89e518c6-c82c-4bca-8f31-51eab31cfff4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='981c221281d663434f9057c4fc300009f52f547887ce66bd384136c12e48527a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a133e47c-8e65-434e-bea2-3cca761c8fe0', node_type=<ObjectType.TEXT: '1'>, metadata={'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, hash='86968118853346f5ba3ebb5c892c16926984a3ab5b84bc0804d132d953d72445'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='12e045d3-280b-49f9-b071-2ba5d60c04fd', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='7ce9228e7448f6e32acd38e63fc537c1e11b50185b5b2f13667eaef71020d967')}, text='Thus: f(a, b) = a * b\\n\\nSource: https://jasdeep06.github.io/posts/towards-backpropagation/\\n\\nThe new output can be calculated easily as (a+h)*b which can be expanded as a*b+h*b. Thus the output increases by a value of h*b as compared to the default case. We can say that with an increase of h in a, the output increases by h*b, thus with a unit increase in a the output would have increased by b. This normalized effect of an increase in the value of one of the input on output is expressed as the derivative of output with respect to that input. Note that when we take the derivative of a function with respect to a variable, then all other variables are kept constant. The mathematical interpretation of the derivative of function f with respect to a is defined as:\\n\\n∂f / ∂a = f((a + h), b) - f(a, b) / h\\n\\nThe above formula is nothing but the mathematical interpretation of the definition of the derivative as mentioned above. (For those familiar with multivariate calculus, this is the partial derivative of f with respect to a.)\\n\\nSimilarly, the derivative of function f with respect to b can be found by increasing b by a small quantity h and normalizing the difference between the final output (a*b+a*h) and the initial output (a*b) that gives us a.\\n\\nThis is the numeric method of finding gradients. It has a significant drawback. Although it is less error-prone, it takes a lot of time to calculate derivatives this way. Time to introduce a new method of finding derivatives: Analytical method.', start_char_idx=4167, end_char_idx=5665, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-2', obj=None), score=0.4507014254233799)], metadata={'node-3': {'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}, 'node-2': {'author': 'Jasdeep Singh Chhabra', 'blog_title': 'Towards Backpropagation'}})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5827c0-6e2f-4c44-9d24-ef4edbf362cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
