{"docstore/data": {"f10e2efe-f9c2-48d0-86d9-59da9190ced3": {"__data__": {"id_": "f10e2efe-f9c2-48d0-86d9-59da9190ced3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7922d048-95e6-4240-b1ac-86a8eb669047", "node_type": "1", "metadata": {}, "hash": "fc5ea429dea57620f6fc650cfa89e0b54f23c63509f063645771cb87592e3593", "class_name": "RelatedNodeInfo"}}, "text": "4/9/24, 7:56 PM                                                                      React App\n         TranslateTracks (AI Dubbing with Human-in-loop) - A\n        About TranslateTracks                           Technical Report\n        Given the development of highly logically capable LLMs enabling SOTA translation and human-like high fidelity AI\n        Text-to-speech services like ElevenLabs, we could see the immense impact this would create in the dubbing industry,\n        where both AI translation and AI Speech could completely change the industry landscape. Given this thesis in our mind,\n        we started working on Translatetracks.\n        TranslateTracks ran for 7 months, did $100K in revenue with a net margin of 75%. Our Human-in-loop service stood out\n        in the clutter of one-shot Dubbing products, where the final results are far from production ready with numerous\n        inaccuracies, both in terms of translations and the final dubs. We created a network of translators and proofreaders for\n        different European languages in India, thus letting us deliver the service at a fraction of the cost of our competitors.", "start_char_idx": 0, "end_char_idx": 1151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7922d048-95e6-4240-b1ac-86a8eb669047": {"__data__": {"id_": "7922d048-95e6-4240-b1ac-86a8eb669047", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f10e2efe-f9c2-48d0-86d9-59da9190ced3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "79fd1ab113dd48a3aa1a8c7a8067e7e8d02f14aa82947c24e4e268ff64ea23ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eadbda39-d539-4b5d-a928-3adf2b5ad1b8", "node_type": "1", "metadata": {}, "hash": "5b4ae32b490065b1d508b91b649e914850a848c73e26b244f3a67213ff9561fa", "class_name": "RelatedNodeInfo"}}, "text": "To\n        be precise, we were 4x cheaper then our nearest competitor of comparable quality.\n        Here are a few videos we have dubbed:\n             The Catamaran Challenge\n             Tim Cook Interview (change audio track to english)\n             Jamy\u2019s Coffee Effects (change audio track to english)\n        Technical Description\n        Introduction\n        Our platform/app basically involves four different steps:\n             Video Upload       : Direct video upload/ Upload video via Youtube link. We extract the Audio from the video file.\n         This is followed by a vocal remover process that separates the Vocal track of the audio from the Background audio,\n         giving us a Vocal Track and a Background Track.\n             Transcription      : The separated Vocal Track is transcribed using our ASR Engine (Automatic Speech Recognition)\n         that uses multiple SOTA transcription APIs like AssemblyAI, Deepgram and Gladia.\n             Translation     : Once the Video is transcribed, and the segments are verified, we translate the text from source\n         language to the target language, using an LLM of our choice (Typically Chatgpt4, Claude, and LLama2).", "start_char_idx": 1152, "end_char_idx": 2339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "eadbda39-d539-4b5d-a928-3adf2b5ad1b8": {"__data__": {"id_": "eadbda39-d539-4b5d-a928-3adf2b5ad1b8", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7922d048-95e6-4240-b1ac-86a8eb669047", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44e52f936f05bcf549827d210c803e6aa294cafe1f7892461a6fd72c06e8d447", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e468cf4-db15-4494-9bfb-5e514d714960", "node_type": "1", "metadata": {}, "hash": "8b7367acc200c0d6f44b567384d8fdc5658570a9d2d13503d059e8cb33d7b972", "class_name": "RelatedNodeInfo"}}, "text": "This step\n         involves multiple LLM based optimizations, to assure that the translations are up to the mark.\n             Text-to-Speech       : This is the last step. For each translated segment, and based on the assigned speaker to the text,\n         an AI speech is generated using a TTS API (Elevenlabs) that is matched with the characteristics of the original voice.\n         This step also has multiple ML/AI optimization pipelines, that makes sure that final audio is expressive and engaging,\n         and at no point, feel machine generated.\n        Video Upload\n        Our app can be accessed      here. Video for dubbing can be uploaded directly or directly via a youtube link.\nlocalhost:3001                                                                                                                          1/64/9/24, 7:56 PM                                                                       React App\n                                  TranslateTracksBetaYOUR VIDEOSCLONE YOUR VoiceiSHOWCASEHOW TO USE?", "start_char_idx": 2340, "end_char_idx": 3369, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5e468cf4-db15-4494-9bfb-5e514d714960": {"__data__": {"id_": "5e468cf4-db15-4494-9bfb-5e514d714960", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eadbda39-d539-4b5d-a928-3adf2b5ad1b8", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "90f6147dc2b78b7ebb35ed1fdf6818956438ca2362c0170da5bec093621098fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbce1c42-cd54-44c0-b5e1-c254fc25695c", "node_type": "1", "metadata": {}, "hash": "56094870facc1eed81f7f908a6fc7619eaea50752fe39f1dcd95ac0893fa2bbb", "class_name": "RelatedNodeInfo"}}, "text": "Hi vinglabs3\n                                                       YOUTUBE URLUPLOAD       files\n                                                                       DragDrop    Browse\n                                                                             Youi\n                                                                         Your Videos\n                                RequestID         Title        Duratlon Target LanguageStatus       Add Backgrcund   Download\n                                  TranslateTracksBetaYOUR VIDEOSCLONE YOUR VOICEISHOWCASEHOW TO USE?                    Hi vinglabs3\n                                                       YOUTUBE URLUPLOAD\n                                                        Youtube URL\n                                                                            GET INFOI\n                                                                         Your Videos\n                                Request ID                     Duratlon Target LanguageSlalus       Add Background   Downbad\n       Our app is hosted on AWS EC2 Ubuntu 22 Server, backend is written in Python FastAPI,and frontend is written in\n       React, and for the database, we are using MongoDB for storing video/user data, audios/videos are hosted on S3. We use\n       various LLMs like ChatGPT, Claude and LLama2 in our application. All the LLM calls are logged in Langfuse for\n       observability, LLM performance monitoring, and running evaluation on output afterwards.", "start_char_idx": 3389, "end_char_idx": 4894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "cbce1c42-cd54-44c0-b5e1-c254fc25695c": {"__data__": {"id_": "cbce1c42-cd54-44c0-b5e1-c254fc25695c", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e468cf4-db15-4494-9bfb-5e514d714960", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "dca86935f0685b6baca85d815be0f70e19f2b47181c244a59c70a63d41252e2e", "class_name": "RelatedNodeInfo"}}, "text": "We are also using\n       PostgresQL for logging user platform usage/story, and NewRelic for instance health monitoring.\n       Once the video is uploaded, we use a Background remover that is deployed on the same server. For every MP4 file, we\n       get Vocal & Bg audio tracks in WAV format. This vocal file ensures that all the background music/noise is separated\n       from the audio track. This ensures high accuracy in the Transcription process as the audio is Noise-free.\n       Transcription\n            We then transcribe the audio file to convert it into text format.", "start_char_idx": 4895, "end_char_idx": 5472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "aebd560f-675d-4420-8182-8da5094e0d6d": {"__data__": {"id_": "aebd560f-675d-4420-8182-8da5094e0d6d", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69ac1a80-545b-4a57-b6a8-336a4cfc0423", "node_type": "1", "metadata": {}, "hash": "a9e878bbb9559dd041d9f6749287d8f7e87282956fc55d15529f5a5a5d37d8c2", "class_name": "RelatedNodeInfo"}}, "text": "4/9/24, 7:56 PM                                                                      React App\n         TranslateTracks (AI Dubbing with Human-in-loop) - A\n        About TranslateTracks                           Technical Report\n        Given the development of highly logically capable LLMs enabling SOTA translation and human-like high fidelity AI\n        Text-to-speech services like ElevenLabs, we could see the immense impact this would create in the dubbing industry,\n        where both AI translation and AI Speech could completely change the industry landscape. Given this thesis in our mind,\n        we started working on Translatetracks.\n        TranslateTracks ran for 7 months, did $100K in revenue with a net margin of 75%. Our Human-in-loop service stood out\n        in the clutter of one-shot Dubbing products, where the final results are far from production ready with numerous\n        inaccuracies, both in terms of translations and the final dubs. We created a network of translators and proofreaders for\n        different European languages in India, thus letting us deliver the service at a fraction of the cost of our competitors. To\n        be precise, we were 4x cheaper then our nearest competitor of comparable quality.\n        Here are a few videos we have dubbed:\n             The Catamaran Challenge\n             Tim Cook Interview (change audio track to english)\n             Jamy\u2019s Coffee Effects (change audio track to english)\n        Technical Description\n        Introduction\n        Our platform/app basically involves four different steps:\n             Video Upload       : Direct video upload/ Upload video via Youtube link. We extract the Audio from the video file.\n         This is followed by a vocal remover process that separates the Vocal track of the audio from the Background audio,\n         giving us a Vocal Track and a Background Track.\n             Transcription      : The separated Vocal Track is transcribed using our ASR Engine (Automatic Speech Recognition)\n         that uses multiple SOTA transcription APIs like AssemblyAI, Deepgram and Gladia.\n             Translation     : Once the Video is transcribed, and the segments are verified, we translate the text from source\n         language to the target language, using an LLM of our choice (Typically Chatgpt4, Claude, and LLama2). This step\n         involves multiple LLM based optimizations, to assure that the translations are up to the mark.\n             Text-to-Speech       : This is the last step.", "start_char_idx": 0, "end_char_idx": 2512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "69ac1a80-545b-4a57-b6a8-336a4cfc0423": {"__data__": {"id_": "69ac1a80-545b-4a57-b6a8-336a4cfc0423", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aebd560f-675d-4420-8182-8da5094e0d6d", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "5d9643bf5af1b6545c42ee82c2cb29bae82c63b1972951767b1e814f5b2fa2d6", "class_name": "RelatedNodeInfo"}}, "text": "For each translated segment, and based on the assigned speaker to the text,\n         an AI speech is generated using a TTS API (Elevenlabs) that is matched with the characteristics of the original voice.\n         This step also has multiple ML/AI optimization pipelines, that makes sure that final audio is expressive and engaging,\n         and at no point, feel machine generated.\n        Video Upload\n        Our app can be accessed      here. Video for dubbing can be uploaded directly or directly via a youtube link.\nlocalhost:3001                                                                                                                          1/64/9/24, 7:56 PM                                                                       React App\n                                  TranslateTracksBetaYOUR VIDEOSCLONE YOUR VoiceiSHOWCASEHOW TO USE?                    Hi vinglabs3\n                                                       YOUTUBE URLUPLOAD       files\n                                                                       DragDrop    Browse\n                                                                             Youi\n                                                                         Your Videos\n                                RequestID         Title        Duratlon Target LanguageStatus       Add Backgrcund   Download\n                                  TranslateTracksBetaYOUR VIDEOSCLONE YOUR VOICEISHOWCASEHOW TO USE?                    Hi vinglabs3\n                                                       YOUTUBE URLUPLOAD\n                                                        Youtube URL\n                                                                            GET INFOI\n                                                                         Your Videos\n                                Request ID                     Duratlon Target LanguageSlalus       Add Background   Downbad\n       Our app is hosted on AWS EC2 Ubuntu 22 Server, backend is written in Python FastAPI,and frontend is written in\n       React, and for the database, we are using MongoDB for storing video/user data, audios/videos are hosted on S3. We use\n       various LLMs like ChatGPT, Claude and LLama2 in our application. All the LLM calls are logged in Langfuse for\n       observability, LLM performance monitoring, and running evaluation on output afterwards. We are also using\n       PostgresQL for logging user platform usage/story, and NewRelic for instance health monitoring.\n       Once the video is uploaded, we use a Background remover that is deployed on the same server. For every MP4 file, we\n       get Vocal & Bg audio tracks in WAV format. This vocal file ensures that all the background music/noise is separated\n       from the audio track. This ensures high accuracy in the Transcription process as the audio is Noise-free.\n       Transcription\n            We then transcribe the audio file to convert it into text format.", "start_char_idx": 2513, "end_char_idx": 5472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-0": {"__data__": {"id_": "node-0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91fb9c65-9f64-4cf9-b6ca-f3df23d8d107", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44cce72dfc54eb3ebcb4be1566f418d29728824ef4849223f273db80f6c9bdca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51c374b4-7b51-409b-9a79-14b6446e823f", "node_type": "1", "metadata": {}, "hash": "d6d27f4c1dfca4b2330e92ece0980df093745533250f83fd44e0c27aede12cc5", "class_name": "RelatedNodeInfo"}}, "text": "4/9/24, 7:56 PM                                                                      React App\n         TranslateTracks (AI Dubbing with Human-in-loop) - A\n        About TranslateTracks                           Technical Report\n        Given the development of highly logically capable LLMs enabling SOTA translation and human-like high fidelity AI\n        Text-to-speech services like ElevenLabs, we could see the immense impact this would create in the dubbing industry,\n        where both AI translation and AI Speech could completely change the industry landscape. Given this thesis in our mind,\n        we started working on Translatetracks.\n        TranslateTracks ran for 7 months, did $100K in revenue with a net margin of 75%. Our Human-in-loop service stood out\n        in the clutter of one-shot Dubbing products, where the final results are far from production ready with numerous\n        inaccuracies, both in terms of translations and the final dubs. We created a network of translators and proofreaders for\n        different European languages in India, thus letting us deliver the service at a fraction of the cost of our competitors. To\n        be precise, we were 4x cheaper then our nearest competitor of comparable quality.\n        Here are a few videos we have dubbed:\n             The Catamaran Challenge\n             Tim Cook Interview (change audio track to english)\n             Jamy\u2019s Coffee Effects (change audio track to english)\n        Technical Description\n        Introduction\n        Our platform/app basically involves four different steps:\n             Video Upload       : Direct video upload/ Upload video via Youtube link. We extract the Audio from the video file.\n         This is followed by a vocal remover process that separates the Vocal track of the audio from the Background audio,\n         giving us a Vocal Track and a Background Track.\n             Transcription      : The separated Vocal Track is transcribed using our ASR Engine (Automatic Speech Recognition)\n         that uses multiple SOTA transcription APIs like AssemblyAI, Deepgram and Gladia.\n             Translation     : Once the Video is transcribed, and the segments are verified, we translate the text from source\n         language to the target language, using an LLM of our choice (Typically Chatgpt4, Claude, and LLama2). This step\n         involves multiple LLM based optimizations, to assure that the translations are up to the mark.\n             Text-to-Speech       : This is the last step. For each translated segment, and based on the assigned speaker to the text,\n         an AI speech is generated using a TTS API (Elevenlabs) that is matched with the characteristics of the original voice.\n         This step also has multiple ML/AI optimization pipelines, that makes sure that final audio is expressive and engaging,\n         and at no point, feel machine generated.\n        Video Upload\n        Our app can be accessed      here. Video for dubbing can be uploaded directly or directly via a youtube link.\nlocalhost:3001                                                                                                                          1/64/9/24, 7:56 PM                                                                       React App\n                                  TranslateTracksBetaYOUR VIDEOSCLONE YOUR VoiceiSHOWCASEHOW TO USE?                    Hi vinglabs3\n                                                       YOUTUBE URLUPLOAD       files\n                                                                       DragDrop    Browse\n                                                                             Youi\n                                                                         Your Videos\n                                RequestID         Title        Duratlon Target LanguageStatus       Add Backgrcund   Download\n                                  TranslateTracksBetaYOUR VIDEOSCLONE YOUR VOICEISHOWCASEHOW TO USE?                    Hi vinglabs3\n                                                       YOUTUBE URLUPLOAD\n                                                        Youtube URL\n                                                                            GET INFOI\n                                                                         Your Videos\n                                Request ID                     Duratlon Target LanguageSlalus       Add Background   Downbad\n       Our app is hosted on AWS EC2 Ubuntu 22 Server, backend is written in Python FastAPI,and frontend is written in\n       React, and for the database, we are using MongoDB for storing video/user data, audios/videos are hosted on S3. We use\n       various LLMs like ChatGPT, Claude and LLama2 in our application. All the LLM calls are logged in Langfuse for\n       observability, LLM performance monitoring, and running evaluation on output afterwards. We are also using\n       PostgresQL for logging user platform usage/story, and NewRelic for instance health monitoring.\n       Once the video is uploaded, we use a Background remover that is deployed on the same server. For every MP4 file, we\n       get Vocal & Bg audio tracks in WAV format. This vocal file ensures that all the background music/noise is separated\n       from the audio track. This ensures high accuracy in the Transcription process as the audio is Noise-free.\n       Transcription\n            We then transcribe the audio file to convert it into text format.", "start_char_idx": 0, "end_char_idx": 5472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d9c07de1-84d1-40af-b7e1-b8660892df75": {"__data__": {"id_": "d9c07de1-84d1-40af-b7e1-b8660892df75", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b165a743-98e0-4b9e-b4e8-50f5a62c2162", "node_type": "1", "metadata": {}, "hash": "ceea5b287db3ebea2c12a9d8804fbf12533fe7090f01264aa40f59bbe09b04d1", "class_name": "RelatedNodeInfo"}}, "text": "Along with generating text, this process also assigns\n        a unique speaker label to each text segment, this process is called Speaker Diarization.\n            For this we use a combination of different Speech to text APIs to get the most accurate transcripts for our video.\n        We use Deepgram, AssemblyAI and Gladia for transcription. Every service has its own merits and demerits.\n            We use our ASR Engine (Automatic speech recognition), that takes input from all three services, and optimises the\n        output, so that we can minimise the Word Error rate, Reduce Hallucinations and maximise the Diarisation accuracy.\n            All the optimization pipelines in our application results in one thing, and that is Lip Synchronized / Lip matched\n        final Audio.\n            To ensure this, we split the transcript segments into smaller sub-segments, so that each segment appropriately\n        matches with the content on the screen.\n            We automatically split these segments at Audio and Video Breakpoints. These are the points where either the\n        Silence is greater than Threshold, or there is scene change.", "start_char_idx": 0, "end_char_idx": 1146, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b165a743-98e0-4b9e-b4e8-50f5a62c2162": {"__data__": {"id_": "b165a743-98e0-4b9e-b4e8-50f5a62c2162", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9c07de1-84d1-40af-b7e1-b8660892df75", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "0f7f75fc345315c3531611ac9c33d45e272d0e935db6cc97d684df189466720d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fc6650f-ce3b-4615-8d26-c795d96f5963", "node_type": "1", "metadata": {}, "hash": "32651eecd783710d5b4c669c965664c21384a0749ef0180cba7d77d03a1239bc", "class_name": "RelatedNodeInfo"}}, "text": "localhost:3001                                                                                                                        2/64/9/24, 7:56 PM                                                          React App    emotion\n                                                    [CAT\n                                                             Video Segment Duralion: 5.945\n                                     Transcription\n                                    Benvenuli in Italia,coslicra amallianauno dci piu bei tratliabbiamo\n                                                          cosia cncqueslo pacse               X 1\n                                       WelcomeItaly; on Ine Amalli Coast, one of the most beautiful stretches of coaslline we have in Ihis country:\n                                     Translalion               TRANSLATESAVE                 Audio Bp\n                                                             Thave assigned the speaker mapping:\n           Finally, we see the segments in the timeline of the editor, along with the Video and audio. Now we have a human in\n        the loop to correct the Transcripts. The editor provides multiple functionalities to modify various aspects of the\n        transcripts.                                         Video to Dub\n                                                          Direct Upload / Youtube link\n                            Extract Audio via FFmpeg         Video Source                    Technologies used -\n                                                                                   ffmpeg - For extracting audio from video\n                               Vocal Remover                                    and to detect Audio and Video breakpoints.", "start_char_idx": 1147, "end_char_idx": 2895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0fc6650f-ce3b-4615-8d26-c795d96f5963": {"__data__": {"id_": "0fc6650f-ce3b-4615-8d26-c795d96f5963", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b165a743-98e0-4b9e-b4e8-50f5a62c2162", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "72d97b448948bbf0aebbd6156c3e9351957bb43cdd70e84c29283399903f4225", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55d4b466-d6e3-4347-b3f7-7d8f88e5eb7c", "node_type": "1", "metadata": {}, "hash": "892b5077333467e84a364e9b9ac96e444ec9c5d633d23792af2cf272287c614b", "class_name": "RelatedNodeInfo"}}, "text": "Vocal Track               Background Track               ASR services - Assembly AI, Deepgram,\n                                                                                Gladia.\n                                                                         Video FileVocalRemover(https://vocalremover.org/)\n                                                        Video Breakpoints using FFmpeg\n        AssemblyAIDeepgram Gladia  Audio Breakpoints using FFmpeg               - A service to seperate vocals from\n                     ASR Engine                    Audio File                   background.\n                   Optimized Transcript                                            Python Libraries - moviepy,pydub\n       Translation Processed Transcript      Dubbing Editor\n           Once transcription is verified, the next step is Translation.\n           The most fundamental requirement here is that, the #segments in Transcript and #Segments in Translation should\n        be the same.", "start_char_idx": 2922, "end_char_idx": 3914, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "55d4b466-d6e3-4347-b3f7-7d8f88e5eb7c": {"__data__": {"id_": "55d4b466-d6e3-4347-b3f7-7d8f88e5eb7c", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fc6650f-ce3b-4615-8d26-c795d96f5963", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "4590529e6de047fc5ec7ba5681510c1f1d6e075db4c935f5dcdefb7948144ee4", "class_name": "RelatedNodeInfo"}}, "text": "localhost:3001                                                                                                                     3/64/9/24, 7:56 PM                                                                React App\n                               \"1\" \"Bienvenidostodos                         1: \"Welcome everyone_\n                               \"2\" \"Hoy ,    vluodeddeode hoy ,   hablar\n                                        en                 vamos                \"Today ,        video,\n                               \"3\" \"E pues bueno,     mencionados                     in today        Were going to\n                                               m |uopels       delmundo                        most mentioned\n                                                    de la relojer |uadeda       \"Well,oneof the             inthe\n                               \"4\" \"Sobretodo delmundo                                                 of watchmaking\n                                                     la relojerluggeda       4: \"Especially from the world\n                                   \"de entrada,                    de luj\n                                             un poco                         5: \"starting     bit into the world oflu;\n                                                        Ilevar relojes                  out,\n                                   \"Es   calibre  suelen           desde\n                                      un       que                                    caliberthat is foundin Watches\n                              euros                                          6: \"It\\' s         large group.", "start_char_idx": 3915, "end_char_idx": 5562, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0979c205-2c8c-4016-8489-943fe27f0367": {"__data__": {"id_": "0979c205-2c8c-4016-8489-943fe27f0367", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16d73c51-f156-48d3-988b-2059b5036e15", "node_type": "1", "metadata": {}, "hash": "a581d32aa881a620f51340cf4175e04c73bfc7dc1189f977667ee9ec5d3334b9", "class_name": "RelatedNodeInfo"}}, "text": "Along with generating text, this process also assigns\n        a unique speaker label to each text segment, this process is called Speaker Diarization.\n            For this we use a combination of different Speech to text APIs to get the most accurate transcripts for our video.\n        We use Deepgram, AssemblyAI and Gladia for transcription. Every service has its own merits and demerits.\n            We use our ASR Engine (Automatic speech recognition), that takes input from all three services, and optimises the\n        output, so that we can minimise the Word Error rate, Reduce Hallucinations and maximise the Diarisation accuracy.\n            All the optimization pipelines in our application results in one thing, and that is Lip Synchronized / Lip matched\n        final Audio.\n            To ensure this, we split the transcript segments into smaller sub-segments, so that each segment appropriately\n        matches with the content on the screen.\n            We automatically split these segments at Audio and Video Breakpoints. These are the points where either the\n        Silence is greater than Threshold, or there is scene change.\nlocalhost:3001                                                                                                                        2/64/9/24, 7:56 PM                                                          React App    emotion\n                                                    [CAT\n                                                             Video Segment Duralion: 5.945\n                                     Transcription\n                                    Benvenuli in Italia,coslicra amallianauno dci piu bei tratliabbiamo\n                                                          cosia cncqueslo pacse               X 1\n                                       WelcomeItaly; on Ine Amalli Coast, one of the most beautiful stretches of coaslline we have in Ihis country:\n                                     Translalion               TRANSLATESAVE                 Audio Bp\n                                                             Thave assigned the speaker mapping:\n           Finally, we see the segments in the timeline of the editor, along with the Video and audio. Now we have a human in\n        the loop to correct the Transcripts. The editor provides multiple functionalities to modify various aspects of the\n        transcripts.                                         Video to Dub\n                                                          Direct Upload / Youtube link\n                            Extract Audio via FFmpeg         Video Source                    Technologies used -\n                                                                                   ffmpeg - For extracting audio from video\n                               Vocal Remover                                    and to detect Audio and Video breakpoints.\n                          Vocal Track               Background Track               ASR services - Assembly AI, Deepgram,\n                                                                                Gladia.", "start_char_idx": 0, "end_char_idx": 3104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "16d73c51-f156-48d3-988b-2059b5036e15": {"__data__": {"id_": "16d73c51-f156-48d3-988b-2059b5036e15", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0979c205-2c8c-4016-8489-943fe27f0367", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "affadbf1ad78eb820b199dda631fd447a8bd37c3e52a6b7031ea3dc39dad999e", "class_name": "RelatedNodeInfo"}}, "text": "Video FileVocalRemover(https://vocalremover.org/)\n                                                        Video Breakpoints using FFmpeg\n        AssemblyAIDeepgram Gladia  Audio Breakpoints using FFmpeg               - A service to seperate vocals from\n                     ASR Engine                    Audio File                   background.\n                   Optimized Transcript                                            Python Libraries - moviepy,pydub\n       Translation Processed Transcript      Dubbing Editor\n           Once transcription is verified, the next step is Translation.\n           The most fundamental requirement here is that, the #segments in Transcript and #Segments in Translation should\n        be the same.\nlocalhost:3001                                                                                                                     3/64/9/24, 7:56 PM                                                                React App\n                               \"1\" \"Bienvenidostodos                         1: \"Welcome everyone_\n                               \"2\" \"Hoy ,    vluodeddeode hoy ,   hablar\n                                        en                 vamos                \"Today ,        video,\n                               \"3\" \"E pues bueno,     mencionados                     in today        Were going to\n                                               m |uopels       delmundo                        most mentioned\n                                                    de la relojer |uadeda       \"Well,oneof the             inthe\n                               \"4\" \"Sobretodo delmundo                                                 of watchmaking\n                                                     la relojerluggeda       4: \"Especially from the world\n                                   \"de entrada,                    de luj\n                                             un poco                         5: \"starting     bit into the world oflu;\n                                                        Ilevar relojes                  out,\n                                   \"Es   calibre  suelen           desde\n                                      un       que                                    caliberthat is foundin Watches\n                              euros                                          6: \"It\\' s         large group.", "start_char_idx": 3178, "end_char_idx": 5562, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-1": {"__data__": {"id_": "node-1", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91fb9c65-9f64-4cf9-b6ca-f3df23d8d107", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44cce72dfc54eb3ebcb4be1566f418d29728824ef4849223f273db80f6c9bdca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b35a1101-f96e-4cad-85ca-88d0801d8e59", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c503c8c9-87d3-487e-b93f-662f742b7737", "node_type": "1", "metadata": {}, "hash": "68b8a7ef225bdd8f3f9ef14954510532f6d040ef41b77a64e13dc60e5caeceff", "class_name": "RelatedNodeInfo"}}, "text": "Along with generating text, this process also assigns\n        a unique speaker label to each text segment, this process is called Speaker Diarization.\n            For this we use a combination of different Speech to text APIs to get the most accurate transcripts for our video.\n        We use Deepgram, AssemblyAI and Gladia for transcription. Every service has its own merits and demerits.\n            We use our ASR Engine (Automatic speech recognition), that takes input from all three services, and optimises the\n        output, so that we can minimise the Word Error rate, Reduce Hallucinations and maximise the Diarisation accuracy.\n            All the optimization pipelines in our application results in one thing, and that is Lip Synchronized / Lip matched\n        final Audio.\n            To ensure this, we split the transcript segments into smaller sub-segments, so that each segment appropriately\n        matches with the content on the screen.\n            We automatically split these segments at Audio and Video Breakpoints. These are the points where either the\n        Silence is greater than Threshold, or there is scene change.\nlocalhost:3001                                                                                                                        2/64/9/24, 7:56 PM                                                          React App    emotion\n                                                    [CAT\n                                                             Video Segment Duralion: 5.945\n                                     Transcription\n                                    Benvenuli in Italia,coslicra amallianauno dci piu bei tratliabbiamo\n                                                          cosia cncqueslo pacse               X 1\n                                       WelcomeItaly; on Ine Amalli Coast, one of the most beautiful stretches of coaslline we have in Ihis country:\n                                     Translalion               TRANSLATESAVE                 Audio Bp\n                                                             Thave assigned the speaker mapping:\n           Finally, we see the segments in the timeline of the editor, along with the Video and audio. Now we have a human in\n        the loop to correct the Transcripts. The editor provides multiple functionalities to modify various aspects of the\n        transcripts.                                         Video to Dub\n                                                          Direct Upload / Youtube link\n                            Extract Audio via FFmpeg         Video Source                    Technologies used -\n                                                                                   ffmpeg - For extracting audio from video\n                               Vocal Remover                                    and to detect Audio and Video breakpoints.\n                          Vocal Track               Background Track               ASR services - Assembly AI, Deepgram,\n                                                                                Gladia.\n                                                                         Video FileVocalRemover(https://vocalremover.org/)\n                                                        Video Breakpoints using FFmpeg\n        AssemblyAIDeepgram Gladia  Audio Breakpoints using FFmpeg               - A service to seperate vocals from\n                     ASR Engine                    Audio File                   background.\n                   Optimized Transcript                                            Python Libraries - moviepy,pydub\n       Translation Processed Transcript      Dubbing Editor\n           Once transcription is verified, the next step is Translation.\n           The most fundamental requirement here is that, the #segments in Transcript and #Segments in Translation should\n        be the same.\nlocalhost:3001                                                                                                                     3/64/9/24, 7:56 PM                                                                React App\n                               \"1\" \"Bienvenidostodos                         1: \"Welcome everyone_\n                               \"2\" \"Hoy ,    vluodeddeode hoy ,   hablar\n                                        en                 vamos                \"Today ,        video,\n                               \"3\" \"E pues bueno,     mencionados                     in today        Were going to\n                                               m |uopels       delmundo                        most mentioned\n                                                    de la relojer |uadeda       \"Well,oneof the             inthe\n                               \"4\" \"Sobretodo delmundo                                                 of watchmaking\n                                                     la relojerluggeda       4: \"Especially from the world\n                                   \"de entrada,                    de luj\n                                             un poco                         5: \"starting     bit into the world oflu;\n                                                        Ilevar relojes                  out,\n                                   \"Es   calibre  suelen           desde\n                                      un       que                                    caliberthat is foundin Watches\n                              euros                                          6: \"It\\' s         large group.", "start_char_idx": 5473, "end_char_idx": 11035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "33387acb-07d1-4762-8ba9-76e010447d29": {"__data__": {"id_": "33387acb-07d1-4762-8ba9-76e010447d29", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2306d4fd-35d6-4700-8a64-55af944a1187", "node_type": "1", "metadata": {}, "hash": "10b78f3a8363c9e13aba6f1bc11432e32783528691622257cc46b5764d90ad77", "class_name": "RelatedNodeInfo"}}, "text": "que  usadoporun grangrupo _              7: 'and it's used by\n                               \"8\"             habrlunoe9isvistolo              And well,  you        in the title\n                                     bueno,como               en   que                   as    ve seen           ana\n                              Powermatic 80                                     \"The ideaof this video is\n                               \"9\"    ideade estevluggeddeo                      \"to give    bit of general\n                                    La                   es_                 10:        you               contextabr\n                                                  contexto general\n                               \"10\"  daros  poco               sobre\n                                         un                                  11: 'what its variants  what itsmost\n                                                 sus variantes,culuedelles                       are             oui\n                               \"11   culuoellesson                                       comes from, what changes it ha:\n                                    de dludaf3ndeviene,", "start_char_idx": 0, "end_char_idx": 1168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2306d4fd-35d6-4700-8a64-55af944a1187": {"__data__": {"id_": "2306d4fd-35d6-4700-8a64-55af944a1187", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33387acb-07d1-4762-8ba9-76e010447d29", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "f91ce012666760362d9a6ccb54efcc94996b1284fbb6f138d840f82bf2d2b243", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3161741e-8d36-468c-ab49-7d8f4964a49b", "node_type": "1", "metadata": {}, "hash": "a213eeaaf42cf8a67e76547162581d7cd64362a7162ab91864ad97f4fb7521c9", "class_name": "RelatedNodeInfo"}}, "text": "quluade9             12: 'where it\n                               \"12\"                         cambiostiene     different typesare\n                                 distintos\n                              los        tipos                                   \"I\\'1l try\n                               \"13   Intentarlugge9         denso  vlugge    13:         to make the video nottoo dense\n                                                que   seamuy                         more enjoyable,\n                               \"14       seam|uadels Ilevadero,      part    14: and              but from this video\n                                      que                   peroque          15: \"recognize _\n                               \"15\"  reconocer\n                               \"16\" \"diferentes                              16:\"different\n                                                                                        or With different\n                                    'relojes   diferentes                    17: \"watches\n                               \"17           con        movimientos                                     movements\n           We convert the input transcript to a JSON, split them into groups of maximum 50 segments, and have overlaps in\n        segment groups to preserve translation context.", "start_char_idx": 1169, "end_char_idx": 2493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3161741e-8d36-468c-ab49-7d8f4964a49b": {"__data__": {"id_": "3161741e-8d36-468c-ab49-7d8f4964a49b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2306d4fd-35d6-4700-8a64-55af944a1187", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c723334d49de28262266d58b21bbd65e57ab7304b8f4485b680da98d1bb2540a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9374339e-5ad4-4356-a690-d9da11aec074", "node_type": "1", "metadata": {}, "hash": "40bdd99a52ee07f95827af2b579d422ac0b98b40d64cfc573bf75cbad8f228dc", "class_name": "RelatedNodeInfo"}}, "text": "For groups with more than 50 segments, the LLMs suffer from a\n        needle in a stack issue, with poor performance, unequal input outputs, and sometimes exceed input tokens limit.\n                                           Verified Transcript\n                                             Preprocessing\n                                  Group     Split into Groups\n                         LLM :Contextual Translation::                                                            Technologies used -\n                                                                                          LLM - OpenAI gpt-4 for contextual translation\n           LLM Validator ::JSON and Balanced I/0:    Yes           Groups 2-n             Orchestrator -\n                                Fail                                                   Langchain(LLMChain,FewShotPromptTemplate,Retr\n                                                                                          Embeddings - gpt-3.5 embeddings(small 1536 dim\n                                        #Retry 32                                      mapping engine to map input and output segments in\n                       Pass                No                                          RetryOutputParser fails.\n                                  Semantic Mapping Engine   Same Process for Groups 2-n   Python Libraries - Langchain\n                                  Grouped Translation Block\n                                Postprocess :  Split Translations:\n           Every group is translated in one LLM prompt, so that Translation context is preserved.", "start_char_idx": 2494, "end_char_idx": 4098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "9374339e-5ad4-4356-a690-d9da11aec074": {"__data__": {"id_": "9374339e-5ad4-4356-a690-d9da11aec074", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3161741e-8d36-468c-ab49-7d8f4964a49b", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "2111d9c7471ccf6983fb0245696eb8ab5fe89a65d1f27fdfaa3deb0ac97fe2c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a9d83a8-28ff-405a-a08f-5252aacd1ef2", "node_type": "1", "metadata": {}, "hash": "1ecc0b33f5fb21a4dd6621e5ba9902e225ed8327cd723441164c7c6c1596df6a", "class_name": "RelatedNodeInfo"}}, "text": "We orchestrate this using\n        Langchain, with a Few Shots example, along with Output Parser to force the output to the defined formats.\n           We have a separate Semantic Mapping Engine, to map the input transcript and output translation in case there is a\n        mismatch between the number of segments in reference and translated group.\n           Once each group is translated, we recombine the groups to get the translated segments. Every segment is assigned\n        a speaker, that is mapped to an AI voice.\nlocalhost:3001                                                                                                                             4/64/9/24, 7:56 PM                                                                     React App\n            Now the main challenge with translation is that the same text, when translated across languages, has different\n         numbers of characters, and therefore different time to say the same thing. For example:\n                          0:07\n                          0:07 / 0:07\n                               / 0:07                                                         0:09\n                                                                                              0:09 / 0:09\n                                      English                                                      / 0:09  Hindi\n            To handle this, we have a separate LLM engine that utilises our voice models.", "start_char_idx": 4099, "end_char_idx": 5553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7a9d83a8-28ff-405a-a08f-5252aacd1ef2": {"__data__": {"id_": "7a9d83a8-28ff-405a-a08f-5252aacd1ef2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9374339e-5ad4-4356-a690-d9da11aec074", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "1e9ab8847dd98e05862f5e5f8babb5d9e02ad955295762dc76214ec2949d4fcf", "class_name": "RelatedNodeInfo"}}, "text": "For every voice, we have developed\n         ML models across languages to predict with ~80% accuracy, the expected duration of the generated audio from a\n         particular text.                                                                          Technologies used -\n                        Postprocess :: Split Translations: =                      scikit-learn, pytorch - LinerRegressionModel and\n                                                Translation #1                SVM for predicting duration of audio.", "start_char_idx": 5554, "end_char_idx": 6073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c2dae1be-e4fc-4d52-bf93-35eed594e4b0": {"__data__": {"id_": "c2dae1be-e4fc-4d52-bf93-35eed594e4b0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62e197a7-f51f-48eb-bbd9-068dc65c2b8c", "node_type": "1", "metadata": {}, "hash": "cc5ee3d9303cebeea29cf2d172258cc42b2e8a7cba834a002f6ca9c32932a728", "class_name": "RelatedNodeInfo"}}, "text": "que  usadoporun grangrupo _              7: 'and it's used by\n                               \"8\"             habrlunoe9isvistolo              And well,  you        in the title\n                                     bueno,como               en   que                   as    ve seen           ana\n                              Powermatic 80                                     \"The ideaof this video is\n                               \"9\"    ideade estevluggeddeo                      \"to give    bit of general\n                                    La                   es_                 10:        you               contextabr\n                                                  contexto general\n                               \"10\"  daros  poco               sobre\n                                         un                                  11: 'what its variants  what itsmost\n                                                 sus variantes,culuedelles                       are             oui\n                               \"11   culuoellesson                                       comes from, what changes it ha:\n                                    de dludaf3ndeviene, quluade9             12: 'where it\n                               \"12\"                         cambiostiene     different typesare\n                                 distintos\n                              los        tipos                                   \"I\\'1l try\n                               \"13   Intentarlugge9         denso  vlugge    13:         to make the video nottoo dense\n                                                que   seamuy                         more enjoyable,\n                               \"14       seam|uadels Ilevadero,      part    14: and              but from this video\n                                      que                   peroque          15: \"recognize _\n                               \"15\"  reconocer\n                               \"16\" \"diferentes                              16:\"different\n                                                                                        or With different\n                                    'relojes   diferentes                    17: \"watches\n                               \"17           con        movimientos                                     movements\n           We convert the input transcript to a JSON, split them into groups of maximum 50 segments, and have overlaps in\n        segment groups to preserve translation context. For groups with more than 50 segments, the LLMs suffer from a\n        needle in a stack issue, with poor performance, unequal input outputs, and sometimes exceed input tokens limit.", "start_char_idx": 0, "end_char_idx": 2675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "62e197a7-f51f-48eb-bbd9-068dc65c2b8c": {"__data__": {"id_": "62e197a7-f51f-48eb-bbd9-068dc65c2b8c", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2dae1be-e4fc-4d52-bf93-35eed594e4b0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "6f5e95b841c18a850aef4aa12ee0be93d466d74aa60ed5a854f7d66dc7ab4465", "class_name": "RelatedNodeInfo"}}, "text": "Verified Transcript\n                                             Preprocessing\n                                  Group     Split into Groups\n                         LLM :Contextual Translation::                                                            Technologies used -\n                                                                                          LLM - OpenAI gpt-4 for contextual translation\n           LLM Validator ::JSON and Balanced I/0:    Yes           Groups 2-n             Orchestrator -\n                                Fail                                                   Langchain(LLMChain,FewShotPromptTemplate,Retr\n                                                                                          Embeddings - gpt-3.5 embeddings(small 1536 dim\n                                        #Retry 32                                      mapping engine to map input and output segments in\n                       Pass                No                                          RetryOutputParser fails.\n                                  Semantic Mapping Engine   Same Process for Groups 2-n   Python Libraries - Langchain\n                                  Grouped Translation Block\n                                Postprocess :  Split Translations:\n           Every group is translated in one LLM prompt, so that Translation context is preserved. We orchestrate this using\n        Langchain, with a Few Shots example, along with Output Parser to force the output to the defined formats.\n           We have a separate Semantic Mapping Engine, to map the input transcript and output translation in case there is a\n        mismatch between the number of segments in reference and translated group.\n           Once each group is translated, we recombine the groups to get the translated segments. Every segment is assigned\n        a speaker, that is mapped to an AI voice.\nlocalhost:3001                                                                                                                             4/64/9/24, 7:56 PM                                                                     React App\n            Now the main challenge with translation is that the same text, when translated across languages, has different\n         numbers of characters, and therefore different time to say the same thing. For example:\n                          0:07\n                          0:07 / 0:07\n                               / 0:07                                                         0:09\n                                                                                              0:09 / 0:09\n                                      English                                                      / 0:09  Hindi\n            To handle this, we have a separate LLM engine that utilises our voice models. For every voice, we have developed\n         ML models across languages to predict with ~80% accuracy, the expected duration of the generated audio from a\n         particular text.                                                                          Technologies used -\n                        Postprocess :: Split Translations: =                      scikit-learn, pytorch - LinerRegressionModel and\n                                                Translation #1                SVM for predicting duration of audio.", "start_char_idx": 2719, "end_char_idx": 6073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-2": {"__data__": {"id_": "node-2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91fb9c65-9f64-4cf9-b6ca-f3df23d8d107", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44cce72dfc54eb3ebcb4be1566f418d29728824ef4849223f273db80f6c9bdca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51c374b4-7b51-409b-9a79-14b6446e823f", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1539cdfa-7a7b-4e3e-98b0-367c5543a119", "node_type": "1", "metadata": {}, "hash": "6bb53f7657b4e4ad1ae5b6456ca6eb7dff2979fe423d19fd12723a32e192eb73", "class_name": "RelatedNodeInfo"}}, "text": "que  usadoporun grangrupo _              7: 'and it's used by\n                               \"8\"             habrlunoe9isvistolo              And well,  you        in the title\n                                     bueno,como               en   que                   as    ve seen           ana\n                              Powermatic 80                                     \"The ideaof this video is\n                               \"9\"    ideade estevluggeddeo                      \"to give    bit of general\n                                    La                   es_                 10:        you               contextabr\n                                                  contexto general\n                               \"10\"  daros  poco               sobre\n                                         un                                  11: 'what its variants  what itsmost\n                                                 sus variantes,culuedelles                       are             oui\n                               \"11   culuoellesson                                       comes from, what changes it ha:\n                                    de dludaf3ndeviene, quluade9             12: 'where it\n                               \"12\"                         cambiostiene     different typesare\n                                 distintos\n                              los        tipos                                   \"I\\'1l try\n                               \"13   Intentarlugge9         denso  vlugge    13:         to make the video nottoo dense\n                                                que   seamuy                         more enjoyable,\n                               \"14       seam|uadels Ilevadero,      part    14: and              but from this video\n                                      que                   peroque          15: \"recognize _\n                               \"15\"  reconocer\n                               \"16\" \"diferentes                              16:\"different\n                                                                                        or With different\n                                    'relojes   diferentes                    17: \"watches\n                               \"17           con        movimientos                                     movements\n           We convert the input transcript to a JSON, split them into groups of maximum 50 segments, and have overlaps in\n        segment groups to preserve translation context. For groups with more than 50 segments, the LLMs suffer from a\n        needle in a stack issue, with poor performance, unequal input outputs, and sometimes exceed input tokens limit.\n                                           Verified Transcript\n                                             Preprocessing\n                                  Group     Split into Groups\n                         LLM :Contextual Translation::                                                            Technologies used -\n                                                                                          LLM - OpenAI gpt-4 for contextual translation\n           LLM Validator ::JSON and Balanced I/0:    Yes           Groups 2-n             Orchestrator -\n                                Fail                                                   Langchain(LLMChain,FewShotPromptTemplate,Retr\n                                                                                          Embeddings - gpt-3.5 embeddings(small 1536 dim\n                                        #Retry 32                                      mapping engine to map input and output segments in\n                       Pass                No                                          RetryOutputParser fails.\n                                  Semantic Mapping Engine   Same Process for Groups 2-n   Python Libraries - Langchain\n                                  Grouped Translation Block\n                                Postprocess :  Split Translations:\n           Every group is translated in one LLM prompt, so that Translation context is preserved. We orchestrate this using\n        Langchain, with a Few Shots example, along with Output Parser to force the output to the defined formats.\n           We have a separate Semantic Mapping Engine, to map the input transcript and output translation in case there is a\n        mismatch between the number of segments in reference and translated group.\n           Once each group is translated, we recombine the groups to get the translated segments. Every segment is assigned\n        a speaker, that is mapped to an AI voice.\nlocalhost:3001                                                                                                                             4/64/9/24, 7:56 PM                                                                     React App\n            Now the main challenge with translation is that the same text, when translated across languages, has different\n         numbers of characters, and therefore different time to say the same thing. For example:\n                          0:07\n                          0:07 / 0:07\n                               / 0:07                                                         0:09\n                                                                                              0:09 / 0:09\n                                      English                                                      / 0:09  Hindi\n            To handle this, we have a separate LLM engine that utilises our voice models. For every voice, we have developed\n         ML models across languages to predict with ~80% accuracy, the expected duration of the generated audio from a\n         particular text.                                                                          Technologies used -\n                        Postprocess :: Split Translations: =                      scikit-learn, pytorch - LinerRegressionModel and\n                                                Translation #1                SVM for predicting duration of audio.", "start_char_idx": 11072, "end_char_idx": 17145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "35d99842-55d2-4f87-a625-13bff6766e4a": {"__data__": {"id_": "35d99842-55d2-4f87-a625-13bff6766e4a", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "9d5368ded17697c59ea7089578116d9ff0d255e186a6c464c403734c3242f9d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2c40411-6557-4c16-91bd-bbcd1616fc23", "node_type": "1", "metadata": {}, "hash": "7d4cc9c83f8185ffa666297dac3e274a10bc5ce7cf529fe403eadccac5dd3cd5", "class_name": "RelatedNodeInfo"}}, "text": "(voice models)\n                                             Translation Segment#1                Orchestrator - Langchain- A custom\n                Translation #2-n                                              agent(AgentExecutor) - CATTM(Context Aware Text\n                                                                              Time Matcher) with 3 custom tools(@tools)-\n                            Target timeTranslation ContextTranslationVoice Model\n                                                                 Predicted time      Text Adjustment Tool - A tool built with\n           Same process for Segment #2-n  LLM Agent :: Text Adjustment::         LLMChain which takes input as contexual\n                               Low At          High At                           translation and gives out smaller or larger text as\n            Processed Translation Segment   Text Adjustment                      required.\n                                                             Updated Predicted time  GPT-4 evaluator Tool - A tool built with\n                                                  Validation Fail                load_evaluator which uses gpt-4 to critique the\n                                            Self Critiqe LLM                     quality of adjusted text given by Text Adjustment\n                                             Validation Pass                     tool.\n                                                    Voice Model                      Voice Model Tool - A tool that takes in text and\n                                                                                 speaker as input and spits out the time required to\n                                                                                 speak that text by that speaker.", "start_char_idx": 0, "end_char_idx": 1784, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a2c40411-6557-4c16-91bd-bbcd1616fc23": {"__data__": {"id_": "a2c40411-6557-4c16-91bd-bbcd1616fc23", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "9d5368ded17697c59ea7089578116d9ff0d255e186a6c464c403734c3242f9d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35d99842-55d2-4f87-a625-13bff6766e4a", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "e3d1361e1400459543958b5964eade7fb9be874015e7d9cb37265740cf98fa0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "680c248a-b463-4ed2-96c4-bad3107b1aa7", "node_type": "1", "metadata": {}, "hash": "33979c950e3d14fb9e702e53b0e7be7beaed5687d0643d0ecb21699ae1217dbb", "class_name": "RelatedNodeInfo"}}, "text": "LLM - LLama2-7b for text adjustment(finetuned\n                                                                              using LORA), GPT-4 for evaluating outputs of llama2-\n                                                                              7b. HuggingFace - PEFT for finetuning llam2-7b using\n                                                                              LORA on custom dataset of shortening and lengthening\n                                                                              text.\n            Utilising those models, we run this process for every translated segment, and modify the translation based on the\n         acceptable limits of \u0394t, for every voice.\n            We have a LLM agent to adjust the translation, and a Self critique LLM to judge the modified translation on\n         parameters such as Grammar, Context loss, Colloquialism etc. Once the translations are modified, these are reviewed\n         by a Human in the loop for final validation.\n       Audio Generation\n            Once the translations are verified, we generate audio for each translated segment. To ensure that the final dubbing\n         is accurate, engaging and preserves the emotions from the original track, we have another LLM engine, that adjusts\n         translation based on continuous feedback, and regenerates audio multiple times to achieve this target.", "start_char_idx": 1867, "end_char_idx": 3253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "680c248a-b463-4ed2-96c4-bad3107b1aa7": {"__data__": {"id_": "680c248a-b463-4ed2-96c4-bad3107b1aa7", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "9d5368ded17697c59ea7089578116d9ff0d255e186a6c464c403734c3242f9d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2c40411-6557-4c16-91bd-bbcd1616fc23", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c30b03e96e1a992054d79d96345a534079cdbfd5a91be732dcc6ff9e3c64fd71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fc9774d-2a71-49c0-a5c2-4bbe5e70db8b", "node_type": "1", "metadata": {}, "hash": "7baf1dbf21907eb22cd19829cbab9ed48cfb7fd2f699c16b3aceaccaa15a8169", "class_name": "RelatedNodeInfo"}}, "text": "localhost:3001                                                                                                                         5/64/9/24, 7:56 PM               Verified Translation segments               React App\n           Translation segment #n\n                                           Reference segment\n             Audio generation                source lang::\n                    High 4\n          actual audio time       target time\n                 LLM                                                                   Technologies used -\n              Text Adjustment:\n                 Low $                        Audio features            ElevenLabs - For Text to Speech and voice cloning.\n             Modified Translation                                       Orchestrator - Langchain - CATTM agent\n                with Audio     Translation #n-1, #n-2\n            Audio Features      Audiofeatures                           Wav2vec and SpeechBrain - For Audio similarity and\n                                                                     continuity engine.", "start_char_idx": 3254, "end_char_idx": 4339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3fc9774d-2a71-49c0-a5c2-4bbe5e70db8b": {"__data__": {"id_": "3fc9774d-2a71-49c0-a5c2-4bbe5e70db8b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "9d5368ded17697c59ea7089578116d9ff0d255e186a6c464c403734c3242f9d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "680c248a-b463-4ed2-96c4-bad3107b1aa7", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "226b98d862e403ab81e12ae1e53ff8661c2bb3b5b7d36ee49325ffd51b6f16ac", "class_name": "RelatedNodeInfo"}}, "text": "LLM : Similarity                     Python Libraries - Langchain, SpeechBrain, pydub,\n                                   and Continuity  Translation segment N| {n}moviepy\n                                    Engine::\n                           Failed    Passed\n                 Audio regenerationFinal Audio for segment #nSame process as Translated #n\n                                              Translated Audio Track\n           In this process, we have a Similarity and continuity engine that takes audio features from previous generated\n        segments, as well as features from the reference audio, and regenerates audio multiple times to ensure that the output\n        audio has similar emotions to the source segment, and when played with previous segments, it should sound\n        continuous.\n           Once this process is done for every segment, the generated audios are verified by our Human experts, and they\n        ensure the final Dubbing is perfect.\nlocalhost:3001                                                                                                                  6/6", "start_char_idx": 4375, "end_char_idx": 5475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ded84fd2-71be-4c5e-b6b7-37eb62a2246a": {"__data__": {"id_": "ded84fd2-71be-4c5e-b6b7-37eb62a2246a", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "9d5368ded17697c59ea7089578116d9ff0d255e186a6c464c403734c3242f9d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbaa96b3-fe9f-4fe1-96b2-00d4918baf5b", "node_type": "1", "metadata": {}, "hash": "f097a7688b97783e9ef6912efcee118fab47eb18e95d1416c82225dae3c268ca", "class_name": "RelatedNodeInfo"}}, "text": "(voice models)\n                                             Translation Segment#1                Orchestrator - Langchain- A custom\n                Translation #2-n                                              agent(AgentExecutor) - CATTM(Context Aware Text\n                                                                              Time Matcher) with 3 custom tools(@tools)-\n                            Target timeTranslation ContextTranslationVoice Model\n                                                                 Predicted time      Text Adjustment Tool - A tool built with\n           Same process for Segment #2-n  LLM Agent :: Text Adjustment::         LLMChain which takes input as contexual\n                               Low At          High At                           translation and gives out smaller or larger text as\n            Processed Translation Segment   Text Adjustment                      required.\n                                                             Updated Predicted time  GPT-4 evaluator Tool - A tool built with\n                                                  Validation Fail                load_evaluator which uses gpt-4 to critique the\n                                            Self Critiqe LLM                     quality of adjusted text given by Text Adjustment\n                                             Validation Pass                     tool.\n                                                    Voice Model                      Voice Model Tool - A tool that takes in text and\n                                                                                 speaker as input and spits out the time required to\n                                                                                 speak that text by that speaker.\n                                                                                  LLM - LLama2-7b for text adjustment(finetuned\n                                                                              using LORA), GPT-4 for evaluating outputs of llama2-\n                                                                              7b. HuggingFace - PEFT for finetuning llam2-7b using\n                                                                              LORA on custom dataset of shortening and lengthening\n                                                                              text.\n            Utilising those models, we run this process for every translated segment, and modify the translation based on the\n         acceptable limits of \u0394t, for every voice.\n            We have a LLM agent to adjust the translation, and a Self critique LLM to judge the modified translation on\n         parameters such as Grammar, Context loss, Colloquialism etc. Once the translations are modified, these are reviewed\n         by a Human in the loop for final validation.\n       Audio Generation\n            Once the translations are verified, we generate audio for each translated segment. To ensure that the final dubbing\n         is accurate, engaging and preserves the emotions from the original track, we have another LLM engine, that adjusts\n         translation based on continuous feedback, and regenerates audio multiple times to achieve this target.", "start_char_idx": 0, "end_char_idx": 3253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "dbaa96b3-fe9f-4fe1-96b2-00d4918baf5b": {"__data__": {"id_": "dbaa96b3-fe9f-4fe1-96b2-00d4918baf5b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "9d5368ded17697c59ea7089578116d9ff0d255e186a6c464c403734c3242f9d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ded84fd2-71be-4c5e-b6b7-37eb62a2246a", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "8c1fb58d8021ca969ef6ca644b6865211a5ec3da14d1ee784bc221eb862c2954", "class_name": "RelatedNodeInfo"}}, "text": "localhost:3001                                                                                                                         5/64/9/24, 7:56 PM               Verified Translation segments               React App\n           Translation segment #n\n                                           Reference segment\n             Audio generation                source lang::\n                    High 4\n          actual audio time       target time\n                 LLM                                                                   Technologies used -\n              Text Adjustment:\n                 Low $                        Audio features            ElevenLabs - For Text to Speech and voice cloning.\n             Modified Translation                                       Orchestrator - Langchain - CATTM agent\n                with Audio     Translation #n-1, #n-2\n            Audio Features      Audiofeatures                           Wav2vec and SpeechBrain - For Audio similarity and\n                                                                     continuity engine.\n                                   LLM : Similarity                     Python Libraries - Langchain, SpeechBrain, pydub,\n                                   and Continuity  Translation segment N| {n}moviepy\n                                    Engine::\n                           Failed    Passed\n                 Audio regenerationFinal Audio for segment #nSame process as Translated #n\n                                              Translated Audio Track\n           In this process, we have a Similarity and continuity engine that takes audio features from previous generated\n        segments, as well as features from the reference audio, and regenerates audio multiple times to ensure that the output\n        audio has similar emotions to the source segment, and when played with previous segments, it should sound\n        continuous.\n           Once this process is done for every segment, the generated audios are verified by our Human experts, and they\n        ensure the final Dubbing is perfect.\nlocalhost:3001                                                                                                                  6/6", "start_char_idx": 3254, "end_char_idx": 5475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-3": {"__data__": {"id_": "node-3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91fb9c65-9f64-4cf9-b6ca-f3df23d8d107", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "44cce72dfc54eb3ebcb4be1566f418d29728824ef4849223f273db80f6c9bdca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c503c8c9-87d3-487e-b93f-662f742b7737", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}, "hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "class_name": "RelatedNodeInfo"}}, "text": "(voice models)\n                                             Translation Segment#1                Orchestrator - Langchain- A custom\n                Translation #2-n                                              agent(AgentExecutor) - CATTM(Context Aware Text\n                                                                              Time Matcher) with 3 custom tools(@tools)-\n                            Target timeTranslation ContextTranslationVoice Model\n                                                                 Predicted time      Text Adjustment Tool - A tool built with\n           Same process for Segment #2-n  LLM Agent :: Text Adjustment::         LLMChain which takes input as contexual\n                               Low At          High At                           translation and gives out smaller or larger text as\n            Processed Translation Segment   Text Adjustment                      required.\n                                                             Updated Predicted time  GPT-4 evaluator Tool - A tool built with\n                                                  Validation Fail                load_evaluator which uses gpt-4 to critique the\n                                            Self Critiqe LLM                     quality of adjusted text given by Text Adjustment\n                                             Validation Pass                     tool.\n                                                    Voice Model                      Voice Model Tool - A tool that takes in text and\n                                                                                 speaker as input and spits out the time required to\n                                                                                 speak that text by that speaker.\n                                                                                  LLM - LLama2-7b for text adjustment(finetuned\n                                                                              using LORA), GPT-4 for evaluating outputs of llama2-\n                                                                              7b. HuggingFace - PEFT for finetuning llam2-7b using\n                                                                              LORA on custom dataset of shortening and lengthening\n                                                                              text.\n            Utilising those models, we run this process for every translated segment, and modify the translation based on the\n         acceptable limits of \u0394t, for every voice.\n            We have a LLM agent to adjust the translation, and a Self critique LLM to judge the modified translation on\n         parameters such as Grammar, Context loss, Colloquialism etc. Once the translations are modified, these are reviewed\n         by a Human in the loop for final validation.\n       Audio Generation\n            Once the translations are verified, we generate audio for each translated segment. To ensure that the final dubbing\n         is accurate, engaging and preserves the emotions from the original track, we have another LLM engine, that adjusts\n         translation based on continuous feedback, and regenerates audio multiple times to achieve this target.\nlocalhost:3001                                                                                                                         5/64/9/24, 7:56 PM               Verified Translation segments               React App\n           Translation segment #n\n                                           Reference segment\n             Audio generation                source lang::\n                    High 4\n          actual audio time       target time\n                 LLM                                                                   Technologies used -\n              Text Adjustment:\n                 Low $                        Audio features            ElevenLabs - For Text to Speech and voice cloning.\n             Modified Translation                                       Orchestrator - Langchain - CATTM agent\n                with Audio     Translation #n-1, #n-2\n            Audio Features      Audiofeatures                           Wav2vec and SpeechBrain - For Audio similarity and\n                                                                     continuity engine.\n                                   LLM : Similarity                     Python Libraries - Langchain, SpeechBrain, pydub,\n                                   and Continuity  Translation segment N| {n}moviepy\n                                    Engine::\n                           Failed    Passed\n                 Audio regenerationFinal Audio for segment #nSame process as Translated #n\n                                              Translated Audio Track\n           In this process, we have a Similarity and continuity engine that takes audio features from previous generated\n        segments, as well as features from the reference audio, and regenerates audio multiple times to ensure that the output\n        audio has similar emotions to the source segment, and when played with previous segments, it should sound\n        continuous.\n           Once this process is done for every segment, the generated audios are verified by our Human experts, and they\n        ensure the final Dubbing is perfect.\nlocalhost:3001                                                                                                                  6/6", "start_char_idx": 17145, "end_char_idx": 22620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5ec4dccc-97a2-4154-b465-75e5535fdd35": {"__data__": {"id_": "5ec4dccc-97a2-4154-b465-75e5535fdd35", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "translatetracks blog", "path": "cache\\experience-blogs\\translatetracks-blog\\parsed\\images\\a2e5988e-f89f-495d-8e5d-5e570e36decc-img_p1_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you provided seems to be related to a web application that offers video and audio processing services. The application appears to be sophisticated, utilizing cloud infrastructure and a variety of programming technologies. It is hosted on an AWS EC2 Ubuntu 22 Server, with a backend in Python FastAPI and a frontend in React. MongoDB is used for storing video and user data, while S3 hosts the actual audio and video files.\n\nThe application integrates various large language models (LLMs) such as ChatGPT, Claude, and LLama2, and uses Langfuse for logging and monitoring the performance of these models. Additionally, PostgreSQL is employed for logging user interactions on the platform, and NewRelic is used for monitoring the health of the server instances.\n\nThe process described involves uploading a video and then using a background remover to separate the vocal track from any background noise, resulting in a clean audio track that is conducive to accurate transcription. The transcription process includes speaker diarization, which assigns unique labels to each speaker in the audio.\n\nA combination of speech-to-text APIs, including Deepgram, AssemblyAI, and Gladia, is used to transcribe the audio into text. An in-house ASR (Automatic Speech Recognition) engine optimizes the output from these services to minimize errors and improve the accuracy of speaker diarization.\n\nThe final goal of the application is to produce lip-synchronized audio that matches the video content. To achieve this, the transcript is broken down into smaller segments that align with", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2f7c053-d6a0-4bf9-b71b-d414c9e92c37": {"__data__": {"id_": "a2f7c053-d6a0-4bf9-b71b-d414c9e92c37", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "translatetracks blog", "path": "cache\\experience-blogs\\translatetracks-blog\\parsed\\images\\a2e5988e-f89f-495d-8e5d-5e570e36decc-img_p2_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The scene captures a moment from a video, likely related to a travel or cultural program, featuring two individuals standing with a picturesque backdrop that suggests they are on the Amalfi Coast in Italy, known for its stunning coastal scenery. The person in the foreground is gesturing with their hand, possibly emphasizing a point or greeting the audience, while the person in the background is standing beside them, both appearing to be presenting or narrating. The setting is serene and visually appealing, indicative of the region's reputation for beauty.\n\nThe context provided suggests that this is part of a React application designed for video editing or dubbing, with features that allow users to transcribe, translate, and edit video content. The application seems to include a timeline editor where segments of the video can be managed and audio can be extracted or modified. The use of various technologies and services, such as FFmpeg for audio extraction and ASR (Automatic Speech Recognition) services like Assembly AI, Deepgram, and Gladia, indicates a sophisticated platform for processing and optimizing video and audio content for purposes such as dubbing or creating subtitles. The interface appears user-friendly, with tools for ensuring that the transcription and translation segments match, which is crucial for accurate dubbing and subtitling.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb994239-87ef-4c19-8fff-caa3e85385e4": {"__data__": {"id_": "fb994239-87ef-4c19-8fff-caa3e85385e4", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "translatetracks blog", "path": "cache\\experience-blogs\\translatetracks-blog\\parsed\\images\\a2e5988e-f89f-495d-8e5d-5e570e36decc-img_p2_2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you're referring to seems to be discussing a process that involves a React application used for handling video and audio content, specifically for transcription and translation purposes. It outlines a workflow that starts with a video source, which can be directly uploaded or linked from YouTube. From this video, audio is extracted using a tool called FFmpeg, which is also used to detect audio and video breakpoints.\n\nThe audio track is then processed to separate the vocal track from the background track, possibly for clearer transcription or for dubbing purposes. This separation is achieved using services like AssemblyAI, Deepgram, and Gladia, which are mentioned as ASR (Automatic Speech Recognition) services.\n\nOnce the audio is processed, an optimized transcript is generated. This transcript is then fed into a dubbing editor, where it can be verified and corrected by a human to ensure accuracy. The transcription process is followed by translation, which emphasizes the importance of maintaining the same number of segments between the original transcript and the translated version.\n\nThe workflow is likely visualized in a diagram that shows the connections and sequence of steps from the video source to the final dubbed and translated output. This diagram would typically include nodes representing each step or service in the process, with arrows indicating the flow of data between these nodes. The diagram serves as a guide to understanding how the various technologies and services are integrated to facilitate the transcription and translation of video content.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad259d2b-e9e8-4776-844f-bbab56c48774": {"__data__": {"id_": "ad259d2b-e9e8-4776-844f-bbab56c48774", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "translatetracks blog", "path": "cache\\experience-blogs\\translatetracks-blog\\parsed\\images\\a2e5988e-f89f-495d-8e5d-5e570e36decc-img_p3_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you provided appears to be a transcript of a video, possibly related to watchmaking and the Powermatic 80 movement, which is a caliber used in various watches. The transcript is in Spanish, with an English translation provided alongside it. The text outlines the structure of a video presentation, where the speaker intends to welcome viewers, discuss the Powermatic 80 movement, its variants, and the different watches that use this movement. The aim is to educate the audience about the movement's characteristics and to help them recognize different watches with these movements.\n\nAdditionally, the text mentions a process for converting the transcript into a JSON format, splitting it into manageable segments, and translating it using language models, specifically OpenAI's GPT-4, for contextual translation. There's a mention of using technologies like Langchain for orchestration, embeddings from GPT-3.5, and Python libraries. The process includes steps like validation, semantic mapping, and post-processing to ensure accurate and contextually relevant translations. The final goal seems to be to assign each translated segment to an AI voice for a multilingual video or presentation. The reference to \"localhost:3001\" suggests that there might be a local server or application running to manage or display this content.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bddc581-befe-4daa-a686-d3391dd4b79d": {"__data__": {"id_": "3bddc581-befe-4daa-a686-d3391dd4b79d", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "translatetracks blog", "path": "cache\\experience-blogs\\translatetracks-blog\\parsed\\images\\a2e5988e-f89f-495d-8e5d-5e570e36decc-img_p3_2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you've provided seems to be related to a process for translating and processing language data. It outlines a method for converting a transcript into a JSON format, which is then split into groups to manage the translation process effectively. This is particularly important for large sets of data, as it helps to maintain context and improve the accuracy of the translation.\n\nThe process involves using a language model, specifically OpenAI's GPT-4, for contextual translation. The language model appears to be validated by a system that ensures the input and output are balanced and that the translation is accurate. If there are more than 50 segments, the data is managed by splitting it into groups and ensuring there is some overlap to preserve the translation context.\n\nThere is mention of a retry mechanism, suggesting that the system can make multiple attempts to get an accurate translation if necessary. Additionally, a Semantic Mapping Engine is used to ensure that the input transcript and the output translation match up correctly, which is crucial when dealing with segmented data.\n\nThe translated data is then recombined, and each segment is assigned a speaker, which could then be mapped to an AI voice for applications like automated dubbing or voiceovers.\n\nThe process is visualized in a flowchart that outlines the steps from preprocessing to postprocessing, including the use of technologies like Langchain and Python libraries. The flowchart also indicates the use of an orchestrator and embeddings, which are likely used to enhance the translation process.\n\nThis method seems to be a sophisticated approach to handling", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98453adf-0b12-423a-92b4-3be9262e8bf9": {"__data__": {"id_": "98453adf-0b12-423a-92b4-3be9262e8bf9", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "translatetracks blog", "path": "cache\\experience-blogs\\translatetracks-blog\\parsed\\images\\a2e5988e-f89f-495d-8e5d-5e570e36decc-img_p4_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you're referring to presents a complex workflow for handling the translation and dubbing process of audio content across different languages. It outlines a system that takes into account the varying lengths of time it takes to speak the same text in different languages. This system uses a variety of technologies and methodologies to ensure that the translated audio matches the timing of the original content as closely as possible.\n\nThe process begins with the translation of the original text into the target language. A specialized LLM (Large Language Model) engine is used to predict the duration of the audio that will result from the translated text. This prediction is made with the help of machine learning models that have been trained to estimate the time it will take for a voice model to speak a given text.\n\nThe workflow includes several steps and tools:\n\n1. Postprocessing to split translations and predict the duration of audio using machine learning models like Linear Regression and SVM.\n2. An orchestrator that manages the translation process, using a Context Aware Text Time Matcher to ensure the timing of the translation aligns with the original audio.\n3. A text adjustment tool that modifies the translated text to fit the required time frame, making it longer or shorter as necessary.\n4. An evaluation tool that uses GPT-4 to assess the quality of the adjusted text, checking for grammar, context retention, and naturalness of language.\n5. A voice model tool that predicts the time required for a specific speaker to deliver the translated text.\n\nThe system also includes a feedback loop", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "caf559c5-229b-46ec-a75d-31c757d3d56d": {"__data__": {"id_": "caf559c5-229b-46ec-a75d-31c757d3d56d", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "translatetracks blog", "path": "cache\\experience-blogs\\translatetracks-blog\\parsed\\images\\a2e5988e-f89f-495d-8e5d-5e570e36decc-img_p5_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you provided outlines a sophisticated audio processing workflow that involves several steps and technologies to produce high-quality translated audio tracks. The process begins with the generation of an audio segment from a text source in a specific language. This initial audio is then subjected to a time adjustment to match the target time constraints.\n\nA key component of this workflow is the Similarity and Continuity Engine, which is responsible for ensuring that the emotional tone of the translated audio is consistent with the original audio segment. It also ensures that the new audio segments are seamless when played consecutively, providing a continuous listening experience.\n\nTechnologies such as ElevenLabs are used for text-to-speech and voice cloning, while Wav2vec and SpeechBrain contribute to the audio similarity and continuity engine. Python libraries like Langchain, SpeechBrain, pydub, and moviepy are also integral to the process.\n\nThe workflow includes a feedback loop where audio features are modified and regenerated until the output passes the quality checks. Human experts play a crucial role in the final verification stage, ensuring that the dubbed audio meets the desired standards.\n\nThe diagram associated with this process likely illustrates the flow of these steps, showing how each segment is processed, verified, and how the final audio is produced. It probably includes decision points indicating whether the audio has passed or failed certain checks, and it may also show how the final audio for each segment is integrated into the translated audio track.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"f10e2efe-f9c2-48d0-86d9-59da9190ced3": {"doc_hash": "79fd1ab113dd48a3aa1a8c7a8067e7e8d02f14aa82947c24e4e268ff64ea23ab", "ref_doc_id": "node-0"}, "7922d048-95e6-4240-b1ac-86a8eb669047": {"doc_hash": "44e52f936f05bcf549827d210c803e6aa294cafe1f7892461a6fd72c06e8d447", "ref_doc_id": "node-0"}, "eadbda39-d539-4b5d-a928-3adf2b5ad1b8": {"doc_hash": "90f6147dc2b78b7ebb35ed1fdf6818956438ca2362c0170da5bec093621098fa", "ref_doc_id": "node-0"}, "5e468cf4-db15-4494-9bfb-5e514d714960": {"doc_hash": "dca86935f0685b6baca85d815be0f70e19f2b47181c244a59c70a63d41252e2e", "ref_doc_id": "node-0"}, "cbce1c42-cd54-44c0-b5e1-c254fc25695c": {"doc_hash": "663b34d5d062eb359f2f385a845a407dc0199314a9cf5c43b2bc3861f657d81e", "ref_doc_id": "node-0"}, "aebd560f-675d-4420-8182-8da5094e0d6d": {"doc_hash": "5d9643bf5af1b6545c42ee82c2cb29bae82c63b1972951767b1e814f5b2fa2d6", "ref_doc_id": "node-0"}, "69ac1a80-545b-4a57-b6a8-336a4cfc0423": {"doc_hash": "aaab93af2de58f29f6b2154a147db026d7c23df308b157a9f550bcdf72165c65", "ref_doc_id": "node-0"}, "node-0": {"doc_hash": "1a539807016d4adfb616542c49e3ffd106cc2ff896dd719e39abdf7fc6fde298", "ref_doc_id": "91fb9c65-9f64-4cf9-b6ca-f3df23d8d107"}, "d9c07de1-84d1-40af-b7e1-b8660892df75": {"doc_hash": "0f7f75fc345315c3531611ac9c33d45e272d0e935db6cc97d684df189466720d", "ref_doc_id": "node-1"}, "b165a743-98e0-4b9e-b4e8-50f5a62c2162": {"doc_hash": "72d97b448948bbf0aebbd6156c3e9351957bb43cdd70e84c29283399903f4225", "ref_doc_id": "node-1"}, "0fc6650f-ce3b-4615-8d26-c795d96f5963": {"doc_hash": "4590529e6de047fc5ec7ba5681510c1f1d6e075db4c935f5dcdefb7948144ee4", "ref_doc_id": "node-1"}, "55d4b466-d6e3-4347-b3f7-7d8f88e5eb7c": {"doc_hash": "ce730c9f2bf0a77bc48b426186abdefb4527953614dd89efa4925ea817111f3f", "ref_doc_id": "node-1"}, "0979c205-2c8c-4016-8489-943fe27f0367": {"doc_hash": "affadbf1ad78eb820b199dda631fd447a8bd37c3e52a6b7031ea3dc39dad999e", "ref_doc_id": "node-1"}, "16d73c51-f156-48d3-988b-2059b5036e15": {"doc_hash": "8fe17b5cdd28b1237c9a2551f44a47726f3314201f1ce6a09e22c1fb25dfcb4c", "ref_doc_id": "node-1"}, "node-1": {"doc_hash": "44b2ae2957693cede2b14f40b99ddaaf2f7768f4b0f670f0305b2215c07a9fda", "ref_doc_id": "91fb9c65-9f64-4cf9-b6ca-f3df23d8d107"}, "33387acb-07d1-4762-8ba9-76e010447d29": {"doc_hash": "f91ce012666760362d9a6ccb54efcc94996b1284fbb6f138d840f82bf2d2b243", "ref_doc_id": "node-2"}, "2306d4fd-35d6-4700-8a64-55af944a1187": {"doc_hash": "c723334d49de28262266d58b21bbd65e57ab7304b8f4485b680da98d1bb2540a", "ref_doc_id": "node-2"}, "3161741e-8d36-468c-ab49-7d8f4964a49b": {"doc_hash": "2111d9c7471ccf6983fb0245696eb8ab5fe89a65d1f27fdfaa3deb0ac97fe2c4", "ref_doc_id": "node-2"}, "9374339e-5ad4-4356-a690-d9da11aec074": {"doc_hash": "1e9ab8847dd98e05862f5e5f8babb5d9e02ad955295762dc76214ec2949d4fcf", "ref_doc_id": "node-2"}, "7a9d83a8-28ff-405a-a08f-5252aacd1ef2": {"doc_hash": "6b22feb880cc20de54e8a3dcb920d5ec4f398a3c979a4ed1084988187f5573e2", "ref_doc_id": "node-2"}, "c2dae1be-e4fc-4d52-bf93-35eed594e4b0": {"doc_hash": "6f5e95b841c18a850aef4aa12ee0be93d466d74aa60ed5a854f7d66dc7ab4465", "ref_doc_id": "node-2"}, "62e197a7-f51f-48eb-bbd9-068dc65c2b8c": {"doc_hash": "05f631d4fcce0759b87695c8d09b9d6bd333ac4714fd4c13b1544a9d4531c4a9", "ref_doc_id": "node-2"}, "node-2": {"doc_hash": "c5be5caeeeb05bddb78516d2c28a3bc197d6e127cc8235052ec5467951f16a50", "ref_doc_id": "91fb9c65-9f64-4cf9-b6ca-f3df23d8d107"}, "35d99842-55d2-4f87-a625-13bff6766e4a": {"doc_hash": "e3d1361e1400459543958b5964eade7fb9be874015e7d9cb37265740cf98fa0f", "ref_doc_id": "node-3"}, "a2c40411-6557-4c16-91bd-bbcd1616fc23": {"doc_hash": "c30b03e96e1a992054d79d96345a534079cdbfd5a91be732dcc6ff9e3c64fd71", "ref_doc_id": "node-3"}, "680c248a-b463-4ed2-96c4-bad3107b1aa7": {"doc_hash": "226b98d862e403ab81e12ae1e53ff8661c2bb3b5b7d36ee49325ffd51b6f16ac", "ref_doc_id": "node-3"}, "3fc9774d-2a71-49c0-a5c2-4bbe5e70db8b": {"doc_hash": "0fa521b534bf1eac95beece63e68047be81e0ec681ef7ffb1b2a25cbfda15cfe", "ref_doc_id": "node-3"}, "ded84fd2-71be-4c5e-b6b7-37eb62a2246a": {"doc_hash": "8c1fb58d8021ca969ef6ca644b6865211a5ec3da14d1ee784bc221eb862c2954", "ref_doc_id": "node-3"}, "dbaa96b3-fe9f-4fe1-96b2-00d4918baf5b": {"doc_hash": "515c25b8aeb9d9ced21bb7ef7704aadeebb22967b46372f95f0753a9e2d0d246", "ref_doc_id": "node-3"}, "node-3": {"doc_hash": "9d5368ded17697c59ea7089578116d9ff0d255e186a6c464c403734c3242f9d6", "ref_doc_id": "91fb9c65-9f64-4cf9-b6ca-f3df23d8d107"}, "5ec4dccc-97a2-4154-b465-75e5535fdd35": {"doc_hash": "06a7c2dca986e8f2bc030627b4cc2ab02cf45e0a03e63cd57a988fcce0267920"}, "a2f7c053-d6a0-4bf9-b71b-d414c9e92c37": {"doc_hash": "ff232df3fdb112aa11ee1a0e7151f41a0ffa9c47f997691e7987aef7bf0aa759"}, "fb994239-87ef-4c19-8fff-caa3e85385e4": {"doc_hash": "653b48c7fd3eb507b623f42b5967d655a81a0fe7588230d3f15c5608eac54fd7"}, "ad259d2b-e9e8-4776-844f-bbab56c48774": {"doc_hash": "e32b867ef6e135a43bb0b2daf40c3ae6f182b37e4b10db24dd1b4f4b153d277d"}, "3bddc581-befe-4daa-a686-d3391dd4b79d": {"doc_hash": "7f1e0da35086c6c2e16f63b8c459d9ec435b86c67e834c0c229679f47ca9cdc4"}, "98453adf-0b12-423a-92b4-3be9262e8bf9": {"doc_hash": "a2d95bd3558b9828b4afb4b1bbfa213ae32da4fe899f3ba072fe0b9a1b4079f7"}, "caf559c5-229b-46ec-a75d-31c757d3d56d": {"doc_hash": "580e4adc2ef4ab4b5bbf79f27dd78e2c51ec09a7f3f095fe3e00a923cfe02275"}}, "docstore/ref_doc_info": {"node-0": {"node_ids": ["f10e2efe-f9c2-48d0-86d9-59da9190ced3", "7922d048-95e6-4240-b1ac-86a8eb669047", "eadbda39-d539-4b5d-a928-3adf2b5ad1b8", "5e468cf4-db15-4494-9bfb-5e514d714960", "cbce1c42-cd54-44c0-b5e1-c254fc25695c", "aebd560f-675d-4420-8182-8da5094e0d6d", "69ac1a80-545b-4a57-b6a8-336a4cfc0423"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}}, "91fb9c65-9f64-4cf9-b6ca-f3df23d8d107": {"node_ids": ["node-0", "node-1", "node-2", "node-3"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}}, "node-1": {"node_ids": ["d9c07de1-84d1-40af-b7e1-b8660892df75", "b165a743-98e0-4b9e-b4e8-50f5a62c2162", "0fc6650f-ce3b-4615-8d26-c795d96f5963", "55d4b466-d6e3-4347-b3f7-7d8f88e5eb7c", "0979c205-2c8c-4016-8489-943fe27f0367", "16d73c51-f156-48d3-988b-2059b5036e15"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}}, "node-2": {"node_ids": ["33387acb-07d1-4762-8ba9-76e010447d29", "2306d4fd-35d6-4700-8a64-55af944a1187", "3161741e-8d36-468c-ab49-7d8f4964a49b", "9374339e-5ad4-4356-a690-d9da11aec074", "7a9d83a8-28ff-405a-a08f-5252aacd1ef2", "c2dae1be-e4fc-4d52-bf93-35eed594e4b0", "62e197a7-f51f-48eb-bbd9-068dc65c2b8c"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}}, "node-3": {"node_ids": ["35d99842-55d2-4f87-a625-13bff6766e4a", "a2c40411-6557-4c16-91bd-bbcd1616fc23", "680c248a-b463-4ed2-96c4-bad3107b1aa7", "3fc9774d-2a71-49c0-a5c2-4bbe5e70db8b", "ded84fd2-71be-4c5e-b6b7-37eb62a2246a", "dbaa96b3-fe9f-4fe1-96b2-00d4918baf5b"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "translatetracks blog"}}}}