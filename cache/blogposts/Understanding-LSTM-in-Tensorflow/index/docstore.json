{"docstore/data": {"2621016f-345a-4369-9c1d-45fd6e51e648": {"__data__": {"id_": "2621016f-345a-4369-9c1d-45fd6e51e648", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "3412ab5148d28f9dd30c4ef2e4d8ae8304be09418cfbc12d8058b470ac6b1dd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08786571-48e9-4638-a413-225dc10d656b", "node_type": "1", "metadata": {}, "hash": "60b9415bc108f0f97ff34bd997bc6345150432b0cfda184517cd9caf8123c262", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:32 PM                                                            Understanding LSTM in Tensorflow\n // window.location =\n \"https://www.knowledgemapper.com/knowmap/knowbook/jasdeepchhabra94@gmail.comUnderstandingLSTMinTensorflow(MNIST\n Tensorflow                                                                                                                               HOME\n Understanding LSTM in Tensorflow(MNIST dataset)\n Long Short Term Memory(LSTM) are the most common types of Recurrent Neural Networks used these days.They are mostly used\n with sequential data.An in depth look at LSTMs can be found in this             incredible blog post.\n Our Aim\n As the title suggests,the main aim of this blogpost is to make the reader comfortable with the implementation details of basic LSTM\n network in tensorflow.\n For fulfilling this aim we will take    MNIST   as our dataset.", "start_char_idx": 0, "end_char_idx": 890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "08786571-48e9-4638-a413-225dc10d656b": {"__data__": {"id_": "08786571-48e9-4638-a413-225dc10d656b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "3412ab5148d28f9dd30c4ef2e4d8ae8304be09418cfbc12d8058b470ac6b1dd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2621016f-345a-4369-9c1d-45fd6e51e648", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "77e035093d793a1cdef05239806ae0106ec1b248a2a42e14d821c6d778dcfeb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68d99c6e-d5e9-4d68-9f03-28287eaa67ad", "node_type": "1", "metadata": {}, "hash": "abddb22a00b3afef435918e50eda076e1d7838ba6d78e7b7051e1ae9d7180bde", "class_name": "RelatedNodeInfo"}}, "text": "The MNIST dataset\n The MNIST dataset consists of images of handwritten digits and their corresponding labels.We can download and read the data in\n tensorflow with the help of following in built functionality-\n  from    tensorflow.examples.tutorials.mnist                     import     input_data\n  mnist    =   input_data.read_data_sets(\"/tmp/data/\",                       one_hot=True)\n The data is split into three parts-\n     1.Training data(mnist.train)-55000 images of training data\n     2.Test data(mnist.test)-10000 images of test data\n     3.Validation data(mnist.validation)-5000 images of validation data.\n Shape of the data\n Let us discuss the shape with respect to training data of MNIST dataset.Shapes of all three splits are identical.", "start_char_idx": 892, "end_char_idx": 1642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "68d99c6e-d5e9-4d68-9f03-28287eaa67ad": {"__data__": {"id_": "68d99c6e-d5e9-4d68-9f03-28287eaa67ad", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "3412ab5148d28f9dd30c4ef2e4d8ae8304be09418cfbc12d8058b470ac6b1dd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08786571-48e9-4638-a413-225dc10d656b", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "90f2650ae8e76b19fc602f3e46432a11a3fb4d68d1867f60445ab5a1b95145d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "251cedac-8d1d-49d6-9289-d4d4fcd3b73b", "node_type": "1", "metadata": {}, "hash": "a98a5ede037ab3e5cd9d59143039d39adaa99ac640f5acfa97cebeb77642aa7c", "class_name": "RelatedNodeInfo"}}, "text": "The training set consists of 55000 images of 28 pixels X 28 pixels each.These 784(28X28) pixel values are flattened in form of a single\n vector of dimensionality 784.The collection of all such 55000 pixel vectors(one for each image) is stored in form of a numpy array of\n shape   (55000,784)       and is referred to as   mnist.train.images.\n Each of these 55000 training images are associated with a label representing the class to which that image belongs.There are 10\n such classes(0,1,2\u20269).Class labels are represented in one hot encoded form.Thus the labels are stored in form of numpy array of\n shape   (55000,10)      and is referred to as   mnist.train.labels.\n Why MNIST?", "start_char_idx": 1644, "end_char_idx": 2324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "251cedac-8d1d-49d6-9289-d4d4fcd3b73b": {"__data__": {"id_": "251cedac-8d1d-49d6-9289-d4d4fcd3b73b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "3412ab5148d28f9dd30c4ef2e4d8ae8304be09418cfbc12d8058b470ac6b1dd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68d99c6e-d5e9-4d68-9f03-28287eaa67ad", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "18f866250b39ec7f1866e2d957bd219394f95eb4d14ac4a7219c6c6890d9b884", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f14e274b-2845-4012-a4c2-424354328c9d", "node_type": "1", "metadata": {}, "hash": "9705c0ed2eb99955e5a4ba27d918d47e6aa85645e0bb04e38730d087d1f87418", "class_name": "RelatedNodeInfo"}}, "text": "LSTMs are generally used for complex sequence related problems like language modelling which involves NLP concepts such as\n word embeddings, encoders etc.These topics themselves need a lot of understanding.It would be nice to eliminate these topics to\n concentrate on implementation details of LSTMs in tensorflow such as input formatting,LSTM cells and network designing.\n MNIST gives us such an opportunity.The input data here is just a set of pixel values.We can easily format these values and\n concentrate on implementation details.\n Implementation\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                                      1/114/5/24, 8:32 PM                                                            Understanding LSTM in Tensorflow\n Before getting our hands dirty with code,let us first draw an outline of our implementation.This will make the coding part more\n intuitive.\n A vanilla RNN\n A Recurrent Neural Network,when unrolled through time,can be visualised as-\n                    S                                                                                St                    St+1\n Here,\n     1.x trefers to the input at time step t.", "start_char_idx": 2326, "end_char_idx": 3547, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f14e274b-2845-4012-a4c2-424354328c9d": {"__data__": {"id_": "f14e274b-2845-4012-a4c2-424354328c9d", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "3412ab5148d28f9dd30c4ef2e4d8ae8304be09418cfbc12d8058b470ac6b1dd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "251cedac-8d1d-49d6-9289-d4d4fcd3b73b", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "eb637e5bed1b29d91ee912565f488aca165ae6b763dc932c719dc988bb800692", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "252707a7-ad63-43f1-af1f-f6f5341fb597", "node_type": "1", "metadata": {}, "hash": "742c5ca2b1159900fa254fb581daa2592b5d84625764b8fa357d7df9e43cc678", "class_name": "RelatedNodeInfo"}}, "text": "2.st refers to the hidden state at time step t.It can be visualised as \u201cmemory\u201d of our network.\n     3.o trefers to the output at time step t.\n     4.U,V and W are parameters that are shared across all the time steps.The significance of this parameter sharing is that our model\n       performs same task at each time step with di\u25aferent input.\n What we have achieved by unrolling the RNN,is that at each time step,the network can be visualised as feed forward network taking\n into account the output of the previous time step(signified by the connections between the time steps).\n Two caveats\n Our implementation will hinge upon two main concepts which will make us comfortable with our implementation:\n     1.Interpretation of LSTM cells in tensorflow.\n     2.Formatting inputs before feeding them to tensorflow RNNs.", "start_char_idx": 3553, "end_char_idx": 4370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "252707a7-ad63-43f1-af1f-f6f5341fb597": {"__data__": {"id_": "252707a7-ad63-43f1-af1f-f6f5341fb597", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "3412ab5148d28f9dd30c4ef2e4d8ae8304be09418cfbc12d8058b470ac6b1dd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f14e274b-2845-4012-a4c2-424354328c9d", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "7297767f922f960eb4b105bac95c604ff0f7aa4cfb4824705b45a9dc5d0f26be", "class_name": "RelatedNodeInfo"}}, "text": "Interpretation of LSTM cells in tensorflow\n A basic LSTM cell is declared in tensorflow as-\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                               2/114/5/24, 8:32 PM                                                  Understanding LSTM in Tensorflow\n  tf.contrib.rnn.BasicLSTMCell(num_units)\n here  num_unitsrefers to the number of units in LSTM cell.", "start_char_idx": 4372, "end_char_idx": 4803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "93048877-9bb4-44e7-ae70-26cc6882cf1d": {"__data__": {"id_": "93048877-9bb4-44e7-ae70-26cc6882cf1d", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "3412ab5148d28f9dd30c4ef2e4d8ae8304be09418cfbc12d8058b470ac6b1dd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81ad88d5-3cd5-40e3-b05e-668e9a7ea24e", "node_type": "1", "metadata": {}, "hash": "3d9cc99e373cc6eef69377fa9f370ad968903431cf153941f2c732eceb284d81", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:32 PM                                                            Understanding LSTM in Tensorflow\n // window.location =\n \"https://www.knowledgemapper.com/knowmap/knowbook/jasdeepchhabra94@gmail.comUnderstandingLSTMinTensorflow(MNIST\n Tensorflow                                                                                                                               HOME\n Understanding LSTM in Tensorflow(MNIST dataset)\n Long Short Term Memory(LSTM) are the most common types of Recurrent Neural Networks used these days.They are mostly used\n with sequential data.An in depth look at LSTMs can be found in this             incredible blog post.\n Our Aim\n As the title suggests,the main aim of this blogpost is to make the reader comfortable with the implementation details of basic LSTM\n network in tensorflow.\n For fulfilling this aim we will take    MNIST   as our dataset.\n The MNIST dataset\n The MNIST dataset consists of images of handwritten digits and their corresponding labels.We can download and read the data in\n tensorflow with the help of following in built functionality-\n  from    tensorflow.examples.tutorials.mnist                     import     input_data\n  mnist    =   input_data.read_data_sets(\"/tmp/data/\",                       one_hot=True)\n The data is split into three parts-\n     1.Training data(mnist.train)-55000 images of training data\n     2.Test data(mnist.test)-10000 images of test data\n     3.Validation data(mnist.validation)-5000 images of validation data.\n Shape of the data\n Let us discuss the shape with respect to training data of MNIST dataset.Shapes of all three splits are identical.\n The training set consists of 55000 images of 28 pixels X 28 pixels each.These 784(28X28) pixel values are flattened in form of a single\n vector of dimensionality 784.The collection of all such 55000 pixel vectors(one for each image) is stored in form of a numpy array of\n shape   (55000,784)       and is referred to as   mnist.train.images.\n Each of these 55000 training images are associated with a label representing the class to which that image belongs.There are 10\n such classes(0,1,2\u20269).Class labels are represented in one hot encoded form.Thus the labels are stored in form of numpy array of\n shape   (55000,10)      and is referred to as   mnist.train.labels.", "start_char_idx": 0, "end_char_idx": 2312, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "81ad88d5-3cd5-40e3-b05e-668e9a7ea24e": {"__data__": {"id_": "81ad88d5-3cd5-40e3-b05e-668e9a7ea24e", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "3412ab5148d28f9dd30c4ef2e4d8ae8304be09418cfbc12d8058b470ac6b1dd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93048877-9bb4-44e7-ae70-26cc6882cf1d", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "14f9532b297a2475ee0f26ef9751b2dff0dcf7de330e55e654a23da5685e9444", "class_name": "RelatedNodeInfo"}}, "text": "Why MNIST?\n LSTMs are generally used for complex sequence related problems like language modelling which involves NLP concepts such as\n word embeddings, encoders etc.These topics themselves need a lot of understanding.It would be nice to eliminate these topics to\n concentrate on implementation details of LSTMs in tensorflow such as input formatting,LSTM cells and network designing.\n MNIST gives us such an opportunity.The input data here is just a set of pixel values.We can easily format these values and\n concentrate on implementation details.\n Implementation\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                                      1/114/5/24, 8:32 PM                                                            Understanding LSTM in Tensorflow\n Before getting our hands dirty with code,let us first draw an outline of our implementation.This will make the coding part more\n intuitive.\n A vanilla RNN\n A Recurrent Neural Network,when unrolled through time,can be visualised as-\n                    S                                                                                St                    St+1\n Here,\n     1.x trefers to the input at time step t.\n     2.st refers to the hidden state at time step t.It can be visualised as \u201cmemory\u201d of our network.\n     3.o trefers to the output at time step t.\n     4.U,V and W are parameters that are shared across all the time steps.The significance of this parameter sharing is that our model\n       performs same task at each time step with di\u25aferent input.\n What we have achieved by unrolling the RNN,is that at each time step,the network can be visualised as feed forward network taking\n into account the output of the previous time step(signified by the connections between the time steps).\n Two caveats\n Our implementation will hinge upon two main concepts which will make us comfortable with our implementation:\n     1.Interpretation of LSTM cells in tensorflow.\n     2.Formatting inputs before feeding them to tensorflow RNNs.\n Interpretation of LSTM cells in tensorflow\n A basic LSTM cell is declared in tensorflow as-\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                               2/114/5/24, 8:32 PM                                                  Understanding LSTM in Tensorflow\n  tf.contrib.rnn.BasicLSTMCell(num_units)\n here  num_unitsrefers to the number of units in LSTM cell.", "start_char_idx": 2314, "end_char_idx": 4803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-0": {"__data__": {"id_": "node-0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e7957c52-8cb0-4ce4-8d1d-5dd717722343", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "eb59f095980836bcc8d6897eed6df5d63aab2014bc76bd88a52b7a9579b503bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "085b61b2-69c1-4326-8309-d8476b696730", "node_type": "1", "metadata": {}, "hash": "f91fdaafe825f1c8aeedff5d36e92d1d15a9c12060d807e4870f9cb2fc6ffd67", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:32 PM                                                            Understanding LSTM in Tensorflow\n // window.location =\n \"https://www.knowledgemapper.com/knowmap/knowbook/jasdeepchhabra94@gmail.comUnderstandingLSTMinTensorflow(MNIST\n Tensorflow                                                                                                                               HOME\n Understanding LSTM in Tensorflow(MNIST dataset)\n Long Short Term Memory(LSTM) are the most common types of Recurrent Neural Networks used these days.They are mostly used\n with sequential data.An in depth look at LSTMs can be found in this             incredible blog post.\n Our Aim\n As the title suggests,the main aim of this blogpost is to make the reader comfortable with the implementation details of basic LSTM\n network in tensorflow.\n For fulfilling this aim we will take    MNIST   as our dataset.\n The MNIST dataset\n The MNIST dataset consists of images of handwritten digits and their corresponding labels.We can download and read the data in\n tensorflow with the help of following in built functionality-\n  from    tensorflow.examples.tutorials.mnist                     import     input_data\n  mnist    =   input_data.read_data_sets(\"/tmp/data/\",                       one_hot=True)\n The data is split into three parts-\n     1.Training data(mnist.train)-55000 images of training data\n     2.Test data(mnist.test)-10000 images of test data\n     3.Validation data(mnist.validation)-5000 images of validation data.\n Shape of the data\n Let us discuss the shape with respect to training data of MNIST dataset.Shapes of all three splits are identical.\n The training set consists of 55000 images of 28 pixels X 28 pixels each.These 784(28X28) pixel values are flattened in form of a single\n vector of dimensionality 784.The collection of all such 55000 pixel vectors(one for each image) is stored in form of a numpy array of\n shape   (55000,784)       and is referred to as   mnist.train.images.\n Each of these 55000 training images are associated with a label representing the class to which that image belongs.There are 10\n such classes(0,1,2\u20269).Class labels are represented in one hot encoded form.Thus the labels are stored in form of numpy array of\n shape   (55000,10)      and is referred to as   mnist.train.labels.\n Why MNIST?\n LSTMs are generally used for complex sequence related problems like language modelling which involves NLP concepts such as\n word embeddings, encoders etc.These topics themselves need a lot of understanding.It would be nice to eliminate these topics to\n concentrate on implementation details of LSTMs in tensorflow such as input formatting,LSTM cells and network designing.\n MNIST gives us such an opportunity.The input data here is just a set of pixel values.We can easily format these values and\n concentrate on implementation details.\n Implementation\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                                      1/114/5/24, 8:32 PM                                                            Understanding LSTM in Tensorflow\n Before getting our hands dirty with code,let us first draw an outline of our implementation.This will make the coding part more\n intuitive.\n A vanilla RNN\n A Recurrent Neural Network,when unrolled through time,can be visualised as-\n                    S                                                                                St                    St+1\n Here,\n     1.x trefers to the input at time step t.\n     2.st refers to the hidden state at time step t.It can be visualised as \u201cmemory\u201d of our network.\n     3.o trefers to the output at time step t.\n     4.U,V and W are parameters that are shared across all the time steps.The significance of this parameter sharing is that our model\n       performs same task at each time step with di\u25aferent input.\n What we have achieved by unrolling the RNN,is that at each time step,the network can be visualised as feed forward network taking\n into account the output of the previous time step(signified by the connections between the time steps).\n Two caveats\n Our implementation will hinge upon two main concepts which will make us comfortable with our implementation:\n     1.Interpretation of LSTM cells in tensorflow.\n     2.Formatting inputs before feeding them to tensorflow RNNs.\n Interpretation of LSTM cells in tensorflow\n A basic LSTM cell is declared in tensorflow as-\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                               2/114/5/24, 8:32 PM                                                  Understanding LSTM in Tensorflow\n  tf.contrib.rnn.BasicLSTMCell(num_units)\n here  num_unitsrefers to the number of units in LSTM cell.", "start_char_idx": 0, "end_char_idx": 4803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "46dbd44c-96d4-4026-8d9d-fcb2a156f05b": {"__data__": {"id_": "46dbd44c-96d4-4026-8d9d-fcb2a156f05b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "7e9fb39885deaff1de023fe6bd840f33eb3be3803355c58527fbe53404da321f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09c739f9-3e07-4f18-b5b0-0f47e73f3b6b", "node_type": "1", "metadata": {}, "hash": "4c4de9384e37a3087fd6dfdc76d539d5aba2f3527e253ff0c933160d7ef4fb1d", "class_name": "RelatedNodeInfo"}}, "text": "num_units    can be interpreted as the analogy of hidden layer from the feed forward neural network.The number of nodes in hidden\n layer of a feed forward neural network is equivalent tonum_units      number of LSTM units in a LSTM cell at every time step of the\n network.Following picture should clear any confusion-\n                                                   OO                                   OO\n Each of the  num_unitsLSTM unit can be seen as a standard LSTM unit-\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                             3/114/5/24, 8:32 PM                                                          Understanding LSTM in Tensorflow\n                                                                                        ht_\n                                                                                  Lali\n                                                               tanh\n                                            Xt\n The above diagram is taken from        this incredible blogpost    which describes the concept of LSTM e\u25afectively.\n Formatting inputs before feeding them to tensorflow RNNs\n The simplest form of RNN in tensorflow is        static_rnn.It is defined in tensorflow as\n  tf.static_rnn(cell,inputs)\n There are other arguments as well but we\u02bcll limit ourselves to deal with only these two arguments.", "start_char_idx": 0, "end_char_idx": 1411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "09c739f9-3e07-4f18-b5b0-0f47e73f3b6b": {"__data__": {"id_": "09c739f9-3e07-4f18-b5b0-0f47e73f3b6b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "7e9fb39885deaff1de023fe6bd840f33eb3be3803355c58527fbe53404da321f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46dbd44c-96d4-4026-8d9d-fcb2a156f05b", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "896f5fe182b73b08ddcd61a789433a07e4cd096755c9c666d9afc3bcd796c5e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6ae9254-7a69-4536-bb28-3c6f34dbd7f5", "node_type": "1", "metadata": {}, "hash": "782164280d95d21133ff3939d9440a33cd0b1f9f894e0021dccf0e513f985924", "class_name": "RelatedNodeInfo"}}, "text": "The  inputs    argument accepts list of tensors of shape       [batch_size,input_size].The length of this list is the number of time\n steps through which network is unrolled i.e. each element of this list corresponds to the input at respective time step of our unrolled\n network.\n For our case of MNIST images,we have images of size 28X28.They can be inferred as images having 28 rows of 28 pixels each.We will\n unroll our network through 28 time steps so that at every time step we can input one row of 28 pixels(input_size) and thus a full\n image through 28 time steps.If we supplybatch_size               number of images,every time step will be supplied with respective row of\n batch_size      images.Following figure should clear any doubts-\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                                   4/114/5/24,", "start_char_idx": 1413, "end_char_idx": 2311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a6ae9254-7a69-4536-bb28-3c6f34dbd7f5": {"__data__": {"id_": "a6ae9254-7a69-4536-bb28-3c6f34dbd7f5", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "7e9fb39885deaff1de023fe6bd840f33eb3be3803355c58527fbe53404da321f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09c739f9-3e07-4f18-b5b0-0f47e73f3b6b", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "cf285e5a6143a99509f62586e344f666f05a53e28b5642b6e9e1ea60a757896c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec1dd949-b117-435d-a923-c7d20f514793", "node_type": "1", "metadata": {}, "hash": "d0b90513eb3ee55f7c7378dcd7079e29ed750744b63c9537f3663f656e57537f", "class_name": "RelatedNodeInfo"}}, "text": "8:32 PM                                                    Understanding LSTM in Tensorflow\n             Ist row image                   Znd row image                                     28th row image\n             Ist rOW image 2                 Znd row image 2                                    28th row image 2\n             Ist row image 3                 2nd row image 3                                    28th row image 3\n                Ist row                           2nd row                                          28th row\n The output generated by    static_rnn     is a list of tensors of shape[batch_size,num_units].The length of the list is number of\n time steps through which network is unrolled i.e. one output tensor for each time step.In this implementation we will only be\n concerned with output of the final time step as the prediction will be generated when all the rows of an image are supplied to RNN\n i.e. at the last time step.\n Now that we have done all the heavy-li\u25afing,we are ready to write the code.The coding part is very straight forward once above\n concepts are clear.", "start_char_idx": 2312, "end_char_idx": 3414, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ec1dd949-b117-435d-a923-c7d20f514793": {"__data__": {"id_": "ec1dd949-b117-435d-a923-c7d20f514793", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "7e9fb39885deaff1de023fe6bd840f33eb3be3803355c58527fbe53404da321f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6ae9254-7a69-4536-bb28-3c6f34dbd7f5", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "68224bbe139cb62f0a3cd72e345a2d51192c3490c17f39ddd53b3e04db669c30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9569cd7b-ec8c-4c29-94ff-64ddcfc74e22", "node_type": "1", "metadata": {}, "hash": "2a1c3b4e71f287811e44846e8819c7bfb591ce09bcd9a882abbe4e05915375e8", "class_name": "RelatedNodeInfo"}}, "text": "Code\n To start with,lets import necessary dependencies,dataset and declare some constants.We will use          batch_size=128      and\n num_units=128.\n  import    tensorflow     as   tf\n  from   tensorflow.contrib         import    rnn\n  #import mnist dataset\n  from   tensorflow.examples.tutorials.mnist                import    input_data\n  mnist=input_data.read_data_sets(\"/tmp/data/\",one_hot=True)\n  #define    constants\n  #unrolled through 28 time steps\n  time_steps=28\n  #hidden LSTM units\n  num_units=128\n  #rows of 28 pixels\n  n_input=28\n  #learning rate for adam\n  learning_rate=0.001\n  #mnist is meant to be classified in 10 classes(0-9).", "start_char_idx": 3416, "end_char_idx": 4064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "9569cd7b-ec8c-4c29-94ff-64ddcfc74e22": {"__data__": {"id_": "9569cd7b-ec8c-4c29-94ff-64ddcfc74e22", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "7e9fb39885deaff1de023fe6bd840f33eb3be3803355c58527fbe53404da321f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec1dd949-b117-435d-a923-c7d20f514793", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "b2b32f578bce6a994eb72a17a8da6f33a58ada3edc8978c1747b1c4562c77932", "class_name": "RelatedNodeInfo"}}, "text": "https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                            5/114/5/24, 8:32 PM                                                  Understanding LSTM in Tensorflow\n  n_classes=10\n  #size of batch\n  batch_size=128\n Lets now declare placeholders and weights and bias variables which will be used to convert the output of shape\n [batch_size,num_units]         to [batch_size,n_classes]        so that correct class can be predicted.", "start_char_idx": 4065, "end_char_idx": 4565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "09aa259a-25bb-4ec9-a9d7-6dcfffbf1475": {"__data__": {"id_": "09aa259a-25bb-4ec9-a9d7-6dcfffbf1475", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "7e9fb39885deaff1de023fe6bd840f33eb3be3803355c58527fbe53404da321f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83adf9da-0335-4484-89e5-a12b09b1a9d5", "node_type": "1", "metadata": {}, "hash": "4734b62749c5f0a2339117cd06f7d985a02a4a831de55df699fe8205f37ea20b", "class_name": "RelatedNodeInfo"}}, "text": "num_units    can be interpreted as the analogy of hidden layer from the feed forward neural network.The number of nodes in hidden\n layer of a feed forward neural network is equivalent tonum_units      number of LSTM units in a LSTM cell at every time step of the\n network.Following picture should clear any confusion-\n                                                   OO                                   OO\n Each of the  num_unitsLSTM unit can be seen as a standard LSTM unit-\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                             3/114/5/24, 8:32 PM                                                          Understanding LSTM in Tensorflow\n                                                                                        ht_\n                                                                                  Lali\n                                                               tanh\n                                            Xt\n The above diagram is taken from        this incredible blogpost    which describes the concept of LSTM e\u25afectively.\n Formatting inputs before feeding them to tensorflow RNNs\n The simplest form of RNN in tensorflow is        static_rnn.It is defined in tensorflow as\n  tf.static_rnn(cell,inputs)\n There are other arguments as well but we\u02bcll limit ourselves to deal with only these two arguments.\n The  inputs    argument accepts list of tensors of shape       [batch_size,input_size].The length of this list is the number of time\n steps through which network is unrolled i.e. each element of this list corresponds to the input at respective time step of our unrolled\n network.", "start_char_idx": 0, "end_char_idx": 1692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "83adf9da-0335-4484-89e5-a12b09b1a9d5": {"__data__": {"id_": "83adf9da-0335-4484-89e5-a12b09b1a9d5", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "7e9fb39885deaff1de023fe6bd840f33eb3be3803355c58527fbe53404da321f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09aa259a-25bb-4ec9-a9d7-6dcfffbf1475", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "d9f2afdbc8ec3a914e55e4e5a4327f34c8b1504830466ebb1a168bfd68933f5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "864518fc-a959-494a-968f-a9f6a2bb78ab", "node_type": "1", "metadata": {}, "hash": "c180e4560953123ab03e1f34f4815dc1a1e60a06c6caae1d4578fb801a549723", "class_name": "RelatedNodeInfo"}}, "text": "For our case of MNIST images,we have images of size 28X28.They can be inferred as images having 28 rows of 28 pixels each.We will\n unroll our network through 28 time steps so that at every time step we can input one row of 28 pixels(input_size) and thus a full\n image through 28 time steps.If we supplybatch_size               number of images,every time step will be supplied with respective row of\n batch_size      images.Following figure should clear any doubts-\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                                   4/114/5/24, 8:32 PM                                                    Understanding LSTM in Tensorflow\n             Ist row image                   Znd row image                                     28th row image\n             Ist rOW image 2                 Znd row image 2                                    28th row image 2\n             Ist row image 3                 2nd row image 3                                    28th row image 3\n                Ist row                           2nd row                                          28th row\n The output generated by    static_rnn     is a list of tensors of shape[batch_size,num_units].The length of the list is number of\n time steps through which network is unrolled i.e. one output tensor for each time step.In this implementation we will only be\n concerned with output of the final time step as the prediction will be generated when all the rows of an image are supplied to RNN\n i.e. at the last time step.\n Now that we have done all the heavy-li\u25afing,we are ready to write the code.The coding part is very straight forward once above\n concepts are clear.\n Code\n To start with,lets import necessary dependencies,dataset and declare some constants.We will use          batch_size=128      and\n num_units=128.", "start_char_idx": 1694, "end_char_idx": 3566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "864518fc-a959-494a-968f-a9f6a2bb78ab": {"__data__": {"id_": "864518fc-a959-494a-968f-a9f6a2bb78ab", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "7e9fb39885deaff1de023fe6bd840f33eb3be3803355c58527fbe53404da321f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83adf9da-0335-4484-89e5-a12b09b1a9d5", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "a649f95c4abdfcdfebe494d544361f6504da66b849b5a5b2c55fa7dee6e71116", "class_name": "RelatedNodeInfo"}}, "text": "import    tensorflow     as   tf\n  from   tensorflow.contrib         import    rnn\n  #import mnist dataset\n  from   tensorflow.examples.tutorials.mnist                import    input_data\n  mnist=input_data.read_data_sets(\"/tmp/data/\",one_hot=True)\n  #define    constants\n  #unrolled through 28 time steps\n  time_steps=28\n  #hidden LSTM units\n  num_units=128\n  #rows of 28 pixels\n  n_input=28\n  #learning rate for adam\n  learning_rate=0.001\n  #mnist is meant to be classified in 10 classes(0-9).\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                            5/114/5/24, 8:32 PM                                                  Understanding LSTM in Tensorflow\n  n_classes=10\n  #size of batch\n  batch_size=128\n Lets now declare placeholders and weights and bias variables which will be used to convert the output of shape\n [batch_size,num_units]         to [batch_size,n_classes]        so that correct class can be predicted.", "start_char_idx": 3569, "end_char_idx": 4565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-1": {"__data__": {"id_": "node-1", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e7957c52-8cb0-4ce4-8d1d-5dd717722343", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "eb59f095980836bcc8d6897eed6df5d63aab2014bc76bd88a52b7a9579b503bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec6d3743-eb9a-47be-9f45-24ffeb321d96", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "3412ab5148d28f9dd30c4ef2e4d8ae8304be09418cfbc12d8058b470ac6b1dd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b97f9d1-9b42-469c-b5ef-a12f152c05ba", "node_type": "1", "metadata": {}, "hash": "f2ea867236d3503b098c28c3390ae766b55f7348ec125c10680a776a947c6f48", "class_name": "RelatedNodeInfo"}}, "text": "num_units    can be interpreted as the analogy of hidden layer from the feed forward neural network.The number of nodes in hidden\n layer of a feed forward neural network is equivalent tonum_units      number of LSTM units in a LSTM cell at every time step of the\n network.Following picture should clear any confusion-\n                                                   OO                                   OO\n Each of the  num_unitsLSTM unit can be seen as a standard LSTM unit-\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                             3/114/5/24, 8:32 PM                                                          Understanding LSTM in Tensorflow\n                                                                                        ht_\n                                                                                  Lali\n                                                               tanh\n                                            Xt\n The above diagram is taken from        this incredible blogpost    which describes the concept of LSTM e\u25afectively.\n Formatting inputs before feeding them to tensorflow RNNs\n The simplest form of RNN in tensorflow is        static_rnn.It is defined in tensorflow as\n  tf.static_rnn(cell,inputs)\n There are other arguments as well but we\u02bcll limit ourselves to deal with only these two arguments.\n The  inputs    argument accepts list of tensors of shape       [batch_size,input_size].The length of this list is the number of time\n steps through which network is unrolled i.e. each element of this list corresponds to the input at respective time step of our unrolled\n network.\n For our case of MNIST images,we have images of size 28X28.They can be inferred as images having 28 rows of 28 pixels each.We will\n unroll our network through 28 time steps so that at every time step we can input one row of 28 pixels(input_size) and thus a full\n image through 28 time steps.If we supplybatch_size               number of images,every time step will be supplied with respective row of\n batch_size      images.Following figure should clear any doubts-\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                                   4/114/5/24, 8:32 PM                                                    Understanding LSTM in Tensorflow\n             Ist row image                   Znd row image                                     28th row image\n             Ist rOW image 2                 Znd row image 2                                    28th row image 2\n             Ist row image 3                 2nd row image 3                                    28th row image 3\n                Ist row                           2nd row                                          28th row\n The output generated by    static_rnn     is a list of tensors of shape[batch_size,num_units].The length of the list is number of\n time steps through which network is unrolled i.e. one output tensor for each time step.In this implementation we will only be\n concerned with output of the final time step as the prediction will be generated when all the rows of an image are supplied to RNN\n i.e. at the last time step.\n Now that we have done all the heavy-li\u25afing,we are ready to write the code.The coding part is very straight forward once above\n concepts are clear.\n Code\n To start with,lets import necessary dependencies,dataset and declare some constants.We will use          batch_size=128      and\n num_units=128.\n  import    tensorflow     as   tf\n  from   tensorflow.contrib         import    rnn\n  #import mnist dataset\n  from   tensorflow.examples.tutorials.mnist                import    input_data\n  mnist=input_data.read_data_sets(\"/tmp/data/\",one_hot=True)\n  #define    constants\n  #unrolled through 28 time steps\n  time_steps=28\n  #hidden LSTM units\n  num_units=128\n  #rows of 28 pixels\n  n_input=28\n  #learning rate for adam\n  learning_rate=0.001\n  #mnist is meant to be classified in 10 classes(0-9).\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                            5/114/5/24, 8:32 PM                                                  Understanding LSTM in Tensorflow\n  n_classes=10\n  #size of batch\n  batch_size=128\n Lets now declare placeholders and weights and bias variables which will be used to convert the output of shape\n [batch_size,num_units]         to [batch_size,n_classes]        so that correct class can be predicted.", "start_char_idx": 4805, "end_char_idx": 9370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e1b39143-a5c1-4315-983b-e7864c4282b0": {"__data__": {"id_": "e1b39143-a5c1-4315-983b-e7864c4282b0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "c293b7fe288ece1cdaab498a51aee1a2b47d7c477a9579bceee0738f388dfd1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdf8a521-079b-4845-b394-a2dbabc3d18b", "node_type": "1", "metadata": {}, "hash": "67e8eb2caf71c56b318c063aedd982c87c900b390d483799efca5cda16ac94f3", "class_name": "RelatedNodeInfo"}}, "text": "#weights and biases of appropriate shape to accomplish above task\n  out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\n  out_bias=tf.Variable(tf.random_normal([n_classes]))\n  #defining placeholders\n  #input image placeholder\n  x=tf.placeholder(\"float\",[None,time_steps,n_input])\n  #input label placeholder\n  y=tf.placeholder(\"float\",[None,n_classes])\n Now that we are receiving inputs of shape   [batch_size,time_steps,n_input],we need to convert it into a list of tensors of\n shape[batch_size,n_inputs]          of lengthtime_steps     so that it can be then fed tostatic_rnn.\n  #processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size\n  input=tf.unstack(x        ,time_steps,1)\n Now we are ready to define our network.We will use one layer of BasicLSTMCell and make our       static_rnn    network out of it.", "start_char_idx": 0, "end_char_idx": 868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "cdf8a521-079b-4845-b394-a2dbabc3d18b": {"__data__": {"id_": "cdf8a521-079b-4845-b394-a2dbabc3d18b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "c293b7fe288ece1cdaab498a51aee1a2b47d7c477a9579bceee0738f388dfd1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1b39143-a5c1-4315-983b-e7864c4282b0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "92f58fed6d53826938ee2689f29f3b6bfa148332c8c99b8a5b2880767e039c28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aebadcd3-16f0-411a-9ed9-0a85e9985727", "node_type": "1", "metadata": {}, "hash": "91a4b1b2a907634a12545266c364246152e8d33cc45e72e8f58fec02a53a61dd", "class_name": "RelatedNodeInfo"}}, "text": "#defining the network\n  lstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\n  outputs,_=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\n As we are considered only with input of last time step,we will generate our prediction out of it.\n  #converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight m\n  prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n Defining loss,optimizer and accuracy.\n  #loss_function\n  loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n  #optimization\n  opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n  #model evaluation\n  correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n  accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n Now that we have defined out graph,we can run it.\n  #initialize variables\n  init=tf.global_variables_initializer()\n  with  tf.Session()      as  sess:\n       sess.", "start_char_idx": 871, "end_char_idx": 1826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "aebadcd3-16f0-411a-9ed9-0a85e9985727": {"__data__": {"id_": "aebadcd3-16f0-411a-9ed9-0a85e9985727", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "c293b7fe288ece1cdaab498a51aee1a2b47d7c477a9579bceee0738f388dfd1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdf8a521-079b-4845-b394-a2dbabc3d18b", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "9238b6bfe061620bc142f075c797c242bbe0f99f15970094ce04c8ee51ca591c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85ec4a4e-26ba-4529-a45f-1a94c9e8b41a", "node_type": "1", "metadata": {}, "hash": "a7aed98f59523dbd017dbcdca93303477046a07fbe39c0bba6a7e9d9d46f81e2", "class_name": "RelatedNodeInfo"}}, "text": "run(init)\n       iter=1\n       while   iter<800:\n             batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\n             batch_x=batch_x.reshape((batch_size,time_steps,n_input))\n             sess.run(opt,      feed_dict={x:      batch_x,    y:  batch_y})\n             if  iter   %10==0:\n                  acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n                  los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n                  print(\"For iter \",iter)\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                           6/114/5/24, 8:32 PM     print(\"Accuracy \",acc)                            Understanding LSTM in Tensorflow\n                    print(\"Loss \",los)\n                    print(\"__________________\")\n              iter=iter+1\n  One crucial thing to note here is that our images were essentially flattened into a single vector of dimensionality 784 to begin\n  with.", "start_char_idx": 1826, "end_char_idx": 2803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "85ec4a4e-26ba-4529-a45f-1a94c9e8b41a": {"__data__": {"id_": "85ec4a4e-26ba-4529-a45f-1a94c9e8b41a", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "c293b7fe288ece1cdaab498a51aee1a2b47d7c477a9579bceee0738f388dfd1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aebadcd3-16f0-411a-9ed9-0a85e9985727", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "b511bdc0b3b58e76fab99f37bfb0dcb68a04c3240c4d8cfa7b365b714b352fbf", "class_name": "RelatedNodeInfo"}}, "text": "The function   next_batch(batch_size)           necessarily returnsbatch_size       batches of these 784 dimensional vectors.They are\n  thus reshaped to[batch_size,time_steps,n_input]                  so that it can be accepted by our placeholder.\n  We can also calculate test accuracy of our model-\n  #calculating test accuracy\n  test_data      =  mnist.test.images[:128].reshape((-1,                  time_steps,       n_input))\n  test_label      =  mnist.test.labels[:128]\n  print(\"Testing Accuracy:\",              sess.run(accuracy,          feed_dict={x:       test_data,      y:   test_label}))\n  On running,the model runs with a test accuracy of 99.21%.\n  This blogpost was aimed at making the reader comfortable with the implementational details of RNNs in tensorflow.We\u02bcll built\n  some more complex models to use RNNs e\u25afectively in tensorflow.Stay tuned!", "start_char_idx": 2803, "end_char_idx": 3666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1abf114a-26b5-4843-b55f-f5dcbab2ecb9": {"__data__": {"id_": "1abf114a-26b5-4843-b55f-f5dcbab2ecb9", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "c293b7fe288ece1cdaab498a51aee1a2b47d7c477a9579bceee0738f388dfd1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "828ee0ad-13d6-4194-aedb-e8260b72c6a6", "node_type": "1", "metadata": {}, "hash": "9375fe913c60ae7b5de87ba553678fee5a84b22881b6f31c63ecdb9447ab9ad9", "class_name": "RelatedNodeInfo"}}, "text": "#weights and biases of appropriate shape to accomplish above task\n  out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\n  out_bias=tf.Variable(tf.random_normal([n_classes]))\n  #defining placeholders\n  #input image placeholder\n  x=tf.placeholder(\"float\",[None,time_steps,n_input])\n  #input label placeholder\n  y=tf.placeholder(\"float\",[None,n_classes])\n Now that we are receiving inputs of shape   [batch_size,time_steps,n_input],we need to convert it into a list of tensors of\n shape[batch_size,n_inputs]          of lengthtime_steps     so that it can be then fed tostatic_rnn.\n  #processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size\n  input=tf.unstack(x        ,time_steps,1)\n Now we are ready to define our network.We will use one layer of BasicLSTMCell and make our       static_rnn    network out of it.\n  #defining the network\n  lstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\n  outputs,_=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\n As we are considered only with input of last time step,we will generate our prediction out of it.\n  #converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight m\n  prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n Defining loss,optimizer and accuracy.\n  #loss_function\n  loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n  #optimization\n  opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n  #model evaluation\n  correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n  accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n Now that we have defined out graph,we can run it.", "start_char_idx": 0, "end_char_idx": 1712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "828ee0ad-13d6-4194-aedb-e8260b72c6a6": {"__data__": {"id_": "828ee0ad-13d6-4194-aedb-e8260b72c6a6", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "c293b7fe288ece1cdaab498a51aee1a2b47d7c477a9579bceee0738f388dfd1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1abf114a-26b5-4843-b55f-f5dcbab2ecb9", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "15acf1d4f38bdd5cbd8e9759eab9269ddff277109902fa6d3abf70340fcbf01a", "class_name": "RelatedNodeInfo"}}, "text": "#initialize variables\n  init=tf.global_variables_initializer()\n  with  tf.Session()      as  sess:\n       sess.run(init)\n       iter=1\n       while   iter<800:\n             batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\n             batch_x=batch_x.reshape((batch_size,time_steps,n_input))\n             sess.run(opt,      feed_dict={x:      batch_x,    y:  batch_y})\n             if  iter   %10==0:\n                  acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n                  los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n                  print(\"For iter \",iter)\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                           6/114/5/24, 8:32 PM     print(\"Accuracy \",acc)                            Understanding LSTM in Tensorflow\n                    print(\"Loss \",los)\n                    print(\"__________________\")\n              iter=iter+1\n  One crucial thing to note here is that our images were essentially flattened into a single vector of dimensionality 784 to begin\n  with.The function   next_batch(batch_size)           necessarily returnsbatch_size       batches of these 784 dimensional vectors.They are\n  thus reshaped to[batch_size,time_steps,n_input]                  so that it can be accepted by our placeholder.\n  We can also calculate test accuracy of our model-\n  #calculating test accuracy\n  test_data      =  mnist.test.images[:128].reshape((-1,                  time_steps,       n_input))\n  test_label      =  mnist.test.labels[:128]\n  print(\"Testing Accuracy:\",              sess.run(accuracy,          feed_dict={x:       test_data,      y:   test_label}))\n  On running,the model runs with a test accuracy of 99.21%.\n  This blogpost was aimed at making the reader comfortable with the implementational details of RNNs in tensorflow.We\u02bcll built\n  some more complex models to use RNNs e\u25afectively in tensorflow.Stay tuned!", "start_char_idx": 1715, "end_char_idx": 3666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-2": {"__data__": {"id_": "node-2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e7957c52-8cb0-4ce4-8d1d-5dd717722343", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "eb59f095980836bcc8d6897eed6df5d63aab2014bc76bd88a52b7a9579b503bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "085b61b2-69c1-4326-8309-d8476b696730", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "7e9fb39885deaff1de023fe6bd840f33eb3be3803355c58527fbe53404da321f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58d0d604-372f-49d7-ac92-3f1ebc5b0f5d", "node_type": "1", "metadata": {}, "hash": "c10b7564d7b5e1eaea2db19224545926fbb59d59fa53a98e69c039107035e135", "class_name": "RelatedNodeInfo"}}, "text": "#weights and biases of appropriate shape to accomplish above task\n  out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\n  out_bias=tf.Variable(tf.random_normal([n_classes]))\n  #defining placeholders\n  #input image placeholder\n  x=tf.placeholder(\"float\",[None,time_steps,n_input])\n  #input label placeholder\n  y=tf.placeholder(\"float\",[None,n_classes])\n Now that we are receiving inputs of shape   [batch_size,time_steps,n_input],we need to convert it into a list of tensors of\n shape[batch_size,n_inputs]          of lengthtime_steps     so that it can be then fed tostatic_rnn.\n  #processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size\n  input=tf.unstack(x        ,time_steps,1)\n Now we are ready to define our network.We will use one layer of BasicLSTMCell and make our       static_rnn    network out of it.\n  #defining the network\n  lstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\n  outputs,_=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\n As we are considered only with input of last time step,we will generate our prediction out of it.\n  #converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight m\n  prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n Defining loss,optimizer and accuracy.\n  #loss_function\n  loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n  #optimization\n  opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n  #model evaluation\n  correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n  accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n Now that we have defined out graph,we can run it.\n  #initialize variables\n  init=tf.global_variables_initializer()\n  with  tf.Session()      as  sess:\n       sess.run(init)\n       iter=1\n       while   iter<800:\n             batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\n             batch_x=batch_x.reshape((batch_size,time_steps,n_input))\n             sess.run(opt,      feed_dict={x:      batch_x,    y:  batch_y})\n             if  iter   %10==0:\n                  acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n                  los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n                  print(\"For iter \",iter)\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                           6/114/5/24, 8:32 PM     print(\"Accuracy \",acc)                            Understanding LSTM in Tensorflow\n                    print(\"Loss \",los)\n                    print(\"__________________\")\n              iter=iter+1\n  One crucial thing to note here is that our images were essentially flattened into a single vector of dimensionality 784 to begin\n  with.The function   next_batch(batch_size)           necessarily returnsbatch_size       batches of these 784 dimensional vectors.They are\n  thus reshaped to[batch_size,time_steps,n_input]                  so that it can be accepted by our placeholder.\n  We can also calculate test accuracy of our model-\n  #calculating test accuracy\n  test_data      =  mnist.test.images[:128].reshape((-1,                  time_steps,       n_input))\n  test_label      =  mnist.test.labels[:128]\n  print(\"Testing Accuracy:\",              sess.run(accuracy,          feed_dict={x:       test_data,      y:   test_label}))\n  On running,the model runs with a test accuracy of 99.21%.\n  This blogpost was aimed at making the reader comfortable with the implementational details of RNNs in tensorflow.We\u02bcll built\n  some more complex models to use RNNs e\u25afectively in tensorflow.Stay tuned!", "start_char_idx": 9373, "end_char_idx": 13039, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7b96a154-0f8c-4a6b-8ee0-93088c340be2": {"__data__": {"id_": "7b96a154-0f8c-4a6b-8ee0-93088c340be2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "c48069dfb8eacf4ad1867b2371e721013b907953c98404ae91a8e08e84f3a091", "class_name": "RelatedNodeInfo"}}, "text": "ALSO ON JASDEEP06                                      Posted on 10 September,2017\n    Getting started with        Further-into-               Lets-Practice              Variablc\n    Getting started with\n    Getting started with        Further-into-\n                                Further-into-               Lets-Practice-\n                                                            Lets-Practice-              Variable-s\n                                                                                        Variable-s\n    Tensorflov                  backpropagation             Backpropagation            ensorflov\n    Tensor\u25afow\n    Tensor\u25afow                   backpropagation\n                                backpropagation             Backpropagation\n                                                            Backpropagation             Tensor\u25afow\n    7 years ago3 comments                                                               Tensor\u25afow\n              \u2022                 7 years ago2 comments\n                                         \u2022                  7 years ago4 comments\n                                                                     \u2022                  7 years ago\u2022\n    Tensorflow : Getting StartedBackpropagation : Further   Lets-practice-              Tensorflow:\n    with Tensorflow             into Backpropagation        backpropagation             in Tensorflow\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                                     7/11", "start_char_idx": 0, "end_char_idx": 1539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "72f3ef6f-152c-4087-9942-f7c6989cc824": {"__data__": {"id_": "72f3ef6f-152c-4087-9942-f7c6989cc824", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "c48069dfb8eacf4ad1867b2371e721013b907953c98404ae91a8e08e84f3a091", "class_name": "RelatedNodeInfo"}}, "text": "ALSO ON JASDEEP06                                      Posted on 10 September,2017\n    Getting started with        Further-into-               Lets-Practice              Variablc\n    Getting started with\n    Getting started with        Further-into-\n                                Further-into-               Lets-Practice-\n                                                            Lets-Practice-              Variable-s\n                                                                                        Variable-s\n    Tensorflov                  backpropagation             Backpropagation            ensorflov\n    Tensor\u25afow\n    Tensor\u25afow                   backpropagation\n                                backpropagation             Backpropagation\n                                                            Backpropagation             Tensor\u25afow\n    7 years ago3 comments                                                               Tensor\u25afow\n              \u2022                 7 years ago2 comments\n                                         \u2022                  7 years ago4 comments\n                                                                     \u2022                  7 years ago\u2022\n    Tensorflow : Getting StartedBackpropagation : Further   Lets-practice-              Tensorflow:\n    with Tensorflow             into Backpropagation        backpropagation             in Tensorflow\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                                     7/11", "start_char_idx": 0, "end_char_idx": 1539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-3": {"__data__": {"id_": "node-3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e7957c52-8cb0-4ce4-8d1d-5dd717722343", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "eb59f095980836bcc8d6897eed6df5d63aab2014bc76bd88a52b7a9579b503bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b97f9d1-9b42-469c-b5ef-a12f152c05ba", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}, "hash": "c293b7fe288ece1cdaab498a51aee1a2b47d7c477a9579bceee0738f388dfd1d", "class_name": "RelatedNodeInfo"}}, "text": "ALSO ON JASDEEP06                                      Posted on 10 September,2017\n    Getting started with        Further-into-               Lets-Practice              Variablc\n    Getting started with\n    Getting started with        Further-into-\n                                Further-into-               Lets-Practice-\n                                                            Lets-Practice-              Variable-s\n                                                                                        Variable-s\n    Tensorflov                  backpropagation             Backpropagation            ensorflov\n    Tensor\u25afow\n    Tensor\u25afow                   backpropagation\n                                backpropagation             Backpropagation\n                                                            Backpropagation             Tensor\u25afow\n    7 years ago3 comments                                                               Tensor\u25afow\n              \u2022                 7 years ago2 comments\n                                         \u2022                  7 years ago4 comments\n                                                                     \u2022                  7 years ago\u2022\n    Tensorflow : Getting StartedBackpropagation : Further   Lets-practice-              Tensorflow:\n    with Tensorflow             into Backpropagation        backpropagation             in Tensorflow\nhttps://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/                                                                     7/11", "start_char_idx": 13043, "end_char_idx": 14582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3813547f-f26a-48e8-9a16-3624cc368d76": {"__data__": {"id_": "3813547f-f26a-48e8-9a16-3624cc368d76", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Understanding LSTM in Tensorflow", "path": "cache\\blogposts\\Understanding-LSTM-in-Tensorflow\\parsed\\images\\7897fe47-d1b2-4e1d-8188-aa818ca78a55-img_p1_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being discussed here pertains to the structure and dynamics of a Recurrent Neural Network (RNN). When an RNN is \"unrolled\" through time, it can be conceptualized as a sequence of events or states that occur in a specific order. Each state in the sequence is represented by a particular time step, and there are inputs, hidden states, and outputs associated with each of these time steps.\n\nAt any given time step 't', the network receives an input 'x_t', processes it, and updates its hidden state 's_t', which acts as a form of memory that carries information from previous time steps. The network then produces an output 'o_t'. The parameters 'U', 'V', and 'W' are crucial as they remain constant across all time steps, ensuring that the network performs consistently at each stage while processing different inputs.\n\nThis unrolled view of the RNN emphasizes the feedforward nature of the network at each time step while also highlighting the connections between consecutive time steps, which allow the network to consider information from the past when making predictions or decisions at the current time step.\n\nThe discussion also introduces the concept of Long Short-Term Memory (LSTM) cells within the TensorFlow framework, which are a more advanced type of RNN cell designed to better capture long-term dependencies and avoid issues like vanishing gradients that can occur with vanilla RNNs. The LSTM cells are a key component in the implementation, and understanding how to interpret and format them", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "182cc2fd-206c-4784-8bf2-85bd82ee4b5b": {"__data__": {"id_": "182cc2fd-206c-4784-8bf2-85bd82ee4b5b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Understanding LSTM in Tensorflow", "path": "cache\\blogposts\\Understanding-LSTM-in-Tensorflow\\parsed\\images\\7897fe47-d1b2-4e1d-8188-aa818ca78a55-img_p2_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being explained here relates to a type of recurrent neural network known as Long Short-Term Memory (LSTM), which is used within the TensorFlow framework. The term `num_units` is a parameter that defines the number of units within an LSTM cell, which can be thought of as analogous to the number of nodes in the hidden layer of a traditional feed-forward neural network. Each unit in the LSTM cell is akin to a standard LSTM unit, and the arrangement of these units is crucial for the LSTM's ability to capture temporal dependencies in data. The representation provided is intended to clarify the structure and function of these LSTM units, illustrating how they are interconnected and operate at each time step within the network. This visualization is helpful for understanding the architecture of LSTMs and how they process sequential data over time.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc86a272-e5f0-4310-83db-e623bad84518": {"__data__": {"id_": "cc86a272-e5f0-4310-83db-e623bad84518", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Understanding LSTM in Tensorflow", "path": "cache\\blogposts\\Understanding-LSTM-in-Tensorflow\\parsed\\images\\7897fe47-d1b2-4e1d-8188-aa818ca78a55-img_p3_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you're referring to illustrates the architecture of a Long Short-Term Memory (LSTM) cell, which is a type of recurrent neural network (RNN) used in deep learning. This LSTM cell is designed to process sequences of data by maintaining a state across time steps, which helps in handling issues like vanishing and exploding gradients that are common in traditional RNNs.\n\nIn the context of the MNIST dataset, which consists of 28x28 pixel images of handwritten digits, the LSTM network would be unrolled for 28 time steps to process each row of the image sequentially. At each time step, one row of 28 pixels is input into the network, allowing the LSTM to build up an understanding of the image one row at a time. By the end of the 28 time steps, the LSTM would have processed the entire image.\n\nThe LSTM cell itself contains various gates and states that allow it to regulate the flow of information. These include the input gate, forget gate, and output gate, as well as the cell state and hidden state. The gates control which information is retained or discarded as the cell processes data, while the states carry relevant information to subsequent time steps. The diagram likely includes symbols representing these gates and states, as well as the mathematical operations that occur at each step, such as element-wise multiplication and the application of activation functions like the hyperbolic tangent (tanh).\n\nThe source provided indicates that the diagram and the explanation are part of a blog", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a1f8a3f-7b07-4c3b-b104-94581bd852e3": {"__data__": {"id_": "8a1f8a3f-7b07-4c3b-b104-94581bd852e3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Understanding LSTM in Tensorflow", "path": "cache\\blogposts\\Understanding-LSTM-in-Tensorflow\\parsed\\images\\7897fe47-d1b2-4e1d-8188-aa818ca78a55-img_p4_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you provided discusses the implementation of a Long Short-Term Memory (LSTM) network using TensorFlow, specifically for processing images. It explains that the LSTM network is unrolled over a number of time steps corresponding to the rows of an image. Each time step processes one row of the image, and the final output is considered only after all rows have been processed, which is at the last time step.\n\nThe text also includes a code snippet that sets up the necessary imports and constants for the LSTM network. It mentions that the network will be unrolled through 28 time steps, which corresponds to the number of rows in the images being processed. Each image row consists of 28 pixels, and the LSTM has 128 hidden units. The MNIST dataset, which consists of handwritten digits, is used for training, and the goal is to classify these digits into one of ten classes (0-9).\n\nThe LSTM network's output is a list of tensors, where each tensor represents the output at a specific time step. However, only the output at the final time step is used for making the prediction. This approach is typical for sequence processing tasks where the final output depends on the entire sequence of inputs.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"2621016f-345a-4369-9c1d-45fd6e51e648": {"doc_hash": "77e035093d793a1cdef05239806ae0106ec1b248a2a42e14d821c6d778dcfeb4", "ref_doc_id": "node-0"}, "08786571-48e9-4638-a413-225dc10d656b": {"doc_hash": "90f2650ae8e76b19fc602f3e46432a11a3fb4d68d1867f60445ab5a1b95145d6", "ref_doc_id": "node-0"}, "68d99c6e-d5e9-4d68-9f03-28287eaa67ad": {"doc_hash": "18f866250b39ec7f1866e2d957bd219394f95eb4d14ac4a7219c6c6890d9b884", "ref_doc_id": "node-0"}, "251cedac-8d1d-49d6-9289-d4d4fcd3b73b": {"doc_hash": "eb637e5bed1b29d91ee912565f488aca165ae6b763dc932c719dc988bb800692", "ref_doc_id": "node-0"}, "f14e274b-2845-4012-a4c2-424354328c9d": {"doc_hash": "7297767f922f960eb4b105bac95c604ff0f7aa4cfb4824705b45a9dc5d0f26be", "ref_doc_id": "node-0"}, "252707a7-ad63-43f1-af1f-f6f5341fb597": {"doc_hash": "b2edded21410aab1a75b1fe05f5fab298e80d38b7ddbfc1f0c1dbf0db8632807", "ref_doc_id": "node-0"}, "93048877-9bb4-44e7-ae70-26cc6882cf1d": {"doc_hash": "14f9532b297a2475ee0f26ef9751b2dff0dcf7de330e55e654a23da5685e9444", "ref_doc_id": "node-0"}, "81ad88d5-3cd5-40e3-b05e-668e9a7ea24e": {"doc_hash": "ed54e00150af5f68481a3bce7b698eccc7b172ab0681cf957d4663c4f8cf6792", "ref_doc_id": "node-0"}, "node-0": {"doc_hash": "3412ab5148d28f9dd30c4ef2e4d8ae8304be09418cfbc12d8058b470ac6b1dd2", "ref_doc_id": "e7957c52-8cb0-4ce4-8d1d-5dd717722343"}, "46dbd44c-96d4-4026-8d9d-fcb2a156f05b": {"doc_hash": "896f5fe182b73b08ddcd61a789433a07e4cd096755c9c666d9afc3bcd796c5e4", "ref_doc_id": "node-1"}, "09c739f9-3e07-4f18-b5b0-0f47e73f3b6b": {"doc_hash": "cf285e5a6143a99509f62586e344f666f05a53e28b5642b6e9e1ea60a757896c", "ref_doc_id": "node-1"}, "a6ae9254-7a69-4536-bb28-3c6f34dbd7f5": {"doc_hash": "68224bbe139cb62f0a3cd72e345a2d51192c3490c17f39ddd53b3e04db669c30", "ref_doc_id": "node-1"}, "ec1dd949-b117-435d-a923-c7d20f514793": {"doc_hash": "b2b32f578bce6a994eb72a17a8da6f33a58ada3edc8978c1747b1c4562c77932", "ref_doc_id": "node-1"}, "9569cd7b-ec8c-4c29-94ff-64ddcfc74e22": {"doc_hash": "3c4a60f1777b48de561fe7b67dd80baef0bda56e3a0c5b1cd6bee234a18af82b", "ref_doc_id": "node-1"}, "09aa259a-25bb-4ec9-a9d7-6dcfffbf1475": {"doc_hash": "d9f2afdbc8ec3a914e55e4e5a4327f34c8b1504830466ebb1a168bfd68933f5e", "ref_doc_id": "node-1"}, "83adf9da-0335-4484-89e5-a12b09b1a9d5": {"doc_hash": "a649f95c4abdfcdfebe494d544361f6504da66b849b5a5b2c55fa7dee6e71116", "ref_doc_id": "node-1"}, "864518fc-a959-494a-968f-a9f6a2bb78ab": {"doc_hash": "152e9cf9d506d525aefd3d04f4297491f8efc558889ae925b3702b6b57e7616c", "ref_doc_id": "node-1"}, "node-1": {"doc_hash": "7e9fb39885deaff1de023fe6bd840f33eb3be3803355c58527fbe53404da321f", "ref_doc_id": "e7957c52-8cb0-4ce4-8d1d-5dd717722343"}, "e1b39143-a5c1-4315-983b-e7864c4282b0": {"doc_hash": "92f58fed6d53826938ee2689f29f3b6bfa148332c8c99b8a5b2880767e039c28", "ref_doc_id": "node-2"}, "cdf8a521-079b-4845-b394-a2dbabc3d18b": {"doc_hash": "9238b6bfe061620bc142f075c797c242bbe0f99f15970094ce04c8ee51ca591c", "ref_doc_id": "node-2"}, "aebadcd3-16f0-411a-9ed9-0a85e9985727": {"doc_hash": "b511bdc0b3b58e76fab99f37bfb0dcb68a04c3240c4d8cfa7b365b714b352fbf", "ref_doc_id": "node-2"}, "85ec4a4e-26ba-4529-a45f-1a94c9e8b41a": {"doc_hash": "1787b0eab11470da76b75a1aae2412608f0a2f5a62ea3bafa1c48781caae1a69", "ref_doc_id": "node-2"}, "1abf114a-26b5-4843-b55f-f5dcbab2ecb9": {"doc_hash": "15acf1d4f38bdd5cbd8e9759eab9269ddff277109902fa6d3abf70340fcbf01a", "ref_doc_id": "node-2"}, "828ee0ad-13d6-4194-aedb-e8260b72c6a6": {"doc_hash": "16dd34864b3ab8fa1977203a2920a112768886243b5c289ca6818f28711740cc", "ref_doc_id": "node-2"}, "node-2": {"doc_hash": "c293b7fe288ece1cdaab498a51aee1a2b47d7c477a9579bceee0738f388dfd1d", "ref_doc_id": "e7957c52-8cb0-4ce4-8d1d-5dd717722343"}, "7b96a154-0f8c-4a6b-8ee0-93088c340be2": {"doc_hash": "c48069dfb8eacf4ad1867b2371e721013b907953c98404ae91a8e08e84f3a091", "ref_doc_id": "node-3"}, "72f3ef6f-152c-4087-9942-f7c6989cc824": {"doc_hash": "c48069dfb8eacf4ad1867b2371e721013b907953c98404ae91a8e08e84f3a091", "ref_doc_id": "node-3"}, "node-3": {"doc_hash": "c48069dfb8eacf4ad1867b2371e721013b907953c98404ae91a8e08e84f3a091", "ref_doc_id": "e7957c52-8cb0-4ce4-8d1d-5dd717722343"}, "3813547f-f26a-48e8-9a16-3624cc368d76": {"doc_hash": "17895442caa85d18f536e5439071c9b814f3086b928818bbf88575479787629b"}, "182cc2fd-206c-4784-8bf2-85bd82ee4b5b": {"doc_hash": "b1f9a85c14c07bd75f55c6f26026382bc369efbe4d392fed5e36a6ca98a21c26"}, "cc86a272-e5f0-4310-83db-e623bad84518": {"doc_hash": "9c6c7737bb31180bc92f35ae44963163bcdc901329e8f040b6311e469d451c71"}, "8a1f8a3f-7b07-4c3b-b104-94581bd852e3": {"doc_hash": "a58743f2a7b2b252572aa44f01f92e9d04321901f56000bf75585bc25a4bb25b"}}, "docstore/ref_doc_info": {"node-0": {"node_ids": ["2621016f-345a-4369-9c1d-45fd6e51e648", "08786571-48e9-4638-a413-225dc10d656b", "68d99c6e-d5e9-4d68-9f03-28287eaa67ad", "251cedac-8d1d-49d6-9289-d4d4fcd3b73b", "f14e274b-2845-4012-a4c2-424354328c9d", "252707a7-ad63-43f1-af1f-f6f5341fb597", "93048877-9bb4-44e7-ae70-26cc6882cf1d", "81ad88d5-3cd5-40e3-b05e-668e9a7ea24e"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}}, "e7957c52-8cb0-4ce4-8d1d-5dd717722343": {"node_ids": ["node-0", "node-1", "node-2", "node-3"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}}, "node-1": {"node_ids": ["46dbd44c-96d4-4026-8d9d-fcb2a156f05b", "09c739f9-3e07-4f18-b5b0-0f47e73f3b6b", "a6ae9254-7a69-4536-bb28-3c6f34dbd7f5", "ec1dd949-b117-435d-a923-c7d20f514793", "9569cd7b-ec8c-4c29-94ff-64ddcfc74e22", "09aa259a-25bb-4ec9-a9d7-6dcfffbf1475", "83adf9da-0335-4484-89e5-a12b09b1a9d5", "864518fc-a959-494a-968f-a9f6a2bb78ab"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}}, "node-2": {"node_ids": ["e1b39143-a5c1-4315-983b-e7864c4282b0", "cdf8a521-079b-4845-b394-a2dbabc3d18b", "aebadcd3-16f0-411a-9ed9-0a85e9985727", "85ec4a4e-26ba-4529-a45f-1a94c9e8b41a", "1abf114a-26b5-4843-b55f-f5dcbab2ecb9", "828ee0ad-13d6-4194-aedb-e8260b72c6a6"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}}, "node-3": {"node_ids": ["7b96a154-0f8c-4a6b-8ee0-93088c340be2", "72f3ef6f-152c-4087-9942-f7c6989cc824"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Understanding LSTM in Tensorflow"}}}}