{"docstore/data": {"a8f4765b-2ab4-4ebc-ab8b-403bc55d48d0": {"__data__": {"id_": "a8f4765b-2ab4-4ebc-ab8b-403bc55d48d0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "a9eee90a9e1ed67f0bff85466ba40b9ece9f2d26ea4b67fc23a838d24f12727f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "294534d6-b2dc-4166-ab17-884f6c957559", "node_type": "1", "metadata": {}, "hash": "81d3ecaa465f19a757211a9387e62ea8806083d021fce17d09c68165ccf85417", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:29 PM                                       Variable-sharing-in-Tensorflow\n  Tensorflow                                                                                         HOME\n  Variable sharing in Tensorflow\n  In previous   post we got familiar with tensorflow and dived into its under the hood\n  working.In this post we will discuss an important concept that will be particularly\n  useful when we create large models in tensorflow.This post will be based on the\n  concept of variable namespaces and variable sharing in tensorflow.\n  Lets get started!!!", "start_char_idx": 0, "end_char_idx": 569, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "294534d6-b2dc-4166-ab17-884f6c957559": {"__data__": {"id_": "294534d6-b2dc-4166-ab17-884f6c957559", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "a9eee90a9e1ed67f0bff85466ba40b9ece9f2d26ea4b67fc23a838d24f12727f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8f4765b-2ab4-4ebc-ab8b-403bc55d48d0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "9e674c9b014394b14405cc4df2b2a8f5d56a04f196026a22449a9a10f751a5a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd2f205d-d305-4a9e-bbf8-c07ebaa24c00", "node_type": "1", "metadata": {}, "hash": "21f725bce9ec02350632f1c43cec6805d3a4d323c41eb82f39fafed712f6b973", "class_name": "RelatedNodeInfo"}}, "text": "As we have already learnt declaring a variable in tensorflow computation graph is\n  pretty straightforward-\n  var=tf.Variable(tf.random_normal([2,3]),name=\"variable1\")\n  This method of adding variables(using           tf.Variable()        ) to the computation graph is\n  pretty handy but as our graph becomes more complicated and large(both in terms of\n  layers in our graph and number of variables) we would want some hierarchy or\n  organization amongst the variable names to avoid name-clashes.Also many times\n  while implementing complex models we might want to share our variables between\n  layers of your computation graph.One example that comes o\u25af                  the top of my head is\n  Recurrent neural networks.While implementing RNNs we would want to share the\n  weight variable between layers of your computation graph(or network).Tensorflow\n  provides a really lightweight and safe way of sharing variables and organizing variable\n  names by implementing the concept of namespaces or variable scopes.To organize and\n  share variables in tensorflow we have three basic concepts:\n        The method     tf.variable_scope()             which provides simple name-spacing to\n        avoid clashes.", "start_char_idx": 572, "end_char_idx": 1778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "dd2f205d-d305-4a9e-bbf8-c07ebaa24c00": {"__data__": {"id_": "dd2f205d-d305-4a9e-bbf8-c07ebaa24c00", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "a9eee90a9e1ed67f0bff85466ba40b9ece9f2d26ea4b67fc23a838d24f12727f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "294534d6-b2dc-4166-ab17-884f6c957559", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "761123e32df997bce990e5581f5621852c2a4fd0a7e5a39c6eab3db8f4c49c6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4e5f748-1e66-4e67-928d-72134f875a69", "node_type": "1", "metadata": {}, "hash": "088217c7e708e421aac913e9fbe86f353f8346b1a65d949487f04f006187fca2", "class_name": "RelatedNodeInfo"}}, "text": "A reuse    flag which is property of scope that tells the tensorflow environment if we\n        want the variables within that scope to be reused or not.\n        The method     tf.getVariable()           that creates/accesses variables from within a\n        variable scope.    tf.get_variable          is usually called as-\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                          1/104/5/24, 8:29 PM                                  Variable-sharing-in-Tensorflow\n         v  =  tf.get_variable(name,            shape,    dtype,     initializer)\n Rather than going into these concepts one by one,discussing them simultaneously\n would be more e\u25afective as they are very closely related to each other.\n Variable Organization in Tensorflow\n First let us see how variables are organized in tensorflow when we declare them inside\n a scope using the method      tf.getVariable().(Yes,tf.getVariable()                can not only\n access existing variables but can also create new variables.We will look into its details\n soon.For now just remember that it can create as well as access variables.)", "start_char_idx": 1787, "end_char_idx": 2926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f4e5f748-1e66-4e67-928d-72134f875a69": {"__data__": {"id_": "f4e5f748-1e66-4e67-928d-72134f875a69", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "a9eee90a9e1ed67f0bff85466ba40b9ece9f2d26ea4b67fc23a838d24f12727f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd2f205d-d305-4a9e-bbf8-c07ebaa24c00", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "ff867c3031b459644b10f2f2bf6fbd5a0a40f2b05a9eeb7be6a1b7d421388d4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c42c293-d9b3-424b-93f1-ac9695a56b5f", "node_type": "1", "metadata": {}, "hash": "a1d229c076583914d070b592e0ab6636745d9d7cfae543b8388a8a82774f6ad4", "class_name": "RelatedNodeInfo"}}, "text": "#import tensorflow\n  import    tensorflow       as  tf\n  #open a variable scope named 'scope1'\n  with   tf.variable_scope(\"scope1\"):\n        #add a new variable to the graph\n        var=tf.get_variable(\"variable1\",[1])\n  #print the name of variable\n  print(var.name)\n Output:\n  scope1/variable1:0\n In above program we created a variable named          variable1     inside a scope named\n scope.The program outputs the variable name as           scope1/variable1:0.Thus\n variable naming in tensorflow inside a variable scope follows a structure analogous to\n the directory structure i.e.  the scope in which variable is named+name of\n the variable.", "start_char_idx": 2929, "end_char_idx": 3576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2c42c293-d9b3-424b-93f1-ac9695a56b5f": {"__data__": {"id_": "2c42c293-d9b3-424b-93f1-ac9695a56b5f", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "a9eee90a9e1ed67f0bff85466ba40b9ece9f2d26ea4b67fc23a838d24f12727f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4e5f748-1e66-4e67-928d-72134f875a69", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "26850d6440f72fb4af961cbcb8c872e6a0f73da36e0e9d73501885b957e10143", "class_name": "RelatedNodeInfo"}}, "text": "We can also have nested scopes.Again the naming will be analogous\n to the directory structure:\n  #import tensorflow\n  import    tensorflow       as  tf\n  #open a variable scope named 'scope1'\n  with   tf.variable_scope(\"scope1\"):\n        #open a nested scope name 'scope2'\n        with   tf.variable_scope(\"scope2\"):\n              #add a new variable to the graph\n              var=tf.get_variable(\"variable1\",[1])\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                  2/104/5/24, 8:29 PM                                      Variable-sharing-in-Tensorflow\n  #print the name of variable\n  print(var.name)\n  Output:\n  scope1/scope2/variable1:0\n  I know the implementation of these new methods might seem a bit blurry and\n  unclear.Don\u02bct worry!We will get into all the details soon.The only thing I want you to\n  take away from above examples is the fact that variable naming inside a scope is\n  analogous to directory structure.Thats all!", "start_char_idx": 3577, "end_char_idx": 4559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "48a108cb-e7ee-49e2-9b50-fededc741c52": {"__data__": {"id_": "48a108cb-e7ee-49e2-9b50-fededc741c52", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "a9eee90a9e1ed67f0bff85466ba40b9ece9f2d26ea4b67fc23a838d24f12727f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b102fc8-db0d-4967-a6a4-32a5ce730a9d", "node_type": "1", "metadata": {}, "hash": "a09b6913c33c55338d3d1c562b97c543494ef90e97ec5924fc9d5ec3226381b2", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:29 PM                                       Variable-sharing-in-Tensorflow\n  Tensorflow                                                                                         HOME\n  Variable sharing in Tensorflow\n  In previous   post we got familiar with tensorflow and dived into its under the hood\n  working.In this post we will discuss an important concept that will be particularly\n  useful when we create large models in tensorflow.This post will be based on the\n  concept of variable namespaces and variable sharing in tensorflow.\n  Lets get started!!!\n  As we have already learnt declaring a variable in tensorflow computation graph is\n  pretty straightforward-\n  var=tf.Variable(tf.random_normal([2,3]),name=\"variable1\")\n  This method of adding variables(using           tf.Variable()        ) to the computation graph is\n  pretty handy but as our graph becomes more complicated and large(both in terms of\n  layers in our graph and number of variables) we would want some hierarchy or\n  organization amongst the variable names to avoid name-clashes.Also many times\n  while implementing complex models we might want to share our variables between\n  layers of your computation graph.One example that comes o\u25af                  the top of my head is\n  Recurrent neural networks.While implementing RNNs we would want to share the\n  weight variable between layers of your computation graph(or network).Tensorflow\n  provides a really lightweight and safe way of sharing variables and organizing variable\n  names by implementing the concept of namespaces or variable scopes.To organize and\n  share variables in tensorflow we have three basic concepts:\n        The method     tf.variable_scope()             which provides simple name-spacing to\n        avoid clashes.\n        A reuse    flag which is property of scope that tells the tensorflow environment if we\n        want the variables within that scope to be reused or not.\n        The method     tf.getVariable()           that creates/accesses variables from within a\n        variable scope.    tf.get_variable          is usually called as-\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                          1/104/5/24, 8:29 PM                                  Variable-sharing-in-Tensorflow\n         v  =  tf.get_variable(name,            shape,    dtype,     initializer)\n Rather than going into these concepts one by one,discussing them simultaneously\n would be more e\u25afective as they are very closely related to each other.", "start_char_idx": 0, "end_char_idx": 2535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7b102fc8-db0d-4967-a6a4-32a5ce730a9d": {"__data__": {"id_": "7b102fc8-db0d-4967-a6a4-32a5ce730a9d", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "a9eee90a9e1ed67f0bff85466ba40b9ece9f2d26ea4b67fc23a838d24f12727f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48a108cb-e7ee-49e2-9b50-fededc741c52", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "fd059112e186f57003ed722e3efeb8a8e27a385eff38cee429d5149ae853eeb9", "class_name": "RelatedNodeInfo"}}, "text": "Variable Organization in Tensorflow\n First let us see how variables are organized in tensorflow when we declare them inside\n a scope using the method      tf.getVariable().(Yes,tf.getVariable()                can not only\n access existing variables but can also create new variables.We will look into its details\n soon.For now just remember that it can create as well as access variables.)\n  #import tensorflow\n  import    tensorflow       as  tf\n  #open a variable scope named 'scope1'\n  with   tf.variable_scope(\"scope1\"):\n        #add a new variable to the graph\n        var=tf.get_variable(\"variable1\",[1])\n  #print the name of variable\n  print(var.name)\n Output:\n  scope1/variable1:0\n In above program we created a variable named          variable1     inside a scope named\n scope.The program outputs the variable name as           scope1/variable1:0.Thus\n variable naming in tensorflow inside a variable scope follows a structure analogous to\n the directory structure i.e.  the scope in which variable is named+name of\n the variable. We can also have nested scopes.Again the naming will be analogous\n to the directory structure:\n  #import tensorflow\n  import    tensorflow       as  tf\n  #open a variable scope named 'scope1'\n  with   tf.variable_scope(\"scope1\"):\n        #open a nested scope name 'scope2'\n        with   tf.variable_scope(\"scope2\"):\n              #add a new variable to the graph\n              var=tf.get_variable(\"variable1\",[1])\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                  2/104/5/24, 8:29 PM                                      Variable-sharing-in-Tensorflow\n  #print the name of variable\n  print(var.name)\n  Output:\n  scope1/scope2/variable1:0\n  I know the implementation of these new methods might seem a bit blurry and\n  unclear.Don\u02bct worry!We will get into all the details soon.The only thing I want you to\n  take away from above examples is the fact that variable naming inside a scope is\n  analogous to directory structure.Thats all!", "start_char_idx": 2537, "end_char_idx": 4559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-0": {"__data__": {"id_": "node-0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a9db459-33d5-46be-bd35-c7b5f60b6d3b", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "46eb9a5f9b4b2ac8acc32798e52fbed64782d6ec5a5526a03d2b27ec8d35b27b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e61ed096-af54-4710-802c-d7103d66d9c7", "node_type": "1", "metadata": {}, "hash": "f99fd01e4d2db0bacaaed0af219b7bc7e2f518050e04d1ce37b4ea3f7dcc389c", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:29 PM                                       Variable-sharing-in-Tensorflow\n  Tensorflow                                                                                         HOME\n  Variable sharing in Tensorflow\n  In previous   post we got familiar with tensorflow and dived into its under the hood\n  working.In this post we will discuss an important concept that will be particularly\n  useful when we create large models in tensorflow.This post will be based on the\n  concept of variable namespaces and variable sharing in tensorflow.\n  Lets get started!!!\n  As we have already learnt declaring a variable in tensorflow computation graph is\n  pretty straightforward-\n  var=tf.Variable(tf.random_normal([2,3]),name=\"variable1\")\n  This method of adding variables(using           tf.Variable()        ) to the computation graph is\n  pretty handy but as our graph becomes more complicated and large(both in terms of\n  layers in our graph and number of variables) we would want some hierarchy or\n  organization amongst the variable names to avoid name-clashes.Also many times\n  while implementing complex models we might want to share our variables between\n  layers of your computation graph.One example that comes o\u25af                  the top of my head is\n  Recurrent neural networks.While implementing RNNs we would want to share the\n  weight variable between layers of your computation graph(or network).Tensorflow\n  provides a really lightweight and safe way of sharing variables and organizing variable\n  names by implementing the concept of namespaces or variable scopes.To organize and\n  share variables in tensorflow we have three basic concepts:\n        The method     tf.variable_scope()             which provides simple name-spacing to\n        avoid clashes.\n        A reuse    flag which is property of scope that tells the tensorflow environment if we\n        want the variables within that scope to be reused or not.\n        The method     tf.getVariable()           that creates/accesses variables from within a\n        variable scope.    tf.get_variable          is usually called as-\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                          1/104/5/24, 8:29 PM                                  Variable-sharing-in-Tensorflow\n         v  =  tf.get_variable(name,            shape,    dtype,     initializer)\n Rather than going into these concepts one by one,discussing them simultaneously\n would be more e\u25afective as they are very closely related to each other.\n Variable Organization in Tensorflow\n First let us see how variables are organized in tensorflow when we declare them inside\n a scope using the method      tf.getVariable().(Yes,tf.getVariable()                can not only\n access existing variables but can also create new variables.We will look into its details\n soon.For now just remember that it can create as well as access variables.)\n  #import tensorflow\n  import    tensorflow       as  tf\n  #open a variable scope named 'scope1'\n  with   tf.variable_scope(\"scope1\"):\n        #add a new variable to the graph\n        var=tf.get_variable(\"variable1\",[1])\n  #print the name of variable\n  print(var.name)\n Output:\n  scope1/variable1:0\n In above program we created a variable named          variable1     inside a scope named\n scope.The program outputs the variable name as           scope1/variable1:0.Thus\n variable naming in tensorflow inside a variable scope follows a structure analogous to\n the directory structure i.e.  the scope in which variable is named+name of\n the variable. We can also have nested scopes.Again the naming will be analogous\n to the directory structure:\n  #import tensorflow\n  import    tensorflow       as  tf\n  #open a variable scope named 'scope1'\n  with   tf.variable_scope(\"scope1\"):\n        #open a nested scope name 'scope2'\n        with   tf.variable_scope(\"scope2\"):\n              #add a new variable to the graph\n              var=tf.get_variable(\"variable1\",[1])\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                  2/104/5/24, 8:29 PM                                      Variable-sharing-in-Tensorflow\n  #print the name of variable\n  print(var.name)\n  Output:\n  scope1/scope2/variable1:0\n  I know the implementation of these new methods might seem a bit blurry and\n  unclear.Don\u02bct worry!We will get into all the details soon.The only thing I want you to\n  take away from above examples is the fact that variable naming inside a scope is\n  analogous to directory structure.Thats all!", "start_char_idx": 0, "end_char_idx": 4559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a34934d0-0c11-46c8-8370-0c7eefd26659": {"__data__": {"id_": "a34934d0-0c11-46c8-8370-0c7eefd26659", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "42bb70a0c2f59bbcf51df3ba29153e466a997b45996fa52b8c1efca133da33ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61e9dc36-bec3-45ba-ab1a-064a23a5e135", "node_type": "1", "metadata": {}, "hash": "97c6879913a7ef5427724954d7a4a8dd95a4a5d5f65a73f20a81552b41907ae0", "class_name": "RelatedNodeInfo"}}, "text": "Getting into variable sharing\n  When we use the word \u201csharing\u201d for a variable then we are automatically talking about\n  reusing it at multiple places.If we want to share the weight variable between layers of\n  our Recurrent neural network then we can easily say that we are reusing the same\n  weight variable for di\u25aferent layers.Thus the concept of \u201csharing\u201d is very much\n  congruent to \u201creusing\u201d.", "start_char_idx": 0, "end_char_idx": 397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "61e9dc36-bec3-45ba-ab1a-064a23a5e135": {"__data__": {"id_": "61e9dc36-bec3-45ba-ab1a-064a23a5e135", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "42bb70a0c2f59bbcf51df3ba29153e466a997b45996fa52b8c1efca133da33ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a34934d0-0c11-46c8-8370-0c7eefd26659", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "ce35632b685b3ac0de6f87e4ad0cf9c01ceb88a44187fd13726b7a56cfb56c0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6120975-647d-4f60-bec1-c696e88b2f7e", "node_type": "1", "metadata": {}, "hash": "bcdbd17bf5c16fc53c8df9308cae59bbbe700294a2283ba18b1212515ac5d376", "class_name": "RelatedNodeInfo"}}, "text": "The       reuse    flag helps us here.It is property of a scope and has a\n  default value of   False    meaning that we cannot reuse variables within that scope.Let us\n  have a look at the code below to have an idea of what we mean by \u201creusing\u201d a variable\n  and consequence of trying to reuse variables in default case:\n  #import tensorflow\n  import      tensorflow       as   tf\n  #open a variable scope named 'scope1'\n  with    tf.variable_scope(\"scope1\"):\n         #declare a variable named variable1\n         var1    =  tf.get_variable(\"variable1\",[1])\n         #declare another variable with same name\n         var2=tf.get_variable(\"variable1\",[1])\n  In above program we wanted to declare two variables with same tensorflow\n  names(variable1) i.e. we wanted two python variables              var1   and  var2   to share the\n  same tensorflow variable       variable1.", "start_char_idx": 398, "end_char_idx": 1270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a6120975-647d-4f60-bec1-c696e88b2f7e": {"__data__": {"id_": "a6120975-647d-4f60-bec1-c696e88b2f7e", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "42bb70a0c2f59bbcf51df3ba29153e466a997b45996fa52b8c1efca133da33ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61e9dc36-bec3-45ba-ab1a-064a23a5e135", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "bbe466178653e4b512304150ba6bc36b3ffad37b889553a8c5fbb514ad69bf5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31104e36-c030-40f2-897c-ea0f0d96e7ec", "node_type": "1", "metadata": {}, "hash": "1022576910f4f9e08feee4dd98a9c9f97ec984f3eecc7ed3f89eb06e0f2121ae", "class_name": "RelatedNodeInfo"}}, "text": "(Note that this is just a handy trick to visualize\n  the significance of tensorflow names of variables.In reality when we declare a variable\n  var1   in python and name it      variable1      in tensorflow they both corrospond to same\n  variable known as      var1   in python environment and        scope1/variable1(in this case)\n  in tensorflow environment.)The above program will generate a Value Error as\n  scope1/variable1          was already declared when we declared            var1   and the value of\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                     3/104/5/24, 8:29 PM                                   Variable-sharing-in-Tensorflow\n  reuse   flag was  False   in default case. Now let us explicitly set the value of thereuse\n  flag to be True.We can do this in two ways.We can either set the          reuse   flag toTrue\n  when we open a scope or we can call the method\n  tf.get_variable_scope().reuse_variables()                      anywhere inside the scope to\n  set the flag to True.", "start_char_idx": 1270, "end_char_idx": 2322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "31104e36-c030-40f2-897c-ea0f0d96e7ec": {"__data__": {"id_": "31104e36-c030-40f2-897c-ea0f0d96e7ec", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "42bb70a0c2f59bbcf51df3ba29153e466a997b45996fa52b8c1efca133da33ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6120975-647d-4f60-bec1-c696e88b2f7e", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "75c5bc56ffced7ed3857316a461b40ad08103c8eb806ad2cb825e2e807b3da75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c72d6e8c-5636-41d4-8a6e-2244263b8c65", "node_type": "1", "metadata": {}, "hash": "d89f7e9da6aa01e0c6c3369a7bddc6f749ea41b59962a0f872ec93ed3e6e9756", "class_name": "RelatedNodeInfo"}}, "text": "#import tensorflow\n  import     tensorflow       as  tf\n  #open a variable scope named 'scope1'\n  with    tf.variable_scope(\"scope1\"):\n        #declare a variable named variable1\n              var1    =  tf.get_variable(\"variable1\",[1])\n              #set reuse flag to True\n              tf.get_variable_scope().reuse_variables()\n              #just an assertion!\n              assert     tf.get_variable_scope().reuse==True\n              #declare another variable with same name\n        var2=tf.get_variable(\"variable1\",[1])\n  assert     var1==var2\n  The above program runs without any error.The last assertion(assert var1==var2)\n  essentially captures the essence of the term \u201creusing\u201d variable as         var1   and  var2\n  correspond to same variable which is known by the name \u201cscope1/variable1\u201d in\n  tensorflow environment.\n  reuse   flag and  tf.get_variable()\n  We have already seen that the method        tf.get_variable()          can create new variables\n  as well as access the existing ones.", "start_char_idx": 2325, "end_char_idx": 3330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c72d6e8c-5636-41d4-8a6e-2244263b8c65": {"__data__": {"id_": "c72d6e8c-5636-41d4-8a6e-2244263b8c65", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "42bb70a0c2f59bbcf51df3ba29153e466a997b45996fa52b8c1efca133da33ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31104e36-c030-40f2-897c-ea0f0d96e7ec", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "207219a31c79b62dc53c77caea953b1ab107c2df0cf34035bf9e57d454cf00f8", "class_name": "RelatedNodeInfo"}}, "text": "The     reuse   flag determines the behaviour of the\n  function  tf.get_variable().There can be two possibilities:\n       The  reuse   flag is set to False:\n       If the reuse   flag isFalse   then  tf.get_variable         will first check if a variable\n       with the name equivalent to current scope name           + the provided name (analogous\n       to directory structure) exists.If it exists then it generates a Value Error otherwise it\n       creates the new variable.\n         #import tensorflow\n         import     tensorflow       as  tf\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                  4/104/5/24, 8:29 PM                                  Variable-sharing-in-Tensorflow\n         #open a variable scope named 'scope1'\n         with   tf.variable_scope(\"scope1\"):\n             #declare a variable named variable1\n             var1    =  tf.get_variable(\"variable1\",[1])\n             #declare another variable with same name\n             var2=tf.get_variable(\"variable1\",[1])\n  The above program generates a Value Error.", "start_char_idx": 3331, "end_char_idx": 4412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "492c9cdd-5d32-48fa-b679-ad48fb637755": {"__data__": {"id_": "492c9cdd-5d32-48fa-b679-ad48fb637755", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "42bb70a0c2f59bbcf51df3ba29153e466a997b45996fa52b8c1efca133da33ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7dc350b3-9b8a-4ce8-b8f1-d610b7a28709", "node_type": "1", "metadata": {}, "hash": "46fafadc495c48b725bd756ca1bc7762aa853a8941d01fd182530546a92a7a6a", "class_name": "RelatedNodeInfo"}}, "text": "Getting into variable sharing\n  When we use the word \u201csharing\u201d for a variable then we are automatically talking about\n  reusing it at multiple places.If we want to share the weight variable between layers of\n  our Recurrent neural network then we can easily say that we are reusing the same\n  weight variable for di\u25aferent layers.Thus the concept of \u201csharing\u201d is very much\n  congruent to \u201creusing\u201d. The       reuse    flag helps us here.It is property of a scope and has a\n  default value of   False    meaning that we cannot reuse variables within that scope.Let us\n  have a look at the code below to have an idea of what we mean by \u201creusing\u201d a variable\n  and consequence of trying to reuse variables in default case:\n  #import tensorflow\n  import      tensorflow       as   tf\n  #open a variable scope named 'scope1'\n  with    tf.variable_scope(\"scope1\"):\n         #declare a variable named variable1\n         var1    =  tf.get_variable(\"variable1\",[1])\n         #declare another variable with same name\n         var2=tf.get_variable(\"variable1\",[1])\n  In above program we wanted to declare two variables with same tensorflow\n  names(variable1) i.e. we wanted two python variables              var1   and  var2   to share the\n  same tensorflow variable       variable1.(Note that this is just a handy trick to visualize\n  the significance of tensorflow names of variables.In reality when we declare a variable\n  var1   in python and name it      variable1      in tensorflow they both corrospond to same\n  variable known as      var1   in python environment and        scope1/variable1(in this case)\n  in tensorflow environment.)The above program will generate a Value Error as\n  scope1/variable1          was already declared when we declared            var1   and the value of\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                     3/104/5/24, 8:29 PM                                   Variable-sharing-in-Tensorflow\n  reuse   flag was  False   in default case.", "start_char_idx": 0, "end_char_idx": 2011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7dc350b3-9b8a-4ce8-b8f1-d610b7a28709": {"__data__": {"id_": "7dc350b3-9b8a-4ce8-b8f1-d610b7a28709", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "42bb70a0c2f59bbcf51df3ba29153e466a997b45996fa52b8c1efca133da33ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "492c9cdd-5d32-48fa-b679-ad48fb637755", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "578bcc4db5e8e5508f405aa56eb72ad42d3ed4737581e9bf29ac62838ba24675", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c551a6f8-073b-41e2-8e9f-c89558ef2da1", "node_type": "1", "metadata": {}, "hash": "dfa792bf18af088c02f30198cc44e5d0f3d4936b594efe37438eebd105e29bd3", "class_name": "RelatedNodeInfo"}}, "text": "Now let us explicitly set the value of thereuse\n  flag to be True.We can do this in two ways.We can either set the          reuse   flag toTrue\n  when we open a scope or we can call the method\n  tf.get_variable_scope().reuse_variables()                      anywhere inside the scope to\n  set the flag to True.\n  #import tensorflow\n  import     tensorflow       as  tf\n  #open a variable scope named 'scope1'\n  with    tf.variable_scope(\"scope1\"):\n        #declare a variable named variable1\n              var1    =  tf.get_variable(\"variable1\",[1])\n              #set reuse flag to True\n              tf.get_variable_scope().reuse_variables()\n              #just an assertion!\n              assert     tf.get_variable_scope().reuse==True\n              #declare another variable with same name\n        var2=tf.get_variable(\"variable1\",[1])\n  assert     var1==var2\n  The above program runs without any error.The last assertion(assert var1==var2)\n  essentially captures the essence of the term \u201creusing\u201d variable as         var1   and  var2\n  correspond to same variable which is known by the name \u201cscope1/variable1\u201d in\n  tensorflow environment.\n  reuse   flag and  tf.get_variable()\n  We have already seen that the method        tf.get_variable()          can create new variables\n  as well as access the existing ones. The     reuse   flag determines the behaviour of the\n  function  tf.get_variable().There can be two possibilities:\n       The  reuse   flag is set to False:\n       If the reuse   flag isFalse   then  tf.get_variable         will first check if a variable\n       with the name equivalent to current scope name           + the provided name (analogous\n       to directory structure) exists.If it exists then it generates a Value Error otherwise it\n       creates the new variable.", "start_char_idx": 2012, "end_char_idx": 3809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c551a6f8-073b-41e2-8e9f-c89558ef2da1": {"__data__": {"id_": "c551a6f8-073b-41e2-8e9f-c89558ef2da1", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "42bb70a0c2f59bbcf51df3ba29153e466a997b45996fa52b8c1efca133da33ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7dc350b3-9b8a-4ce8-b8f1-d610b7a28709", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "60e71512a7fb4f5c28a24fb37ccbd925f60b3ed99454c146ad03d902ec0fd94b", "class_name": "RelatedNodeInfo"}}, "text": "#import tensorflow\n         import     tensorflow       as  tf\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                  4/104/5/24, 8:29 PM                                  Variable-sharing-in-Tensorflow\n         #open a variable scope named 'scope1'\n         with   tf.variable_scope(\"scope1\"):\n             #declare a variable named variable1\n             var1    =  tf.get_variable(\"variable1\",[1])\n             #declare another variable with same name\n             var2=tf.get_variable(\"variable1\",[1])\n  The above program generates a Value Error.", "start_char_idx": 3819, "end_char_idx": 4412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-1": {"__data__": {"id_": "node-1", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a9db459-33d5-46be-bd35-c7b5f60b6d3b", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "46eb9a5f9b4b2ac8acc32798e52fbed64782d6ec5a5526a03d2b27ec8d35b27b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f51baa56-da7d-4b74-aca8-17d1a0c33623", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "a9eee90a9e1ed67f0bff85466ba40b9ece9f2d26ea4b67fc23a838d24f12727f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a30f3c7-c53f-4389-af02-158d128d0ad2", "node_type": "1", "metadata": {}, "hash": "640f02047a688fcee7a913972a187acdae5b6dd3c4153ca5bf19c6b3a585f4b2", "class_name": "RelatedNodeInfo"}}, "text": "Getting into variable sharing\n  When we use the word \u201csharing\u201d for a variable then we are automatically talking about\n  reusing it at multiple places.If we want to share the weight variable between layers of\n  our Recurrent neural network then we can easily say that we are reusing the same\n  weight variable for di\u25aferent layers.Thus the concept of \u201csharing\u201d is very much\n  congruent to \u201creusing\u201d. The       reuse    flag helps us here.It is property of a scope and has a\n  default value of   False    meaning that we cannot reuse variables within that scope.Let us\n  have a look at the code below to have an idea of what we mean by \u201creusing\u201d a variable\n  and consequence of trying to reuse variables in default case:\n  #import tensorflow\n  import      tensorflow       as   tf\n  #open a variable scope named 'scope1'\n  with    tf.variable_scope(\"scope1\"):\n         #declare a variable named variable1\n         var1    =  tf.get_variable(\"variable1\",[1])\n         #declare another variable with same name\n         var2=tf.get_variable(\"variable1\",[1])\n  In above program we wanted to declare two variables with same tensorflow\n  names(variable1) i.e. we wanted two python variables              var1   and  var2   to share the\n  same tensorflow variable       variable1.(Note that this is just a handy trick to visualize\n  the significance of tensorflow names of variables.In reality when we declare a variable\n  var1   in python and name it      variable1      in tensorflow they both corrospond to same\n  variable known as      var1   in python environment and        scope1/variable1(in this case)\n  in tensorflow environment.)The above program will generate a Value Error as\n  scope1/variable1          was already declared when we declared            var1   and the value of\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                     3/104/5/24, 8:29 PM                                   Variable-sharing-in-Tensorflow\n  reuse   flag was  False   in default case. Now let us explicitly set the value of thereuse\n  flag to be True.We can do this in two ways.We can either set the          reuse   flag toTrue\n  when we open a scope or we can call the method\n  tf.get_variable_scope().reuse_variables()                      anywhere inside the scope to\n  set the flag to True.\n  #import tensorflow\n  import     tensorflow       as  tf\n  #open a variable scope named 'scope1'\n  with    tf.variable_scope(\"scope1\"):\n        #declare a variable named variable1\n              var1    =  tf.get_variable(\"variable1\",[1])\n              #set reuse flag to True\n              tf.get_variable_scope().reuse_variables()\n              #just an assertion!\n              assert     tf.get_variable_scope().reuse==True\n              #declare another variable with same name\n        var2=tf.get_variable(\"variable1\",[1])\n  assert     var1==var2\n  The above program runs without any error.The last assertion(assert var1==var2)\n  essentially captures the essence of the term \u201creusing\u201d variable as         var1   and  var2\n  correspond to same variable which is known by the name \u201cscope1/variable1\u201d in\n  tensorflow environment.\n  reuse   flag and  tf.get_variable()\n  We have already seen that the method        tf.get_variable()          can create new variables\n  as well as access the existing ones. The     reuse   flag determines the behaviour of the\n  function  tf.get_variable().There can be two possibilities:\n       The  reuse   flag is set to False:\n       If the reuse   flag isFalse   then  tf.get_variable         will first check if a variable\n       with the name equivalent to current scope name           + the provided name (analogous\n       to directory structure) exists.If it exists then it generates a Value Error otherwise it\n       creates the new variable.\n         #import tensorflow\n         import     tensorflow       as  tf\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                  4/104/5/24, 8:29 PM                                  Variable-sharing-in-Tensorflow\n         #open a variable scope named 'scope1'\n         with   tf.variable_scope(\"scope1\"):\n             #declare a variable named variable1\n             var1    =  tf.get_variable(\"variable1\",[1])\n             #declare another variable with same name\n             var2=tf.get_variable(\"variable1\",[1])\n  The above program generates a Value Error.", "start_char_idx": 4562, "end_char_idx": 8974, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2560a6e7-8ef3-4ae1-b046-f444162dbbc0": {"__data__": {"id_": "2560a6e7-8ef3-4ae1-b046-f444162dbbc0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "efbf773ada9c16adabc16e337124ae7545eeb80f67472f3b5ea57c71fca4cfd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7a72e93-4e35-4034-9f17-468082465178", "node_type": "1", "metadata": {}, "hash": "d4c2bfe2bad78fc8d3d36634e3338e408a0080e0d28f60399f845ff54f8fdc22", "class_name": "RelatedNodeInfo"}}, "text": "The  reuse   flag is set toTrue:\n       If thereuse   flag is set to true within a scope then   tf.get_variable()         will look\n       for the variable with the name equivalent to current scope name          + the provided\n       name (analogous to directory structure).If it exists then it will return that existing\n       variable otherwise it throws a Value Error.\n         #import tensorflow\n         import    tensorflow       as  tf\n         #open a variable scope named 'scope1'\n         with   tf.variable_scope(\"scope1\"):\n             #declare a variable named variable1\n             var1    =  tf.get_variable(\"variable1\",[1])\n             #set reuse flag to True\n             tf.get_variable_scope().reuse_variables()\n             #just an assertion!", "start_char_idx": 0, "end_char_idx": 765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c7a72e93-4e35-4034-9f17-468082465178": {"__data__": {"id_": "c7a72e93-4e35-4034-9f17-468082465178", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "efbf773ada9c16adabc16e337124ae7545eeb80f67472f3b5ea57c71fca4cfd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2560a6e7-8ef3-4ae1-b046-f444162dbbc0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "a1e07fe84023d3326d0b277d789eff4486e76ddbbc4be829d426440cfcb22666", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d02e5c0-da8f-40b9-8b2a-fa171e4380c0", "node_type": "1", "metadata": {}, "hash": "c6c409e7228d7e62490a2e1dcc1b46c85c329a3899e958f5fd56339877dcc328", "class_name": "RelatedNodeInfo"}}, "text": "assert     tf.get_variable_scope().reuse==True\n             #declare another variable with same name\n             var2=tf.get_variable(\"variable1\",[1])\n  In the above program when we declare the variable         var1, the   reuse   flag was with its\n  default value of  False   and so the method     tf.get_variable()         searched for variable\n  named   scope1/variable1.As it did not exist,it created a new variable.A\u25afer setting\n  reuse   to true when we called    tf.get_variable()         again to declare variable   var2\n  with same tensorflow name       scope1/variable1        it again searched for a variable\n  named   scope1/variable1         and this time found it thus returning and storing it in\n  var2.\n  Name scopes in Tensorflow\n  We saw that   tf.variable_scope         a\u25afects the variable names declared in that\n  scope.But sometimes in large computation graphs we would like to organize the names\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                  5/104/5/24,", "start_char_idx": 779, "end_char_idx": 1807, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3d02e5c0-da8f-40b9-8b2a-fa171e4380c0": {"__data__": {"id_": "3d02e5c0-da8f-40b9-8b2a-fa171e4380c0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "efbf773ada9c16adabc16e337124ae7545eeb80f67472f3b5ea57c71fca4cfd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7a72e93-4e35-4034-9f17-468082465178", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "de019e5b80c3fb84d624c4d3b0746e17befd2313f866e81a6d155097ae385f64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31e4373e-74e3-4f90-9421-de0bfc993a6d", "node_type": "1", "metadata": {}, "hash": "e43efd225e861e15503062a004ccd694f1d3e9cef992dfb6579222111a76f241", "class_name": "RelatedNodeInfo"}}, "text": "8:29 PM                                 Variable-sharing-in-Tensorflow\n of our operations too.This feature is added by name scopes in tensorflow.We can open\n a name scope by     tf.name_scope.A combined usage of name scope and variable\n scope is shown:\n  #import tensorflow\n  import    tensorflow      as   tf\n  #open a variable scope named 'variable_scope1'\n  with   tf.variable_scope(\"variable_scope1\"):\n        #open a name scope named 'name_scope1'\n        with   tf.name_scope(\"name_scope1\"):\n              #declare a variable named variable1\n              var1   =  tf.get_variable(\"variable1\",[1])\n              #declare an operation\n              var2=1.0+var1\n  print(var1.name)\n  print(var2.name)\n Output:\n  variable_scope1/variable1:0\n  variable_scope1/name_scope1/add:0\n From output it is clear that variable scope does a\u25afect the operation name and name\n scope is ignored by   tf.", "start_char_idx": 1808, "end_char_idx": 2700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "31e4373e-74e3-4f90-9421-de0bfc993a6d": {"__data__": {"id_": "31e4373e-74e3-4f90-9421-de0bfc993a6d", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "efbf773ada9c16adabc16e337124ae7545eeb80f67472f3b5ea57c71fca4cfd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d02e5c0-da8f-40b9-8b2a-fa171e4380c0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "5adf0bb20db37ebad104123a6f3f139c50ced0335366b77ddcf33ac8a7ac7cd3", "class_name": "RelatedNodeInfo"}}, "text": "get_variable\n Although we have not explored a practical example where namespaces and variable\n sharing are used but I hope this post helped you understand the concept of variable\n sharing in tensorflow.We will implement this concept when we will implement complex\n models like RNNs in tensorflow in future blogposts.\n                                   Posted on 3 February,2017\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                      6/10", "start_char_idx": 2700, "end_char_idx": 3169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ce97ae73-b852-48a0-8e7f-62f397a68bd8": {"__data__": {"id_": "ce97ae73-b852-48a0-8e7f-62f397a68bd8", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "efbf773ada9c16adabc16e337124ae7545eeb80f67472f3b5ea57c71fca4cfd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03fc838f-c1f3-4365-91fa-6c33aea5098d", "node_type": "1", "metadata": {}, "hash": "05bf100ded0f1c306d0002e1785057bf9ca7d3226344c519e50dfba7d83cfe2a", "class_name": "RelatedNodeInfo"}}, "text": "The  reuse   flag is set toTrue:\n       If thereuse   flag is set to true within a scope then   tf.get_variable()         will look\n       for the variable with the name equivalent to current scope name          + the provided\n       name (analogous to directory structure).If it exists then it will return that existing\n       variable otherwise it throws a Value Error.\n         #import tensorflow\n         import    tensorflow       as  tf\n         #open a variable scope named 'scope1'\n         with   tf.variable_scope(\"scope1\"):\n             #declare a variable named variable1\n             var1    =  tf.get_variable(\"variable1\",[1])\n             #set reuse flag to True\n             tf.get_variable_scope().reuse_variables()\n             #just an assertion!\n             assert     tf.get_variable_scope().reuse==True\n             #declare another variable with same name\n             var2=tf.get_variable(\"variable1\",[1])\n  In the above program when we declare the variable         var1, the   reuse   flag was with its\n  default value of  False   and so the method     tf.get_variable()         searched for variable\n  named   scope1/variable1.As it did not exist,it created a new variable.A\u25afer setting\n  reuse   to true when we called    tf.get_variable()         again to declare variable   var2\n  with same tensorflow name       scope1/variable1        it again searched for a variable\n  named   scope1/variable1         and this time found it thus returning and storing it in\n  var2.", "start_char_idx": 0, "end_char_idx": 1497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "03fc838f-c1f3-4365-91fa-6c33aea5098d": {"__data__": {"id_": "03fc838f-c1f3-4365-91fa-6c33aea5098d", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "efbf773ada9c16adabc16e337124ae7545eeb80f67472f3b5ea57c71fca4cfd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce97ae73-b852-48a0-8e7f-62f397a68bd8", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "2b11390aee148b42985831f7ae15782f7394b41c24e957ed96f43cd2019b6235", "class_name": "RelatedNodeInfo"}}, "text": "Name scopes in Tensorflow\n  We saw that   tf.variable_scope         a\u25afects the variable names declared in that\n  scope.But sometimes in large computation graphs we would like to organize the names\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                  5/104/5/24, 8:29 PM                                 Variable-sharing-in-Tensorflow\n of our operations too.This feature is added by name scopes in tensorflow.We can open\n a name scope by     tf.name_scope.A combined usage of name scope and variable\n scope is shown:\n  #import tensorflow\n  import    tensorflow      as   tf\n  #open a variable scope named 'variable_scope1'\n  with   tf.variable_scope(\"variable_scope1\"):\n        #open a name scope named 'name_scope1'\n        with   tf.name_scope(\"name_scope1\"):\n              #declare a variable named variable1\n              var1   =  tf.get_variable(\"variable1\",[1])\n              #declare an operation\n              var2=1.0+var1\n  print(var1.name)\n  print(var2.name)\n Output:\n  variable_scope1/variable1:0\n  variable_scope1/name_scope1/add:0\n From output it is clear that variable scope does a\u25afect the operation name and name\n scope is ignored by   tf.get_variable\n Although we have not explored a practical example where namespaces and variable\n sharing are used but I hope this post helped you understand the concept of variable\n sharing in tensorflow.We will implement this concept when we will implement complex\n models like RNNs in tensorflow in future blogposts.\n                                   Posted on 3 February,2017\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                      6/10", "start_char_idx": 1500, "end_char_idx": 3169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-2": {"__data__": {"id_": "node-2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a9db459-33d5-46be-bd35-c7b5f60b6d3b", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "46eb9a5f9b4b2ac8acc32798e52fbed64782d6ec5a5526a03d2b27ec8d35b27b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e61ed096-af54-4710-802c-d7103d66d9c7", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}, "hash": "42bb70a0c2f59bbcf51df3ba29153e466a997b45996fa52b8c1efca133da33ba", "class_name": "RelatedNodeInfo"}}, "text": "The  reuse   flag is set toTrue:\n       If thereuse   flag is set to true within a scope then   tf.get_variable()         will look\n       for the variable with the name equivalent to current scope name          + the provided\n       name (analogous to directory structure).If it exists then it will return that existing\n       variable otherwise it throws a Value Error.\n         #import tensorflow\n         import    tensorflow       as  tf\n         #open a variable scope named 'scope1'\n         with   tf.variable_scope(\"scope1\"):\n             #declare a variable named variable1\n             var1    =  tf.get_variable(\"variable1\",[1])\n             #set reuse flag to True\n             tf.get_variable_scope().reuse_variables()\n             #just an assertion!\n             assert     tf.get_variable_scope().reuse==True\n             #declare another variable with same name\n             var2=tf.get_variable(\"variable1\",[1])\n  In the above program when we declare the variable         var1, the   reuse   flag was with its\n  default value of  False   and so the method     tf.get_variable()         searched for variable\n  named   scope1/variable1.As it did not exist,it created a new variable.A\u25afer setting\n  reuse   to true when we called    tf.get_variable()         again to declare variable   var2\n  with same tensorflow name       scope1/variable1        it again searched for a variable\n  named   scope1/variable1         and this time found it thus returning and storing it in\n  var2.\n  Name scopes in Tensorflow\n  We saw that   tf.variable_scope         a\u25afects the variable names declared in that\n  scope.But sometimes in large computation graphs we would like to organize the names\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                                  5/104/5/24, 8:29 PM                                 Variable-sharing-in-Tensorflow\n of our operations too.This feature is added by name scopes in tensorflow.We can open\n a name scope by     tf.name_scope.A combined usage of name scope and variable\n scope is shown:\n  #import tensorflow\n  import    tensorflow      as   tf\n  #open a variable scope named 'variable_scope1'\n  with   tf.variable_scope(\"variable_scope1\"):\n        #open a name scope named 'name_scope1'\n        with   tf.name_scope(\"name_scope1\"):\n              #declare a variable named variable1\n              var1   =  tf.get_variable(\"variable1\",[1])\n              #declare an operation\n              var2=1.0+var1\n  print(var1.name)\n  print(var2.name)\n Output:\n  variable_scope1/variable1:0\n  variable_scope1/name_scope1/add:0\n From output it is clear that variable scope does a\u25afect the operation name and name\n scope is ignored by   tf.get_variable\n Although we have not explored a practical example where namespaces and variable\n sharing are used but I hope this post helped you understand the concept of variable\n sharing in tensorflow.We will implement this concept when we will implement complex\n models like RNNs in tensorflow in future blogposts.\n                                   Posted on 3 February,2017\nhttps://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/                      6/10", "start_char_idx": 8982, "end_char_idx": 12151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}}, "docstore/metadata": {"a8f4765b-2ab4-4ebc-ab8b-403bc55d48d0": {"doc_hash": "9e674c9b014394b14405cc4df2b2a8f5d56a04f196026a22449a9a10f751a5a8", "ref_doc_id": "node-0"}, "294534d6-b2dc-4166-ab17-884f6c957559": {"doc_hash": "761123e32df997bce990e5581f5621852c2a4fd0a7e5a39c6eab3db8f4c49c6c", "ref_doc_id": "node-0"}, "dd2f205d-d305-4a9e-bbf8-c07ebaa24c00": {"doc_hash": "ff867c3031b459644b10f2f2bf6fbd5a0a40f2b05a9eeb7be6a1b7d421388d4c", "ref_doc_id": "node-0"}, "f4e5f748-1e66-4e67-928d-72134f875a69": {"doc_hash": "26850d6440f72fb4af961cbcb8c872e6a0f73da36e0e9d73501885b957e10143", "ref_doc_id": "node-0"}, "2c42c293-d9b3-424b-93f1-ac9695a56b5f": {"doc_hash": "c348bd797c3a1cffac4916b98f00a318f0c6ef5298509bf2ffb6e4b5926d3ea0", "ref_doc_id": "node-0"}, "48a108cb-e7ee-49e2-9b50-fededc741c52": {"doc_hash": "fd059112e186f57003ed722e3efeb8a8e27a385eff38cee429d5149ae853eeb9", "ref_doc_id": "node-0"}, "7b102fc8-db0d-4967-a6a4-32a5ce730a9d": {"doc_hash": "cd13ac258dfa3bb48ffa9ee2756806b47647ad6ed7523151f0f0541375183808", "ref_doc_id": "node-0"}, "node-0": {"doc_hash": "a9eee90a9e1ed67f0bff85466ba40b9ece9f2d26ea4b67fc23a838d24f12727f", "ref_doc_id": "9a9db459-33d5-46be-bd35-c7b5f60b6d3b"}, "a34934d0-0c11-46c8-8370-0c7eefd26659": {"doc_hash": "ce35632b685b3ac0de6f87e4ad0cf9c01ceb88a44187fd13726b7a56cfb56c0c", "ref_doc_id": "node-1"}, "61e9dc36-bec3-45ba-ab1a-064a23a5e135": {"doc_hash": "bbe466178653e4b512304150ba6bc36b3ffad37b889553a8c5fbb514ad69bf5e", "ref_doc_id": "node-1"}, "a6120975-647d-4f60-bec1-c696e88b2f7e": {"doc_hash": "75c5bc56ffced7ed3857316a461b40ad08103c8eb806ad2cb825e2e807b3da75", "ref_doc_id": "node-1"}, "31104e36-c030-40f2-897c-ea0f0d96e7ec": {"doc_hash": "207219a31c79b62dc53c77caea953b1ab107c2df0cf34035bf9e57d454cf00f8", "ref_doc_id": "node-1"}, "c72d6e8c-5636-41d4-8a6e-2244263b8c65": {"doc_hash": "2ffe18ef1bc705ea0b739e9257c66a30d30babf1f7689740f02a1193fd24d6b2", "ref_doc_id": "node-1"}, "492c9cdd-5d32-48fa-b679-ad48fb637755": {"doc_hash": "578bcc4db5e8e5508f405aa56eb72ad42d3ed4737581e9bf29ac62838ba24675", "ref_doc_id": "node-1"}, "7dc350b3-9b8a-4ce8-b8f1-d610b7a28709": {"doc_hash": "60e71512a7fb4f5c28a24fb37ccbd925f60b3ed99454c146ad03d902ec0fd94b", "ref_doc_id": "node-1"}, "c551a6f8-073b-41e2-8e9f-c89558ef2da1": {"doc_hash": "2246f746ee9b4f0f6f7bf1d3d535b3a6e2c89cdb3879a00132ba51d773cbdbf9", "ref_doc_id": "node-1"}, "node-1": {"doc_hash": "42bb70a0c2f59bbcf51df3ba29153e466a997b45996fa52b8c1efca133da33ba", "ref_doc_id": "9a9db459-33d5-46be-bd35-c7b5f60b6d3b"}, "2560a6e7-8ef3-4ae1-b046-f444162dbbc0": {"doc_hash": "a1e07fe84023d3326d0b277d789eff4486e76ddbbc4be829d426440cfcb22666", "ref_doc_id": "node-2"}, "c7a72e93-4e35-4034-9f17-468082465178": {"doc_hash": "de019e5b80c3fb84d624c4d3b0746e17befd2313f866e81a6d155097ae385f64", "ref_doc_id": "node-2"}, "3d02e5c0-da8f-40b9-8b2a-fa171e4380c0": {"doc_hash": "5adf0bb20db37ebad104123a6f3f139c50ced0335366b77ddcf33ac8a7ac7cd3", "ref_doc_id": "node-2"}, "31e4373e-74e3-4f90-9421-de0bfc993a6d": {"doc_hash": "ec6c6cbc490aa9fe3c6990b22bf523eab55efa6d113d4c0e4b17aff1ebf2876c", "ref_doc_id": "node-2"}, "ce97ae73-b852-48a0-8e7f-62f397a68bd8": {"doc_hash": "2b11390aee148b42985831f7ae15782f7394b41c24e957ed96f43cd2019b6235", "ref_doc_id": "node-2"}, "03fc838f-c1f3-4365-91fa-6c33aea5098d": {"doc_hash": "6cfe97b4fd2754f8f9e0c3682d6948b719df9da29351b1611462898e2ce09fab", "ref_doc_id": "node-2"}, "node-2": {"doc_hash": "efbf773ada9c16adabc16e337124ae7545eeb80f67472f3b5ea57c71fca4cfd2", "ref_doc_id": "9a9db459-33d5-46be-bd35-c7b5f60b6d3b"}}, "docstore/ref_doc_info": {"node-0": {"node_ids": ["a8f4765b-2ab4-4ebc-ab8b-403bc55d48d0", "294534d6-b2dc-4166-ab17-884f6c957559", "dd2f205d-d305-4a9e-bbf8-c07ebaa24c00", "f4e5f748-1e66-4e67-928d-72134f875a69", "2c42c293-d9b3-424b-93f1-ac9695a56b5f", "48a108cb-e7ee-49e2-9b50-fededc741c52", "7b102fc8-db0d-4967-a6a4-32a5ce730a9d"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}}, "9a9db459-33d5-46be-bd35-c7b5f60b6d3b": {"node_ids": ["node-0", "node-1", "node-2"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}}, "node-1": {"node_ids": ["a34934d0-0c11-46c8-8370-0c7eefd26659", "61e9dc36-bec3-45ba-ab1a-064a23a5e135", "a6120975-647d-4f60-bec1-c696e88b2f7e", "31104e36-c030-40f2-897c-ea0f0d96e7ec", "c72d6e8c-5636-41d4-8a6e-2244263b8c65", "492c9cdd-5d32-48fa-b679-ad48fb637755", "7dc350b3-9b8a-4ce8-b8f1-d610b7a28709", "c551a6f8-073b-41e2-8e9f-c89558ef2da1"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}}, "node-2": {"node_ids": ["2560a6e7-8ef3-4ae1-b046-f444162dbbc0", "c7a72e93-4e35-4034-9f17-468082465178", "3d02e5c0-da8f-40b9-8b2a-fa171e4380c0", "31e4373e-74e3-4f90-9421-de0bfc993a6d", "ce97ae73-b852-48a0-8e7f-62f397a68bd8", "03fc838f-c1f3-4365-91fa-6c33aea5098d"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Variable sharing in Tensorflow"}}}}