{"docstore/data": {"0485c35d-5dd7-4a75-bfb9-a02b8e2f9755": {"__data__": {"id_": "0485c35d-5dd7-4a75-bfb9-a02b8e2f9755", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "07945799e149dc2b1a5075bffc957b60a70311b4e70cee6bb560a62d013b47a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46e533c5-db0d-415b-992a-fd22790e8dd3", "node_type": "1", "metadata": {}, "hash": "835eba8171873d15a3bb811ab214bd25cc0574b84b04f1ebe578d7ff4d9ad373", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:24 PM                                          Lets-Practice-Backpropagation\n  Backpropagation                                                                                     HOME\n  Lets practice Backpropagation\n  In theprevious post    we went through a system of nested nodes and analysed the update rules for\n  the system.We also went through the intuitive notion of backpropagation and figured out that it is\n  nothing but applying chain rule over and over again.Initially for this post I was looking to apply\n  backpropagation to neural networks but then I felt some practice of chain rule in complex systems\n  would not hurt.So,in this post we will apply backpropogation to systems with complex functions\n  so that the reader gets comfortable with chain rule and its applications to complex systems.\n  Lets get started!!!", "start_char_idx": 0, "end_char_idx": 841, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "46e533c5-db0d-415b-992a-fd22790e8dd3": {"__data__": {"id_": "46e533c5-db0d-415b-992a-fd22790e8dd3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "07945799e149dc2b1a5075bffc957b60a70311b4e70cee6bb560a62d013b47a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0485c35d-5dd7-4a75-bfb9-a02b8e2f9755", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "83d9a1cdff31ac29aecc132a9784f77c987cb809066f09e2d62eb33bad5e5c41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a4182d7-4c57-4dfe-8486-55a51290aadb", "node_type": "1", "metadata": {}, "hash": "c6ec4ebbcebbd825b46edfca8f512422f2dde9a3b9c10ade0e15fd7d69d65ebf", "class_name": "RelatedNodeInfo"}}, "text": "Lets start with a single node system but this time with a complex function:\n                  sigmoid unit\n                   M/(I+e(-a))\n  This system can be represented as:\n  import     numpy    as   np\n  def   sigmoid(x):\n  a=-2        return     1/(1+np.exp(-x))\n  f=sigmoid(a)\n  print(f)      #outputs 0.1192\n  Aim\n  Our aim essentially remains the same as in previous posts viz:we have to increase the value of\n  output(f) by manipulating the values of input(a).", "start_char_idx": 844, "end_char_idx": 1312, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3a4182d7-4c57-4dfe-8486-55a51290aadb": {"__data__": {"id_": "3a4182d7-4c57-4dfe-8486-55a51290aadb", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "07945799e149dc2b1a5075bffc957b60a70311b4e70cee6bb560a62d013b47a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46e533c5-db0d-415b-992a-fd22790e8dd3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "24b14d82228b3e1da98e9c86da1ef5cb5bd03ad3f35b061536dd894bff9cb92f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "301e5a70-4573-415a-9665-0100ddab3c44", "node_type": "1", "metadata": {}, "hash": "2cf28dabf6957626133ac8a5d8c92e954d6ce60b471c1c106a74843a221d9532", "class_name": "RelatedNodeInfo"}}, "text": "The node in above figure represents a sigmoid unit.Sigmoid function is expressed as-\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                            1/74/5/24, 8:24 PM                                        Lets-Practice-Backpropagation\n                                          \u03c3(x) =         1 +  1 e \u2212x\n       If you analyse the sigmoid function,you will notice that irrespective of the input value,this\n       function gives a value between 0 and 1 as output.It is a nice property to have as it gives a\n       sense of probabilistic values of inputs.It was once the most popular activation function\n       used in neural network design.\n Where do we start?Once again let us look at our update rule:    df\n                                           a  =   a  +    h  \u2217   da\n (Here we are using total derivative(   d  ) instead of partial derivative(\u2202  ) because here our node is a\n function of only one variable i.e.a.And thats the only di\u25aference there is between the two\n notations.Rest everything remains the same.)", "start_char_idx": 1315, "end_char_idx": 2379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "301e5a70-4573-415a-9665-0100ddab3c44": {"__data__": {"id_": "301e5a70-4573-415a-9665-0100ddab3c44", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "07945799e149dc2b1a5075bffc957b60a70311b4e70cee6bb560a62d013b47a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a4182d7-4c57-4dfe-8486-55a51290aadb", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "f45e59b0c7896958d871058fd08e4ebf013234fce83bb4f3e4914c2969055f35", "class_name": "RelatedNodeInfo"}}, "text": "df\n We just have to find value of the derivative     da  . If you are aware of basic rules of calculus(Refer\n Derivetive rules) then you can easily find the derivative of sigmoid with respect to a.If you are new\n to calculus then just remember for now that derivative of sigmoid funtion is given by:\n                                   d\u03c3    = (\u03c3(a)) \u2217 (1 \u2212              \u03c3(a))\n                                   da\n Let us put this value in our update rule:\n                             a  =    a  +   h  \u2217 (\u03c3(a)) \u2217 (1 \u2212             \u03c3(a))\n Using this update rule in python:\n  import    numpy    as  np\n  def   sigmoid(x):\n             return     1/(1+np.exp(-x))\n  def   derivative_sigmoid(x):\n  a=-2       return     sigmoid(x)*(1-sigmoid(x))\n  h=.1\n  a=a+h*derivative_sigmoid(a)\n  f=sigmoid(a)\n  print(f)      #outputs 0.1203\n The above program outputs 0.1203 which is greater than 0.1192.It worked!!!", "start_char_idx": 2384, "end_char_idx": 3286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "25d53414-3058-413c-8050-666ba188b4ed": {"__data__": {"id_": "25d53414-3058-413c-8050-666ba188b4ed", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "07945799e149dc2b1a5075bffc957b60a70311b4e70cee6bb560a62d013b47a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db994ff1-f6fa-4945-a0e3-55d59534be90", "node_type": "1", "metadata": {}, "hash": "b2d30f7e147359573fcacdbcb884d74c175182ee41ea9952870ae78557c2ed07", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:24 PM                                          Lets-Practice-Backpropagation\n  Backpropagation                                                                                     HOME\n  Lets practice Backpropagation\n  In theprevious post    we went through a system of nested nodes and analysed the update rules for\n  the system.We also went through the intuitive notion of backpropagation and figured out that it is\n  nothing but applying chain rule over and over again.Initially for this post I was looking to apply\n  backpropagation to neural networks but then I felt some practice of chain rule in complex systems\n  would not hurt.So,in this post we will apply backpropogation to systems with complex functions\n  so that the reader gets comfortable with chain rule and its applications to complex systems.\n  Lets get started!!!\n  Lets start with a single node system but this time with a complex function:\n                  sigmoid unit\n                   M/(I+e(-a))\n  This system can be represented as:\n  import     numpy    as   np\n  def   sigmoid(x):\n  a=-2        return     1/(1+np.exp(-x))\n  f=sigmoid(a)\n  print(f)      #outputs 0.1192\n  Aim\n  Our aim essentially remains the same as in previous posts viz:we have to increase the value of\n  output(f) by manipulating the values of input(a).\n  The node in above figure represents a sigmoid unit.Sigmoid function is expressed as-\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                            1/74/5/24, 8:24 PM                                        Lets-Practice-Backpropagation\n                                          \u03c3(x) =         1 +  1 e \u2212x\n       If you analyse the sigmoid function,you will notice that irrespective of the input value,this\n       function gives a value between 0 and 1 as output.It is a nice property to have as it gives a\n       sense of probabilistic values of inputs.It was once the most popular activation function\n       used in neural network design.", "start_char_idx": 0, "end_char_idx": 1999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "db994ff1-f6fa-4945-a0e3-55d59534be90": {"__data__": {"id_": "db994ff1-f6fa-4945-a0e3-55d59534be90", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "07945799e149dc2b1a5075bffc957b60a70311b4e70cee6bb560a62d013b47a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25d53414-3058-413c-8050-666ba188b4ed", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "dc2ba53662b6115772abc380e987b467fee526731a2ac84e9c6d87de7fb87a35", "class_name": "RelatedNodeInfo"}}, "text": "Where do we start?Once again let us look at our update rule:    df\n                                           a  =   a  +    h  \u2217   da\n (Here we are using total derivative(   d  ) instead of partial derivative(\u2202  ) because here our node is a\n function of only one variable i.e.a.And thats the only di\u25aference there is between the two\n notations.Rest everything remains the same.)     df\n We just have to find value of the derivative     da  . If you are aware of basic rules of calculus(Refer\n Derivetive rules) then you can easily find the derivative of sigmoid with respect to a.If you are new\n to calculus then just remember for now that derivative of sigmoid funtion is given by:\n                                   d\u03c3    = (\u03c3(a)) \u2217 (1 \u2212              \u03c3(a))\n                                   da\n Let us put this value in our update rule:\n                             a  =    a  +   h  \u2217 (\u03c3(a)) \u2217 (1 \u2212             \u03c3(a))\n Using this update rule in python:\n  import    numpy    as  np\n  def   sigmoid(x):\n             return     1/(1+np.exp(-x))\n  def   derivative_sigmoid(x):\n  a=-2       return     sigmoid(x)*(1-sigmoid(x))\n  h=.1\n  a=a+h*derivative_sigmoid(a)\n  f=sigmoid(a)\n  print(f)      #outputs 0.1203\n The above program outputs 0.1203 which is greater than 0.1192.It worked!!!", "start_char_idx": 2001, "end_char_idx": 3286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-0": {"__data__": {"id_": "node-0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eae5678d-6d03-46a7-a635-9c3ecc27c593", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "2e57d86cc70685394eb12d26c807012414a1ac6ef82ae78d603afedeaab7e6ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f50e1c9-0c37-4391-97e7-4b5d766be7fa", "node_type": "1", "metadata": {}, "hash": "18a6a28d006cd486dddec2def8aa6cb95b273eb4bcaff7aac21b68c85aff2a86", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:24 PM                                          Lets-Practice-Backpropagation\n  Backpropagation                                                                                     HOME\n  Lets practice Backpropagation\n  In theprevious post    we went through a system of nested nodes and analysed the update rules for\n  the system.We also went through the intuitive notion of backpropagation and figured out that it is\n  nothing but applying chain rule over and over again.Initially for this post I was looking to apply\n  backpropagation to neural networks but then I felt some practice of chain rule in complex systems\n  would not hurt.So,in this post we will apply backpropogation to systems with complex functions\n  so that the reader gets comfortable with chain rule and its applications to complex systems.\n  Lets get started!!!\n  Lets start with a single node system but this time with a complex function:\n                  sigmoid unit\n                   M/(I+e(-a))\n  This system can be represented as:\n  import     numpy    as   np\n  def   sigmoid(x):\n  a=-2        return     1/(1+np.exp(-x))\n  f=sigmoid(a)\n  print(f)      #outputs 0.1192\n  Aim\n  Our aim essentially remains the same as in previous posts viz:we have to increase the value of\n  output(f) by manipulating the values of input(a).\n  The node in above figure represents a sigmoid unit.Sigmoid function is expressed as-\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                            1/74/5/24, 8:24 PM                                        Lets-Practice-Backpropagation\n                                          \u03c3(x) =         1 +  1 e \u2212x\n       If you analyse the sigmoid function,you will notice that irrespective of the input value,this\n       function gives a value between 0 and 1 as output.It is a nice property to have as it gives a\n       sense of probabilistic values of inputs.It was once the most popular activation function\n       used in neural network design.\n Where do we start?Once again let us look at our update rule:    df\n                                           a  =   a  +    h  \u2217   da\n (Here we are using total derivative(   d  ) instead of partial derivative(\u2202  ) because here our node is a\n function of only one variable i.e.a.And thats the only di\u25aference there is between the two\n notations.Rest everything remains the same.)     df\n We just have to find value of the derivative     da  . If you are aware of basic rules of calculus(Refer\n Derivetive rules) then you can easily find the derivative of sigmoid with respect to a.If you are new\n to calculus then just remember for now that derivative of sigmoid funtion is given by:\n                                   d\u03c3    = (\u03c3(a)) \u2217 (1 \u2212              \u03c3(a))\n                                   da\n Let us put this value in our update rule:\n                             a  =    a  +   h  \u2217 (\u03c3(a)) \u2217 (1 \u2212             \u03c3(a))\n Using this update rule in python:\n  import    numpy    as  np\n  def   sigmoid(x):\n             return     1/(1+np.exp(-x))\n  def   derivative_sigmoid(x):\n  a=-2       return     sigmoid(x)*(1-sigmoid(x))\n  h=.1\n  a=a+h*derivative_sigmoid(a)\n  f=sigmoid(a)\n  print(f)      #outputs 0.1203\n The above program outputs 0.1203 which is greater than 0.1192.It worked!!!", "start_char_idx": 0, "end_char_idx": 3286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3b0fb430-f97d-4e08-ae69-cd2b4e9aad4c": {"__data__": {"id_": "3b0fb430-f97d-4e08-ae69-cd2b4e9aad4c", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "2c5ed88ed8de35746b5390d8b1ea3e04794649edd3d10e2d7b24cef1fb9f6e08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bf2f42f-9329-45df-9f10-b0663963b4de", "node_type": "1", "metadata": {}, "hash": "abb0b0eaf40def79f3411494ffb0e43251d13065af0f1b341bbbeeeada693288", "class_name": "RelatedNodeInfo"}}, "text": "https://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                            2/74/5/24, 8:24 PM                                      Lets-Practice-Backpropagation\n  Let us take the discussion one notch above.Consider the system:sigmoid\n                                                                                             The system\n  consists of three inputsa,b   and c.The former two pass through an addition node and give output       d\n  which products with the inputc    to generatee   which is passed through sigmoid node to give final\n  outputf.Let us represent this system in python:\n  import    numpy    as  np\n  def   addition(x,y):\n              return    x+y\n  def   product(x,y):\n              return    x*y\n  def   sigmoid(x):\n  a=1         return    1/(1+np.exp(-x))\n  b=-2\n  c=-3\n  d=addition(a,b)\n  e=product(c,d)\n  f=sigmoid(e)\n  print(f)       #outputs 0.", "start_char_idx": 0, "end_char_idx": 912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "8bf2f42f-9329-45df-9f10-b0663963b4de": {"__data__": {"id_": "8bf2f42f-9329-45df-9f10-b0663963b4de", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "2c5ed88ed8de35746b5390d8b1ea3e04794649edd3d10e2d7b24cef1fb9f6e08", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b0fb430-f97d-4e08-ae69-cd2b4e9aad4c", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "a4e14c9de156b9d4d8c5898bba0b409cc3e240df3e037a145b31b720b480fa9d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6194c227-78a7-48b9-ac49-7156e8a01803", "node_type": "1", "metadata": {}, "hash": "69018ccbc4a11adb4755336d50e0000890f1008bee70bfd0822add4b8501f77b", "class_name": "RelatedNodeInfo"}}, "text": "952574\n  Our aim essentially remains the same viz:to tweak the values of input      a,b and  c in order to\n  increase the value off.Once again like our previous approaches,let us look at our update rules:\n                                          a  =   a  +    h  \u2217 \u2202a\u2202f\n                                          b  =   b  +   h  \u2217 \u2202b\u2202f\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                          3/74/5/24, 8:24 PM                               Lets-Practice-Backpropagation\n                                                       \u2202f\n                                    c  =   c +   h  \u2217 \u2202c\n                                                      \u2202f   \u2202f       \u2202f\n We have to somehow find the values of the derivatives\u2202a , \u2202b  and  \u2202c .", "start_char_idx": 912, "end_char_idx": 1693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6194c227-78a7-48b9-ac49-7156e8a01803": {"__data__": {"id_": "6194c227-78a7-48b9-ac49-7156e8a01803", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "2c5ed88ed8de35746b5390d8b1ea3e04794649edd3d10e2d7b24cef1fb9f6e08", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8bf2f42f-9329-45df-9f10-b0663963b4de", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "138b307b1f52c81fc0236994eb6166b820907ca7e10ecd12c48917c8d624067b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7434ac26-0906-4f80-8005-d246b6eaae7b", "node_type": "1", "metadata": {}, "hash": "0280c6b61dca3bb8dbb58ca6ae803a9c0c664871008e6c83a5aff046027fb308", "class_name": "RelatedNodeInfo"}}, "text": "Using chain rule described inprevious postwe can write:\n                                \u2202f        \u2202f        \u2202e      \u2202d\n                                \u2202a    =    \u2202e   \u2217  \u2202d    \u2217  \u2202a\n                                \u2202f        \u2202f        \u2202e      \u2202d\n                                \u2202b    =    \u2202e   \u2217  \u2202d    \u2217   \u2202b\n                                     \u2202f        \u2202f       \u2202e\n                                     \u2202c   =    \u2202e    \u2217  \u2202c\n      This is a good example to get an intuition about chain rule.Observe how in order to\n      compute derivatives offwith respect to various inputs,we are just travelling to those\n      inputs fromfand multiplying(chaining) the derivatives that we encounter as we reach\n      the input.\n Lets start finding the values of derivatives:\n Let us traverse the system from output to input i.e.", "start_char_idx": 1695, "end_char_idx": 2511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7434ac26-0906-4f80-8005-d246b6eaae7b": {"__data__": {"id_": "7434ac26-0906-4f80-8005-d246b6eaae7b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "2c5ed88ed8de35746b5390d8b1ea3e04794649edd3d10e2d7b24cef1fb9f6e08", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6194c227-78a7-48b9-ac49-7156e8a01803", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "860116a3f90e54a350a40e32c55e85b2c14c47bf85071968b7484891df6a75cf", "class_name": "RelatedNodeInfo"}}, "text": "backward.While crossing the sigmoid node the\n value of \u2202f  can be written easily as                           .Further while crossing the\n          \u2202e                        (\u03c3(e)) \u2217 (1 \u2212        \u03c3(e))\n product node the values of \u2202e  and  \u2202e can easily be written as and   respectively.Further\n                            \u2202d       \u2202c       \u2202e       \u2202e      c     d\n while crossing the addition node the values of   and      can be easily written asand\n                                              \u2202d       \u2202c                        1     1\n respectively.If you are having trouble in getting your head around these derivatives,I suggest you\n to have a look atfirstand thesecond post of this series.", "start_char_idx": 2512, "end_char_idx": 3209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c7dead6a-45bb-439c-8475-ac7046075af3": {"__data__": {"id_": "c7dead6a-45bb-439c-8475-ac7046075af3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "2c5ed88ed8de35746b5390d8b1ea3e04794649edd3d10e2d7b24cef1fb9f6e08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fbe3f48-f1ae-4e57-a324-68b60832bf04", "node_type": "1", "metadata": {}, "hash": "60472fb834415251658540d0ee7aaf1739c97b9501e46162b67b921aa561d6fc", "class_name": "RelatedNodeInfo"}}, "text": "https://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                            2/74/5/24, 8:24 PM                                      Lets-Practice-Backpropagation\n  Let us take the discussion one notch above.Consider the system:sigmoid\n                                                                                             The system\n  consists of three inputsa,b   and c.The former two pass through an addition node and give output       d\n  which products with the inputc    to generatee   which is passed through sigmoid node to give final\n  outputf.Let us represent this system in python:\n  import    numpy    as  np\n  def   addition(x,y):\n              return    x+y\n  def   product(x,y):\n              return    x*y\n  def   sigmoid(x):\n  a=1         return    1/(1+np.exp(-x))\n  b=-2\n  c=-3\n  d=addition(a,b)\n  e=product(c,d)\n  f=sigmoid(e)\n  print(f)       #outputs 0.952574\n  Our aim essentially remains the same viz:to tweak the values of input      a,b and  c in order to\n  increase the value off.Once again like our previous approaches,let us look at our update rules:\n                                          a  =   a  +    h  \u2217 \u2202a\u2202f\n                                          b  =   b  +   h  \u2217 \u2202b\u2202f\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                          3/74/5/24, 8:24 PM                               Lets-Practice-Backpropagation\n                                                       \u2202f\n                                    c  =   c +   h  \u2217 \u2202c\n                                                      \u2202f   \u2202f       \u2202f\n We have to somehow find the values of the derivatives\u2202a , \u2202b  and  \u2202c .", "start_char_idx": 0, "end_char_idx": 1693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3fbe3f48-f1ae-4e57-a324-68b60832bf04": {"__data__": {"id_": "3fbe3f48-f1ae-4e57-a324-68b60832bf04", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "2c5ed88ed8de35746b5390d8b1ea3e04794649edd3d10e2d7b24cef1fb9f6e08", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7dead6a-45bb-439c-8475-ac7046075af3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "e411d0a6b71a4d302f6de38703dd42fb507c63e57f0570068f1c3c93783cecde", "class_name": "RelatedNodeInfo"}}, "text": "Using chain rule described inprevious postwe can write:\n                                \u2202f        \u2202f        \u2202e      \u2202d\n                                \u2202a    =    \u2202e   \u2217  \u2202d    \u2217  \u2202a\n                                \u2202f        \u2202f        \u2202e      \u2202d\n                                \u2202b    =    \u2202e   \u2217  \u2202d    \u2217   \u2202b\n                                     \u2202f        \u2202f       \u2202e\n                                     \u2202c   =    \u2202e    \u2217  \u2202c\n      This is a good example to get an intuition about chain rule.Observe how in order to\n      compute derivatives offwith respect to various inputs,we are just travelling to those\n      inputs fromfand multiplying(chaining) the derivatives that we encounter as we reach\n      the input.\n Lets start finding the values of derivatives:\n Let us traverse the system from output to input i.e. backward.While crossing the sigmoid node the\n value of \u2202f  can be written easily as                           .Further while crossing the\n          \u2202e                        (\u03c3(e)) \u2217 (1 \u2212        \u03c3(e))\n product node the values of \u2202e  and  \u2202e can easily be written as and   respectively.Further\n                            \u2202d       \u2202c       \u2202e       \u2202e      c     d\n while crossing the addition node the values of   and      can be easily written asand\n                                              \u2202d       \u2202c                        1     1\n respectively.If you are having trouble in getting your head around these derivatives,I suggest you\n to have a look atfirstand thesecond post of this series.", "start_char_idx": 1695, "end_char_idx": 3209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-1": {"__data__": {"id_": "node-1", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eae5678d-6d03-46a7-a635-9c3ecc27c593", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "2e57d86cc70685394eb12d26c807012414a1ac6ef82ae78d603afedeaab7e6ac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "687b6fef-d572-4940-a3d2-516479bbd9e5", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "07945799e149dc2b1a5075bffc957b60a70311b4e70cee6bb560a62d013b47a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "816cd53b-1b50-453c-995c-ecf34f031cca", "node_type": "1", "metadata": {}, "hash": "20fb3c1a984a301b343ed4b0dee5755a6bec1a1e14de05b08a951b921aa79693", "class_name": "RelatedNodeInfo"}}, "text": "https://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                            2/74/5/24, 8:24 PM                                      Lets-Practice-Backpropagation\n  Let us take the discussion one notch above.Consider the system:sigmoid\n                                                                                             The system\n  consists of three inputsa,b   and c.The former two pass through an addition node and give output       d\n  which products with the inputc    to generatee   which is passed through sigmoid node to give final\n  outputf.Let us represent this system in python:\n  import    numpy    as  np\n  def   addition(x,y):\n              return    x+y\n  def   product(x,y):\n              return    x*y\n  def   sigmoid(x):\n  a=1         return    1/(1+np.exp(-x))\n  b=-2\n  c=-3\n  d=addition(a,b)\n  e=product(c,d)\n  f=sigmoid(e)\n  print(f)       #outputs 0.952574\n  Our aim essentially remains the same viz:to tweak the values of input      a,b and  c in order to\n  increase the value off.Once again like our previous approaches,let us look at our update rules:\n                                          a  =   a  +    h  \u2217 \u2202a\u2202f\n                                          b  =   b  +   h  \u2217 \u2202b\u2202f\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                          3/74/5/24, 8:24 PM                               Lets-Practice-Backpropagation\n                                                       \u2202f\n                                    c  =   c +   h  \u2217 \u2202c\n                                                      \u2202f   \u2202f       \u2202f\n We have to somehow find the values of the derivatives\u2202a , \u2202b  and  \u2202c .\n Using chain rule described inprevious postwe can write:\n                                \u2202f        \u2202f        \u2202e      \u2202d\n                                \u2202a    =    \u2202e   \u2217  \u2202d    \u2217  \u2202a\n                                \u2202f        \u2202f        \u2202e      \u2202d\n                                \u2202b    =    \u2202e   \u2217  \u2202d    \u2217   \u2202b\n                                     \u2202f        \u2202f       \u2202e\n                                     \u2202c   =    \u2202e    \u2217  \u2202c\n      This is a good example to get an intuition about chain rule.Observe how in order to\n      compute derivatives offwith respect to various inputs,we are just travelling to those\n      inputs fromfand multiplying(chaining) the derivatives that we encounter as we reach\n      the input.\n Lets start finding the values of derivatives:\n Let us traverse the system from output to input i.e. backward.While crossing the sigmoid node the\n value of \u2202f  can be written easily as                           .Further while crossing the\n          \u2202e                        (\u03c3(e)) \u2217 (1 \u2212        \u03c3(e))\n product node the values of \u2202e  and  \u2202e can easily be written as and   respectively.Further\n                            \u2202d       \u2202c       \u2202e       \u2202e      c     d\n while crossing the addition node the values of   and      can be easily written asand\n                                              \u2202d       \u2202c                        1     1\n respectively.If you are having trouble in getting your head around these derivatives,I suggest you\n to have a look atfirstand thesecond post of this series.", "start_char_idx": 3287, "end_char_idx": 6496, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "851d5866-5ecb-4159-82b2-275927950a44": {"__data__": {"id_": "851d5866-5ecb-4159-82b2-275927950a44", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "0adf23bbc70aa5f6ca55730e6efadb81279e8d615f4502323a05b6f9661a706e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2504b511-6b3e-48b1-8998-d2ecd49ca3dd", "node_type": "1", "metadata": {}, "hash": "0881f0dbacfc9480e55e85628f675d132844669243f1687f7c55e610f0297e7e", "class_name": "RelatedNodeInfo"}}, "text": "Writing our update rules we get:\n                         \u2202f\n                         \u2202a   = (\u03c3(e)) \u2217 (1 \u2212          \u03c3(e)) \u2217    c \u2217 1\n                         \u2202f   = (\u03c3(e)) \u2217 (1 \u2212          \u03c3(e)) \u2217    c \u2217 1\n                         \u2202b\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                               4/74/5/24, 8:24 PM                                   Lets-Practice-Backpropagation\n                             \u2202f    = (\u03c3(e)) \u2217 (1 \u2212           \u03c3(e)) \u2217     d\n                             \u2202c\n  Let us represent this in python:\n  import   numpy   as  np\n  def  addition(x,y):\n             return   x+y\n  def  product(x,y):\n             return   x*y\n  def  sigmoid(x):\n             return   1/(1+np.", "start_char_idx": 0, "end_char_idx": 723, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2504b511-6b3e-48b1-8998-d2ecd49ca3dd": {"__data__": {"id_": "2504b511-6b3e-48b1-8998-d2ecd49ca3dd", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "0adf23bbc70aa5f6ca55730e6efadb81279e8d615f4502323a05b6f9661a706e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "851d5866-5ecb-4159-82b2-275927950a44", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "70d09a07ce0db5ade6bf821f0ab63c47f243c0636bd6e2cb9dba9e4ed4b940c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a24bce56-6460-41c8-b4b0-c4cce10e8263", "node_type": "1", "metadata": {}, "hash": "0972871df24c7a3fb048e15f231d9ff0c75b0e94697f93e217b1ac801e885b37", "class_name": "RelatedNodeInfo"}}, "text": "exp(-x))\n  def  derivative_sigmoid(x):\n             return   sigmoid(x)*(1-sigmoid(x))\n  #initialization\n  a=1\n  b=-2\n  c=-3\n  #forward-propogation\n  d=addition(a,b)\n  e=product(c,d)\n  #step size\n  h=0.", "start_char_idx": 723, "end_char_idx": 925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a24bce56-6460-41c8-b4b0-c4cce10e8263": {"__data__": {"id_": "a24bce56-6460-41c8-b4b0-c4cce10e8263", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "0adf23bbc70aa5f6ca55730e6efadb81279e8d615f4502323a05b6f9661a706e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2504b511-6b3e-48b1-8998-d2ecd49ca3dd", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "230d796dbb75e6807c5c486a7d418a4bd799c4c990ea69e391c41d5dee25f5ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b77a78c8-a4e2-4be1-a6d8-b3b869fc7202", "node_type": "1", "metadata": {}, "hash": "0f1fdf8cfd12761d97399c5627119fc788f0232fafc5fd352a3a4cf8c3ed6ba4", "class_name": "RelatedNodeInfo"}}, "text": "1\n  #derivatives\n  derivative_f_wrt_e=derivative_sigmoid(e)\n  derivative_e_wrt_d=c\n  derivative_e_wrt_c=d\n  derivative_d_wrt_a=1\n  derivative_d_wrt_b=1\n  #backward-propogation (Chain rule)\n  derivative_f_wrt_a=derivative_f_wrt_e*derivative_e_wrt_d*derivative_d_wrt_a\n  derivative_f_wrt_b=derivative_f_wrt_e*derivative_e_wrt_d*derivative_d_wrt_b\n  derivative_f_wrt_c=derivative_f_wrt_e*derivative_e_wrt_c\n  #update-parameters\n  a=a+h*derivative_f_wrt_a\n  b=b+h*derivative_f_wrt_b\n  c=c+h*derivative_f_wrt_c\n  d=addition(a,b)\n  e=product(c,d)\n  f=sigmoid(e)\n  print(f)     #prints 0.9563\n  The output of above program is 0.9563 which is greater than 0.9525.", "start_char_idx": 925, "end_char_idx": 1580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b77a78c8-a4e2-4be1-a6d8-b3b869fc7202": {"__data__": {"id_": "b77a78c8-a4e2-4be1-a6d8-b3b869fc7202", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "0adf23bbc70aa5f6ca55730e6efadb81279e8d615f4502323a05b6f9661a706e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a24bce56-6460-41c8-b4b0-c4cce10e8263", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "5017b7d19fd19763993e6e77030e15ad71f10f26573b3bfb384187be90af433b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a13a7d5-d86e-4480-8285-5f0834347dd2", "node_type": "1", "metadata": {}, "hash": "f7ede1d946228fca0ef7c88b2e195671dfcfac7982753c4ef9287f23b7b57cdf", "class_name": "RelatedNodeInfo"}}, "text": "Similarly you can backpropagate through any complex system containing complex functions to\n  generate update rules and thus manipulate the value of output by applying those update rules.\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                 5/74/5/24, 8:24 PM                                           Lets-Practice-Backpropagation\n  In the next post we will look to apply backpropagation to neural networks.We will start by a simple\n  two layered network and then extend our discussion to three-layered network.", "start_char_idx": 1583, "end_char_idx": 2138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7a13a7d5-d86e-4480-8285-5f0834347dd2": {"__data__": {"id_": "7a13a7d5-d86e-4480-8285-5f0834347dd2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "0adf23bbc70aa5f6ca55730e6efadb81279e8d615f4502323a05b6f9661a706e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b77a78c8-a4e2-4be1-a6d8-b3b869fc7202", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "f990e38f8bd2a9f0d20945315bd7447dca17c953ddfce62056774e4732ea5924", "class_name": "RelatedNodeInfo"}}, "text": "ALSO ON JASDEEP06                        Posted on 16 January,2017\n    Getting started with          Understanding LSTMin          Variable-sharing-=in-         Further-into-\n    Getting started with\n    Getting started with          Understanding LSTM in\n                                  Understanding LSTM in         Variable-sharing-in-\n                                                                Variable-sharing-in-          Further-into-\n                                                                                              Further-into-\n    Tensorflow                    Tensorflow                    Tensorflow                    backpropagation\n    Tensor\u25afow\n    Tensor\u25afow                     Tensor\u25afow\n                                  Tensor\u25afow                     Tensor\u25afow\n                                                                Tensor\u25afow                     backpropagation\n                                                                                              backpropagation\n              \u2022                             \u2022                             \u2022                             \u2022\n    7 years ago3 comments         7 years ago32 comments        7 years ago14 comments        7 years ago2 comments\n    Tensorflow : Getting Started  CNNs in Tensorflow(cifar-     Tensorflow: Variable sharing  Backpropagation : Further\n    with Tensorflow               10)                           in Tensorflow                 into Backpropagation\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                                       6/7", "start_char_idx": 2142, "end_char_idx": 3740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f31be08f-8908-4844-9b27-ff048ddd2dc1": {"__data__": {"id_": "f31be08f-8908-4844-9b27-ff048ddd2dc1", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "0adf23bbc70aa5f6ca55730e6efadb81279e8d615f4502323a05b6f9661a706e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8e4ade3-1e8f-47ca-a25e-8c7be29eccc6", "node_type": "1", "metadata": {}, "hash": "bd115f7d8927f8baecf947ae120c89424710ecbd7dfd2fb62908b36937ae55d2", "class_name": "RelatedNodeInfo"}}, "text": "Writing our update rules we get:\n                         \u2202f\n                         \u2202a   = (\u03c3(e)) \u2217 (1 \u2212          \u03c3(e)) \u2217    c \u2217 1\n                         \u2202f   = (\u03c3(e)) \u2217 (1 \u2212          \u03c3(e)) \u2217    c \u2217 1\n                         \u2202b\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                               4/74/5/24, 8:24 PM                                   Lets-Practice-Backpropagation\n                             \u2202f    = (\u03c3(e)) \u2217 (1 \u2212           \u03c3(e)) \u2217     d\n                             \u2202c\n  Let us represent this in python:\n  import   numpy   as  np\n  def  addition(x,y):\n             return   x+y\n  def  product(x,y):\n             return   x*y\n  def  sigmoid(x):\n             return   1/(1+np.exp(-x))\n  def  derivative_sigmoid(x):\n             return   sigmoid(x)*(1-sigmoid(x))\n  #initialization\n  a=1\n  b=-2\n  c=-3\n  #forward-propogation\n  d=addition(a,b)\n  e=product(c,d)\n  #step size\n  h=0.1\n  #derivatives\n  derivative_f_wrt_e=derivative_sigmoid(e)\n  derivative_e_wrt_d=c\n  derivative_e_wrt_c=d\n  derivative_d_wrt_a=1\n  derivative_d_wrt_b=1\n  #backward-propogation (Chain rule)\n  derivative_f_wrt_a=derivative_f_wrt_e*derivative_e_wrt_d*derivative_d_wrt_a\n  derivative_f_wrt_b=derivative_f_wrt_e*derivative_e_wrt_d*derivative_d_wrt_b\n  derivative_f_wrt_c=derivative_f_wrt_e*derivative_e_wrt_c\n  #update-parameters\n  a=a+h*derivative_f_wrt_a\n  b=b+h*derivative_f_wrt_b\n  c=c+h*derivative_f_wrt_c\n  d=addition(a,b)\n  e=product(c,d)\n  f=sigmoid(e)\n  print(f)     #prints 0.", "start_char_idx": 0, "end_char_idx": 1506, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b8e4ade3-1e8f-47ca-a25e-8c7be29eccc6": {"__data__": {"id_": "b8e4ade3-1e8f-47ca-a25e-8c7be29eccc6", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "0adf23bbc70aa5f6ca55730e6efadb81279e8d615f4502323a05b6f9661a706e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f31be08f-8908-4844-9b27-ff048ddd2dc1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "4251ea112c231c18910e5fac9cc48f4f0e2dbe38bfc6824f758acf2b09cb5c92", "class_name": "RelatedNodeInfo"}}, "text": "9563\n  The output of above program is 0.9563 which is greater than 0.9525.\n  Similarly you can backpropagate through any complex system containing complex functions to\n  generate update rules and thus manipulate the value of output by applying those update rules.\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                 5/74/5/24, 8:24 PM                                           Lets-Practice-Backpropagation\n  In the next post we will look to apply backpropagation to neural networks.We will start by a simple\n  two layered network and then extend our discussion to three-layered network.\n   ALSO ON JASDEEP06                        Posted on 16 January,2017\n    Getting started with          Understanding LSTMin          Variable-sharing-=in-         Further-into-\n    Getting started with\n    Getting started with          Understanding LSTM in\n                                  Understanding LSTM in         Variable-sharing-in-\n                                                                Variable-sharing-in-          Further-into-\n                                                                                              Further-into-\n    Tensorflow                    Tensorflow                    Tensorflow                    backpropagation\n    Tensor\u25afow\n    Tensor\u25afow                     Tensor\u25afow\n                                  Tensor\u25afow                     Tensor\u25afow\n                                                                Tensor\u25afow                     backpropagation\n                                                                                              backpropagation\n              \u2022                             \u2022                             \u2022                             \u2022\n    7 years ago3 comments         7 years ago32 comments        7 years ago14 comments        7 years ago2 comments\n    Tensorflow : Getting Started  CNNs in Tensorflow(cifar-     Tensorflow: Variable sharing  Backpropagation : Further\n    with Tensorflow               10)                           in Tensorflow                 into Backpropagation\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                                       6/7", "start_char_idx": 1506, "end_char_idx": 3740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-2": {"__data__": {"id_": "node-2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eae5678d-6d03-46a7-a635-9c3ecc27c593", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "2e57d86cc70685394eb12d26c807012414a1ac6ef82ae78d603afedeaab7e6ac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f50e1c9-0c37-4391-97e7-4b5d766be7fa", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}, "hash": "2c5ed88ed8de35746b5390d8b1ea3e04794649edd3d10e2d7b24cef1fb9f6e08", "class_name": "RelatedNodeInfo"}}, "text": "Writing our update rules we get:\n                         \u2202f\n                         \u2202a   = (\u03c3(e)) \u2217 (1 \u2212          \u03c3(e)) \u2217    c \u2217 1\n                         \u2202f   = (\u03c3(e)) \u2217 (1 \u2212          \u03c3(e)) \u2217    c \u2217 1\n                         \u2202b\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                               4/74/5/24, 8:24 PM                                   Lets-Practice-Backpropagation\n                             \u2202f    = (\u03c3(e)) \u2217 (1 \u2212           \u03c3(e)) \u2217     d\n                             \u2202c\n  Let us represent this in python:\n  import   numpy   as  np\n  def  addition(x,y):\n             return   x+y\n  def  product(x,y):\n             return   x*y\n  def  sigmoid(x):\n             return   1/(1+np.exp(-x))\n  def  derivative_sigmoid(x):\n             return   sigmoid(x)*(1-sigmoid(x))\n  #initialization\n  a=1\n  b=-2\n  c=-3\n  #forward-propogation\n  d=addition(a,b)\n  e=product(c,d)\n  #step size\n  h=0.1\n  #derivatives\n  derivative_f_wrt_e=derivative_sigmoid(e)\n  derivative_e_wrt_d=c\n  derivative_e_wrt_c=d\n  derivative_d_wrt_a=1\n  derivative_d_wrt_b=1\n  #backward-propogation (Chain rule)\n  derivative_f_wrt_a=derivative_f_wrt_e*derivative_e_wrt_d*derivative_d_wrt_a\n  derivative_f_wrt_b=derivative_f_wrt_e*derivative_e_wrt_d*derivative_d_wrt_b\n  derivative_f_wrt_c=derivative_f_wrt_e*derivative_e_wrt_c\n  #update-parameters\n  a=a+h*derivative_f_wrt_a\n  b=b+h*derivative_f_wrt_b\n  c=c+h*derivative_f_wrt_c\n  d=addition(a,b)\n  e=product(c,d)\n  f=sigmoid(e)\n  print(f)     #prints 0.9563\n  The output of above program is 0.9563 which is greater than 0.9525.\n  Similarly you can backpropagate through any complex system containing complex functions to\n  generate update rules and thus manipulate the value of output by applying those update rules.\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                 5/74/5/24, 8:24 PM                                           Lets-Practice-Backpropagation\n  In the next post we will look to apply backpropagation to neural networks.We will start by a simple\n  two layered network and then extend our discussion to three-layered network.\n   ALSO ON JASDEEP06                        Posted on 16 January,2017\n    Getting started with          Understanding LSTMin          Variable-sharing-=in-         Further-into-\n    Getting started with\n    Getting started with          Understanding LSTM in\n                                  Understanding LSTM in         Variable-sharing-in-\n                                                                Variable-sharing-in-          Further-into-\n                                                                                              Further-into-\n    Tensorflow                    Tensorflow                    Tensorflow                    backpropagation\n    Tensor\u25afow\n    Tensor\u25afow                     Tensor\u25afow\n                                  Tensor\u25afow                     Tensor\u25afow\n                                                                Tensor\u25afow                     backpropagation\n                                                                                              backpropagation\n              \u2022                             \u2022                             \u2022                             \u2022\n    7 years ago3 comments         7 years ago32 comments        7 years ago14 comments        7 years ago2 comments\n    Tensorflow : Getting Started  CNNs in Tensorflow(cifar-     Tensorflow: Variable sharing  Backpropagation : Further\n    with Tensorflow               10)                           in Tensorflow                 into Backpropagation\nhttps://jasdeep06.github.io/posts/Lets-practice-backpropagation/                                                       6/7", "start_char_idx": 6498, "end_char_idx": 10238, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "90dd81b3-016a-4a4d-9d78-03bca784041a": {"__data__": {"id_": "90dd81b3-016a-4a4d-9d78-03bca784041a", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Lets Practice Backpropagation", "path": "cache\\blogposts\\Lets-Practice-Backpropagation\\parsed\\images\\80d30ef4-4121-417a-9aba-e232f5a1f6f8-img_p0_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The subject at hand is a single node system that utilizes a sigmoid function, which is a type of activation function commonly used in neural networks. The sigmoid function is mathematically represented as \\( \\frac{1}{1+e^{-a}} \\), where 'a' is the input to the function. The output of this function is denoted as 'f'. The goal is to adjust the input 'a' to maximize the output 'f'.\n\nIn the context of backpropagation, this system is used to illustrate how the chain rule can be applied to compute the gradient of 'f' with respect to 'a'. This is a fundamental step in training neural networks, where the gradients are used to update the weights of the network to minimize a loss function.\n\nThe sigmoid function is particularly interesting because it maps any input value into a range between 0 and 1, making it useful for binary classification tasks where the output can be interpreted as a probability. The function is S-shaped, which is where it gets its name.\n\nIn practice, the sigmoid function can be implemented in Python using the NumPy library, as shown in the provided code snippet. The code defines a sigmoid function, applies it to an input value 'a', and prints the result 'f'. The specific value of 'a' used in the code is -2, and the output 'f' for this input is approximately 0.1192.\n\nUnderstanding and practicing the application of the chain rule in such systems is", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78dc0d36-cc58-4227-aeec-67250121b25e": {"__data__": {"id_": "78dc0d36-cc58-4227-aeec-67250121b25e", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Lets Practice Backpropagation", "path": "cache\\blogposts\\Lets-Practice-Backpropagation\\parsed\\images\\80d30ef4-4121-417a-9aba-e232f5a1f6f8-img_p2_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The system being discussed is a simple computational graph used to illustrate the concept of backpropagation in neural networks. It involves a sequence of operations starting with three inputs labeled as 'a', 'b', and 'c'. The inputs 'a' and 'b' are first combined using an addition operation, resulting in an intermediate output 'd'. This output 'd' is then multiplied with the third input 'c' to produce another intermediate value 'e'. Finally, the value 'e' is passed through a sigmoid function, which is a common activation function in neural networks, to produce the final output 'f'.\n\nThe Python code provided defines functions for the addition, product, and sigmoid operations, and then applies these operations to specific values for 'a', 'b', and 'c'. The result of the computation is printed out, showing that the final output 'f' is approximately 0.952574.\n\nThe discussion also touches upon the goal of adjusting the input values 'a', 'b', and 'c' to maximize the final output 'f'. This is where the concept of backpropagation comes into play, as it provides a method to calculate the gradients of 'f' with respect to each input. These gradients are then used to update the input values in the direction that increases 'f'. The update rules provided are mathematical expressions that represent how each input should be adjusted, with 'h' being a small step size and the partial derivatives representing the sensitivity of 'f'", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"0485c35d-5dd7-4a75-bfb9-a02b8e2f9755": {"doc_hash": "83d9a1cdff31ac29aecc132a9784f77c987cb809066f09e2d62eb33bad5e5c41", "ref_doc_id": "node-0"}, "46e533c5-db0d-415b-992a-fd22790e8dd3": {"doc_hash": "24b14d82228b3e1da98e9c86da1ef5cb5bd03ad3f35b061536dd894bff9cb92f", "ref_doc_id": "node-0"}, "3a4182d7-4c57-4dfe-8486-55a51290aadb": {"doc_hash": "f45e59b0c7896958d871058fd08e4ebf013234fce83bb4f3e4914c2969055f35", "ref_doc_id": "node-0"}, "301e5a70-4573-415a-9665-0100ddab3c44": {"doc_hash": "f253bbab7f9ac4669c93a115da3b56435cf1ae2fc761bd69f532b44c15252607", "ref_doc_id": "node-0"}, "25d53414-3058-413c-8050-666ba188b4ed": {"doc_hash": "dc2ba53662b6115772abc380e987b467fee526731a2ac84e9c6d87de7fb87a35", "ref_doc_id": "node-0"}, "db994ff1-f6fa-4945-a0e3-55d59534be90": {"doc_hash": "ed32cf56a076c004042d59001b4cffbb5e783f2bcb4d5e7e3bf5e7d965043414", "ref_doc_id": "node-0"}, "node-0": {"doc_hash": "07945799e149dc2b1a5075bffc957b60a70311b4e70cee6bb560a62d013b47a0", "ref_doc_id": "eae5678d-6d03-46a7-a635-9c3ecc27c593"}, "3b0fb430-f97d-4e08-ae69-cd2b4e9aad4c": {"doc_hash": "a4e14c9de156b9d4d8c5898bba0b409cc3e240df3e037a145b31b720b480fa9d", "ref_doc_id": "node-1"}, "8bf2f42f-9329-45df-9f10-b0663963b4de": {"doc_hash": "138b307b1f52c81fc0236994eb6166b820907ca7e10ecd12c48917c8d624067b", "ref_doc_id": "node-1"}, "6194c227-78a7-48b9-ac49-7156e8a01803": {"doc_hash": "860116a3f90e54a350a40e32c55e85b2c14c47bf85071968b7484891df6a75cf", "ref_doc_id": "node-1"}, "7434ac26-0906-4f80-8005-d246b6eaae7b": {"doc_hash": "d74ea5d14d192dbe371b4fd1101a69645e8b34ddac43824a29a381d3a8385045", "ref_doc_id": "node-1"}, "c7dead6a-45bb-439c-8475-ac7046075af3": {"doc_hash": "e411d0a6b71a4d302f6de38703dd42fb507c63e57f0570068f1c3c93783cecde", "ref_doc_id": "node-1"}, "3fbe3f48-f1ae-4e57-a324-68b60832bf04": {"doc_hash": "37942969a2a4ee57901db3e8ff5e6a77b01a371a869def9dad16d584fbda7f97", "ref_doc_id": "node-1"}, "node-1": {"doc_hash": "2c5ed88ed8de35746b5390d8b1ea3e04794649edd3d10e2d7b24cef1fb9f6e08", "ref_doc_id": "eae5678d-6d03-46a7-a635-9c3ecc27c593"}, "851d5866-5ecb-4159-82b2-275927950a44": {"doc_hash": "70d09a07ce0db5ade6bf821f0ab63c47f243c0636bd6e2cb9dba9e4ed4b940c2", "ref_doc_id": "node-2"}, "2504b511-6b3e-48b1-8998-d2ecd49ca3dd": {"doc_hash": "230d796dbb75e6807c5c486a7d418a4bd799c4c990ea69e391c41d5dee25f5ea", "ref_doc_id": "node-2"}, "a24bce56-6460-41c8-b4b0-c4cce10e8263": {"doc_hash": "5017b7d19fd19763993e6e77030e15ad71f10f26573b3bfb384187be90af433b", "ref_doc_id": "node-2"}, "b77a78c8-a4e2-4be1-a6d8-b3b869fc7202": {"doc_hash": "f990e38f8bd2a9f0d20945315bd7447dca17c953ddfce62056774e4732ea5924", "ref_doc_id": "node-2"}, "7a13a7d5-d86e-4480-8285-5f0834347dd2": {"doc_hash": "69498badb575f6862dfae3593f620e489cda8a17480856669e51278133969abf", "ref_doc_id": "node-2"}, "f31be08f-8908-4844-9b27-ff048ddd2dc1": {"doc_hash": "4251ea112c231c18910e5fac9cc48f4f0e2dbe38bfc6824f758acf2b09cb5c92", "ref_doc_id": "node-2"}, "b8e4ade3-1e8f-47ca-a25e-8c7be29eccc6": {"doc_hash": "624928afcab3f846d6a42e6d53bdf7e7edc5bea7c3f458c7f5d2e33345e6cf42", "ref_doc_id": "node-2"}, "node-2": {"doc_hash": "0adf23bbc70aa5f6ca55730e6efadb81279e8d615f4502323a05b6f9661a706e", "ref_doc_id": "eae5678d-6d03-46a7-a635-9c3ecc27c593"}, "90dd81b3-016a-4a4d-9d78-03bca784041a": {"doc_hash": "4d7c59e4c02a5c65257b2d52e19596c9cc3c76725ae677495782dab04b4d23ca"}, "78dc0d36-cc58-4227-aeec-67250121b25e": {"doc_hash": "65775ab290d6b393a00c2de5a0e4f028ca8c2adb2f466edde90d8f0bc31cf18a"}}, "docstore/ref_doc_info": {"node-0": {"node_ids": ["0485c35d-5dd7-4a75-bfb9-a02b8e2f9755", "46e533c5-db0d-415b-992a-fd22790e8dd3", "3a4182d7-4c57-4dfe-8486-55a51290aadb", "301e5a70-4573-415a-9665-0100ddab3c44", "25d53414-3058-413c-8050-666ba188b4ed", "db994ff1-f6fa-4945-a0e3-55d59534be90"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}}, "eae5678d-6d03-46a7-a635-9c3ecc27c593": {"node_ids": ["node-0", "node-1", "node-2"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}}, "node-1": {"node_ids": ["3b0fb430-f97d-4e08-ae69-cd2b4e9aad4c", "8bf2f42f-9329-45df-9f10-b0663963b4de", "6194c227-78a7-48b9-ac49-7156e8a01803", "7434ac26-0906-4f80-8005-d246b6eaae7b", "c7dead6a-45bb-439c-8475-ac7046075af3", "3fbe3f48-f1ae-4e57-a324-68b60832bf04"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}}, "node-2": {"node_ids": ["851d5866-5ecb-4159-82b2-275927950a44", "2504b511-6b3e-48b1-8998-d2ecd49ca3dd", "a24bce56-6460-41c8-b4b0-c4cce10e8263", "b77a78c8-a4e2-4be1-a6d8-b3b869fc7202", "7a13a7d5-d86e-4480-8285-5f0834347dd2", "f31be08f-8908-4844-9b27-ff048ddd2dc1", "b8e4ade3-1e8f-47ca-a25e-8c7be29eccc6"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Lets Practice Backpropagation"}}}}