{"docstore/data": {"21c10818-190a-4f44-8b72-57d60e7a4848": {"__data__": {"id_": "21c10818-190a-4f44-8b72-57d60e7a4848", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "0954b0c789c1cfcc3c326a568e402b163f195797f14739cb62e39ad029fc0414", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30b203de-aac4-48dd-9bac-387073c4f3e9", "node_type": "1", "metadata": {}, "hash": "c4697df0c7525b2fa6d40a447fbcccc1b8873bc8757bfbc2831de4e9897b0fd1", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:25 PM                                                                    Further-into-backpropagation\n  Backpropagation                                                                                                                                         HOME\n  Further into Backpropagation\n  In the  previous postwe applied chain rule(funkily called backpropagation!) to systems with complex functions.In this post we will\n  apply backpropagation to neural networks.In this post we will apply backprorpagation to a two layered neural network.Note that\n  instead of derivation of mathematical formulaes we will focus on intuitive sense of it.We will look to explore implementation of\n  backpropagation with multiple inputs represented in form of matrix of training data.\n  Lets get started!!!", "start_char_idx": 0, "end_char_idx": 803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "30b203de-aac4-48dd-9bac-387073c4f3e9": {"__data__": {"id_": "30b203de-aac4-48dd-9bac-387073c4f3e9", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "0954b0c789c1cfcc3c326a568e402b163f195797f14739cb62e39ad029fc0414", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21c10818-190a-4f44-8b72-57d60e7a4848", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "3c929206a355253e76165f345cd07bd7881ea6431e03f204851ee39405ae1eae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ded9423-6975-4869-b9c9-dadad69ee987", "node_type": "1", "metadata": {}, "hash": "277099325e1460a88d8c41ebb044140ef8644390339adf6f342ddf8ce26d784d", "class_name": "RelatedNodeInfo"}}, "text": "Consider the network:\n                                                                                                        sigmoid\n                                                                                                                                      Cost\n                                                                                                        sigmoid                    Function\n  The figure consists of a two layered network.The first layer is the input layer and contains 3 nodes.The second layer is output layer and\n  contains two nodes.In a standard neural network,the sigmoid layer is a part of output layer.For clarity of concept I have drawn it as a\n  separate layer.The sigmoid layer quashes the output values in the range of 0 and 1.The two layers are connected to each other with\n  weights which are represented by edges in the figure.Each node of first layer is connected to every node of second layer.", "start_char_idx": 806, "end_char_idx": 1753, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6ded9423-6975-4869-b9c9-dadad69ee987": {"__data__": {"id_": "6ded9423-6975-4869-b9c9-dadad69ee987", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "0954b0c789c1cfcc3c326a568e402b163f195797f14739cb62e39ad029fc0414", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30b203de-aac4-48dd-9bac-387073c4f3e9", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "7e52512aa311d825e33bd81cdd9c82758e6de97014cc8b656b2ed5c189c38b24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f89de763-2c31-4a29-873e-f5f38be7e3be", "node_type": "1", "metadata": {}, "hash": "1b79b15fe9c4c7ab7a35bf4d71386d707d612adc4a2e4e0329b5bc01ed3d28b8", "class_name": "RelatedNodeInfo"}}, "text": "For those who don\u02bct know how neural networks work here is a short description(Note that this is just a very simple and crude\n  explaination and is su\u25aficient for this post.However,to appriciate the exact mechanism behind it consult                         other  resources too):\n  The input to the network is pumped through input layer.Here our inputs would be 3 dimensional as there are three nodes in our input\n  layer.These dimensions are also referred to as features.The inputs are multiplied with randomly initialized weights usually in form of\n  matrix.The output of this matrix multiplication is subjected to a sigmoid function.The sigmoidal outputs are used to generate cost\n  function.A cost function is a function that is a measure of deviation of our output from the actual value during the training of\n  network.For this post we will use        cross-entropy     cost function.The nodes in output layer of our network represent di\u25aferent classes to\n  which the input has to be classified.During the training of network we have a label corresponding to every input.This label represents\n  the true class of that input.", "start_char_idx": 1756, "end_char_idx": 2883, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f89de763-2c31-4a29-873e-f5f38be7e3be": {"__data__": {"id_": "f89de763-2c31-4a29-873e-f5f38be7e3be", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "0954b0c789c1cfcc3c326a568e402b163f195797f14739cb62e39ad029fc0414", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ded9423-6975-4869-b9c9-dadad69ee987", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "2d87907044ff99156c539a372d99da40d3336c3ab3453347ab1d0d171202b026", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16f135a9-0e8c-492c-b42e-bf03b88dbbbf", "node_type": "1", "metadata": {}, "hash": "991b7dfc0ebc82e25fcb4599f8ede93df509558976e1ce3bb7068a882d74fb71", "class_name": "RelatedNodeInfo"}}, "text": "Aim\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                                                                 1/74/5/24, 8:25 PM                                                    Further-into-backpropagation\n Our aim would be to adjust the values of weights such that our cost function is minimum(i.e. the deviation of our output from actual\n value is minimum).\n So where do we start?If you are following the posts in series then you would know the answer to this!Thats right!Our update rules.Here\n we have to manipulate the values of weights to decrease the value of our cost thereby minimizing it.Thus updating weights to decrease\n the cost:                                         W     =    W     \u2212   h  \u2217    \u2202C\n                                                      ij         ij            \u2202Wij\n where  W   ijdenotes weight connecting   i th node in input layer tojth  node in output layer.\n We have to somehow find the value of     \u2202C    and fill it in our update rule which would decrease the cost function(due to the minus\n sign).", "start_char_idx": 2886, "end_char_idx": 3979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "16f135a9-0e8c-492c-b42e-bf03b88dbbbf": {"__data__": {"id_": "16f135a9-0e8c-492c-b42e-bf03b88dbbbf", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "0954b0c789c1cfcc3c326a568e402b163f195797f14739cb62e39ad029fc0414", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f89de763-2c31-4a29-873e-f5f38be7e3be", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "830d09cac0271ee9f2040c8ce300596261825309a22b1f4dc88bf07b4831804c", "class_name": "RelatedNodeInfo"}}, "text": "\u2202Wij\n Now that we know what we desire,lets analyse our cost function   C  .To do this we must first forward propagate through our network to\n analyse the dependencies of our cost function.Here we would take only one training example which would enable us to appriciate the\n intricacies better and from there we would extend this to multiple training examples.", "start_char_idx": 4013, "end_char_idx": 4372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "80b9fe56-26ad-40d0-bc43-a8e394a7c4fd": {"__data__": {"id_": "80b9fe56-26ad-40d0-bc43-a8e394a7c4fd", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "0954b0c789c1cfcc3c326a568e402b163f195797f14739cb62e39ad029fc0414", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7601ec8b-65f7-4cdb-885a-18115363d231", "node_type": "1", "metadata": {}, "hash": "b7512f75bc426d98d89c12aa98f6b2a3b961aaeae410cc6cdba587b1e7dadd4c", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:25 PM                                                                    Further-into-backpropagation\n  Backpropagation                                                                                                                                         HOME\n  Further into Backpropagation\n  In the  previous postwe applied chain rule(funkily called backpropagation!) to systems with complex functions.In this post we will\n  apply backpropagation to neural networks.In this post we will apply backprorpagation to a two layered neural network.Note that\n  instead of derivation of mathematical formulaes we will focus on intuitive sense of it.We will look to explore implementation of\n  backpropagation with multiple inputs represented in form of matrix of training data.\n  Lets get started!!!\n  Consider the network:\n                                                                                                        sigmoid\n                                                                                                                                      Cost\n                                                                                                        sigmoid                    Function\n  The figure consists of a two layered network.The first layer is the input layer and contains 3 nodes.The second layer is output layer and\n  contains two nodes.In a standard neural network,the sigmoid layer is a part of output layer.For clarity of concept I have drawn it as a\n  separate layer.The sigmoid layer quashes the output values in the range of 0 and 1.The two layers are connected to each other with\n  weights which are represented by edges in the figure.Each node of first layer is connected to every node of second layer.", "start_char_idx": 0, "end_char_idx": 1753, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7601ec8b-65f7-4cdb-885a-18115363d231": {"__data__": {"id_": "7601ec8b-65f7-4cdb-885a-18115363d231", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "0954b0c789c1cfcc3c326a568e402b163f195797f14739cb62e39ad029fc0414", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80b9fe56-26ad-40d0-bc43-a8e394a7c4fd", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "beeab6275cf3f3d6c7f79ae1d155fc87a60d5f33ecdce3149458a1a2783ad8ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c4657f1-b2d9-4e0a-9383-77eb9cffdc88", "node_type": "1", "metadata": {}, "hash": "991b7dfc0ebc82e25fcb4599f8ede93df509558976e1ce3bb7068a882d74fb71", "class_name": "RelatedNodeInfo"}}, "text": "For those who don\u02bct know how neural networks work here is a short description(Note that this is just a very simple and crude\n  explaination and is su\u25aficient for this post.However,to appriciate the exact mechanism behind it consult                         other  resources too):\n  The input to the network is pumped through input layer.Here our inputs would be 3 dimensional as there are three nodes in our input\n  layer.These dimensions are also referred to as features.The inputs are multiplied with randomly initialized weights usually in form of\n  matrix.The output of this matrix multiplication is subjected to a sigmoid function.The sigmoidal outputs are used to generate cost\n  function.A cost function is a function that is a measure of deviation of our output from the actual value during the training of\n  network.For this post we will use        cross-entropy     cost function.The nodes in output layer of our network represent di\u25aferent classes to\n  which the input has to be classified.During the training of network we have a label corresponding to every input.This label represents\n  the true class of that input.\n  Aim\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                                                                 1/74/5/24, 8:25 PM                                                    Further-into-backpropagation\n Our aim would be to adjust the values of weights such that our cost function is minimum(i.e. the deviation of our output from actual\n value is minimum).\n So where do we start?If you are following the posts in series then you would know the answer to this!Thats right!Our update rules.Here\n we have to manipulate the values of weights to decrease the value of our cost thereby minimizing it.Thus updating weights to decrease\n the cost:                                         W     =    W     \u2212   h  \u2217    \u2202C\n                                                      ij         ij            \u2202Wij\n where  W   ijdenotes weight connecting   i th node in input layer tojth  node in output layer.\n We have to somehow find the value of     \u2202C    and fill it in our update rule which would decrease the cost function(due to the minus\n sign).", "start_char_idx": 1756, "end_char_idx": 3979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5c4657f1-b2d9-4e0a-9383-77eb9cffdc88": {"__data__": {"id_": "5c4657f1-b2d9-4e0a-9383-77eb9cffdc88", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "0954b0c789c1cfcc3c326a568e402b163f195797f14739cb62e39ad029fc0414", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7601ec8b-65f7-4cdb-885a-18115363d231", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "8f61538787bb3988aada95f884440f902a135a271583c11cf51fbc2b04297100", "class_name": "RelatedNodeInfo"}}, "text": "\u2202Wij\n Now that we know what we desire,lets analyse our cost function   C  .To do this we must first forward propagate through our network to\n analyse the dependencies of our cost function.Here we would take only one training example which would enable us to appriciate the\n intricacies better and from there we would extend this to multiple training examples.", "start_char_idx": 4013, "end_char_idx": 4372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-0": {"__data__": {"id_": "node-0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ff6e428-f8cf-4eeb-804e-19f93cab97d4", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "04b91a201da822821acf57c9f425f56accbc39a6b42b39e09cd3d706cee6897d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4069e59-9588-402f-b94d-d5eb9f52434a", "node_type": "1", "metadata": {}, "hash": "30ffc83c1fe3b9bf1b619a1ebaf4e49d0983e1f647d7148f0e6aedc00be42f9d", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:25 PM                                                                    Further-into-backpropagation\n  Backpropagation                                                                                                                                         HOME\n  Further into Backpropagation\n  In the  previous postwe applied chain rule(funkily called backpropagation!) to systems with complex functions.In this post we will\n  apply backpropagation to neural networks.In this post we will apply backprorpagation to a two layered neural network.Note that\n  instead of derivation of mathematical formulaes we will focus on intuitive sense of it.We will look to explore implementation of\n  backpropagation with multiple inputs represented in form of matrix of training data.\n  Lets get started!!!\n  Consider the network:\n                                                                                                        sigmoid\n                                                                                                                                      Cost\n                                                                                                        sigmoid                    Function\n  The figure consists of a two layered network.The first layer is the input layer and contains 3 nodes.The second layer is output layer and\n  contains two nodes.In a standard neural network,the sigmoid layer is a part of output layer.For clarity of concept I have drawn it as a\n  separate layer.The sigmoid layer quashes the output values in the range of 0 and 1.The two layers are connected to each other with\n  weights which are represented by edges in the figure.Each node of first layer is connected to every node of second layer.\n  For those who don\u02bct know how neural networks work here is a short description(Note that this is just a very simple and crude\n  explaination and is su\u25aficient for this post.However,to appriciate the exact mechanism behind it consult                         other  resources too):\n  The input to the network is pumped through input layer.Here our inputs would be 3 dimensional as there are three nodes in our input\n  layer.These dimensions are also referred to as features.The inputs are multiplied with randomly initialized weights usually in form of\n  matrix.The output of this matrix multiplication is subjected to a sigmoid function.The sigmoidal outputs are used to generate cost\n  function.A cost function is a function that is a measure of deviation of our output from the actual value during the training of\n  network.For this post we will use        cross-entropy     cost function.The nodes in output layer of our network represent di\u25aferent classes to\n  which the input has to be classified.During the training of network we have a label corresponding to every input.This label represents\n  the true class of that input.\n  Aim\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                                                                 1/74/5/24, 8:25 PM                                                    Further-into-backpropagation\n Our aim would be to adjust the values of weights such that our cost function is minimum(i.e. the deviation of our output from actual\n value is minimum).\n So where do we start?If you are following the posts in series then you would know the answer to this!Thats right!Our update rules.Here\n we have to manipulate the values of weights to decrease the value of our cost thereby minimizing it.Thus updating weights to decrease\n the cost:                                         W     =    W     \u2212   h  \u2217    \u2202C\n                                                      ij         ij            \u2202Wij\n where  W   ijdenotes weight connecting   i th node in input layer tojth  node in output layer.\n We have to somehow find the value of     \u2202C    and fill it in our update rule which would decrease the cost function(due to the minus\n sign).                                  \u2202Wij\n Now that we know what we desire,lets analyse our cost function   C  .To do this we must first forward propagate through our network to\n analyse the dependencies of our cost function.Here we would take only one training example which would enable us to appriciate the\n intricacies better and from there we would extend this to multiple training examples.", "start_char_idx": 0, "end_char_idx": 4372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3a5e81ec-b17c-43db-a9b4-358172d27636": {"__data__": {"id_": "3a5e81ec-b17c-43db-a9b4-358172d27636", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "9cc8941baeabb09c2684d36820a9311cebe19fd51acd8df553c89f891bfc9c5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea93c57e-c214-418d-bd69-3eb6773b4bb4", "node_type": "1", "metadata": {}, "hash": "7ba0232f8befb17de2a25af761285f6e88e2a98bfde912c133430703521700ab", "class_name": "RelatedNodeInfo"}}, "text": "Forward Propagation\n The training example on which this analysis will be based is x 1, x 2 ,x  3 which can be represented in form of 1X3 matrix as:\n                                                      X   = [   x 1    x  2    x 3 ]\n The weights can also be represented in form of a 3x2 matrix as\n                                                              \u23a1  W   11     W  12  \u23a4", "start_char_idx": 0, "end_char_idx": 381, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ea93c57e-c214-418d-bd69-3eb6773b4bb4": {"__data__": {"id_": "ea93c57e-c214-418d-bd69-3eb6773b4bb4", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "9cc8941baeabb09c2684d36820a9311cebe19fd51acd8df553c89f891bfc9c5a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a5e81ec-b17c-43db-a9b4-358172d27636", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "1b811dd8865fda4c9071c3b4887ee6135c29fe90ac61ed4badb27baff346c36a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "278a326d-1033-4b3e-8697-3d7d0ceb456e", "node_type": "1", "metadata": {}, "hash": "6d35a7216d16ac727cf75aee2ddcd9998e3eca12369300d5f9f60e02b2e72493", "class_name": "RelatedNodeInfo"}}, "text": "W    =   \u23a2W     21     W  22  \u23a5\n                                                              \u23a3  W   31     W  32  \u23a6\n The output before application of sigmoid can easily be found out by multiplying the two matrix to generate a 1x2 output matrix:\n                                      \u23a1 W  11  W  12\u23a4", "start_char_idx": 435, "end_char_idx": 734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "278a326d-1033-4b3e-8697-3d7d0ceb456e": {"__data__": {"id_": "278a326d-1033-4b3e-8697-3d7d0ceb456e", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "9cc8941baeabb09c2684d36820a9311cebe19fd51acd8df553c89f891bfc9c5a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea93c57e-c214-418d-bd69-3eb6773b4bb4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "a8c014578a1d2835ac527fab232fff37d173b56308b7f570d02b42a1c5a93cfd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "faa5511e-4faf-425f-8a64-bb7015069bc2", "node_type": "1", "metadata": {}, "hash": "aaed914bb50d2ad2d165d815f71e55912349f598e88b6a177a9903e44c4d4391", "class_name": "RelatedNodeInfo"}}, "text": "[x 1  x 2   x3 ] \u2217\u23a2W   21  W  22\u23a5  = [x 1W  11+  x 2W 21 +  x3W 31   x 1W 12 +  x2W  22+x   3W 32]\n                                      \u23a3 W  31  W  32\u23a6\n Note that the output matrix is a 1x2 matrix with two elements of which one belongs to the first node and other to the second node (of\n the pre sigmoidal output layer)for a single training example.", "start_char_idx": 755, "end_char_idx": 1105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "faa5511e-4faf-425f-8a64-bb7015069bc2": {"__data__": {"id_": "faa5511e-4faf-425f-8a64-bb7015069bc2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "9cc8941baeabb09c2684d36820a9311cebe19fd51acd8df553c89f891bfc9c5a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "278a326d-1033-4b3e-8697-3d7d0ceb456e", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "24df268717e4753b4f98c05f99f4ef6f68f7103e2f2ae108090669f3936b7781", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "327c823a-c057-497f-9def-d120b4e3cc77", "node_type": "1", "metadata": {}, "hash": "acd29237d1cc8d1c6f93cd702ddb4e0b9c2976fa453e8cb7f3461ae4dd745a1e", "class_name": "RelatedNodeInfo"}}, "text": "Let this matrix be represented as:\n             y = [   x 1 W  11  +   x 2 W  21  +   x 3 W  31     x 1 W  12  +   x 2 W  22  +   x 3W   32  ] \u2261 [   y1     y2  ]\n where  y 1 ,y 2 ,y  are placeholders to facilitate understanding.\n Applying sigmoid on this matrix we get:\n                              y o  = [   sigmoid(y       1 )    sigmoid(y        2 ) ] \u2261 [   y o1    y o2  ]\n where  y o1 , yo2 , y o are placeholders.", "start_char_idx": 1105, "end_char_idx": 1526, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "327c823a-c057-497f-9def-d120b4e3cc77": {"__data__": {"id_": "327c823a-c057-497f-9def-d120b4e3cc77", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "9cc8941baeabb09c2684d36820a9311cebe19fd51acd8df553c89f891bfc9c5a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "faa5511e-4faf-425f-8a64-bb7015069bc2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "e3b8bbea820ae1d2dab96ab670e56fe613e3e812d015bb5c5a7d8fb973cfc5e6", "class_name": "RelatedNodeInfo"}}, "text": "Cost function\n y o1 , y o2 obtained from forward propagation are used in cost function.Here we are using   cross-entropycost function which is given\n by:\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                                         2/74/5/24, 8:25 PM                                             Further-into-backpropagation\n                                            C  = \u2212     1   \u2211    p i \u2217 log(q  i)\n                                                       N     i\n where   is the number of catagories for classification(equivalent to number of nodes in output unit),is true label of that class and\n        i                                                                                  pi\n qi is predicted value and N is total number of training examples.\n For this system       (as number of nodes in output layer=2 i.e. 2 classification classes).", "start_char_idx": 1528, "end_char_idx": 2440, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ef858ddc-60b2-4a6f-a16e-e5ee92ee1343": {"__data__": {"id_": "ef858ddc-60b2-4a6f-a16e-e5ee92ee1343", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "9cc8941baeabb09c2684d36820a9311cebe19fd51acd8df553c89f891bfc9c5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28bd0c05-db02-499d-9030-1d46c7ad1cf8", "node_type": "1", "metadata": {}, "hash": "acd29237d1cc8d1c6f93cd702ddb4e0b9c2976fa453e8cb7f3461ae4dd745a1e", "class_name": "RelatedNodeInfo"}}, "text": "Forward Propagation\n The training example on which this analysis will be based is x 1, x 2 ,x  3 which can be represented in form of 1X3 matrix as:\n                                                      X   = [   x 1    x  2    x 3 ]\n The weights can also be represented in form of a 3x2 matrix as\n                                                              \u23a1  W   11     W  12  \u23a4\n                                                     W    =   \u23a2W     21     W  22  \u23a5\n                                                              \u23a3  W   31     W  32  \u23a6\n The output before application of sigmoid can easily be found out by multiplying the two matrix to generate a 1x2 output matrix:\n                                      \u23a1 W  11  W  12\u23a4\n                    [x 1  x 2   x3 ] \u2217\u23a2W   21  W  22\u23a5  = [x 1W  11+  x 2W 21 +  x3W 31   x 1W 12 +  x2W  22+x   3W 32]\n                                      \u23a3 W  31  W  32\u23a6\n Note that the output matrix is a 1x2 matrix with two elements of which one belongs to the first node and other to the second node (of\n the pre sigmoidal output layer)for a single training example.Let this matrix be represented as:\n             y = [   x 1 W  11  +   x 2 W  21  +   x 3 W  31     x 1 W  12  +   x 2 W  22  +   x 3W   32  ] \u2261 [   y1     y2  ]\n where  y 1 ,y 2 ,y  are placeholders to facilitate understanding.\n Applying sigmoid on this matrix we get:\n                              y o  = [   sigmoid(y       1 )    sigmoid(y        2 ) ] \u2261 [   y o1    y o2  ]\n where  y o1 , yo2 , y o are placeholders.", "start_char_idx": 0, "end_char_idx": 1526, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "28bd0c05-db02-499d-9030-1d46c7ad1cf8": {"__data__": {"id_": "28bd0c05-db02-499d-9030-1d46c7ad1cf8", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "9cc8941baeabb09c2684d36820a9311cebe19fd51acd8df553c89f891bfc9c5a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef858ddc-60b2-4a6f-a16e-e5ee92ee1343", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "5cce55ae03c561119f8bd37896d411f30965ce79b0c0a19d9a7a1558a309ead5", "class_name": "RelatedNodeInfo"}}, "text": "Cost function\n y o1 , y o2 obtained from forward propagation are used in cost function.Here we are using   cross-entropycost function which is given\n by:\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                                         2/74/5/24, 8:25 PM                                             Further-into-backpropagation\n                                            C  = \u2212     1   \u2211    p i \u2217 log(q  i)\n                                                       N     i\n where   is the number of catagories for classification(equivalent to number of nodes in output unit),is true label of that class and\n        i                                                                                  pi\n qi is predicted value and N is total number of training examples.\n For this system       (as number of nodes in output layer=2 i.e. 2 classification classes).", "start_char_idx": 1528, "end_char_idx": 2440, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-1": {"__data__": {"id_": "node-1", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ff6e428-f8cf-4eeb-804e-19f93cab97d4", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "04b91a201da822821acf57c9f425f56accbc39a6b42b39e09cd3d706cee6897d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c28666c-7190-48e4-bf49-26ea37ecfc3f", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "0954b0c789c1cfcc3c326a568e402b163f195797f14739cb62e39ad029fc0414", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c428b34a-9673-46f4-baf2-20d5e9ad5dcd", "node_type": "1", "metadata": {}, "hash": "70eee9fd0a21dca9432a926326fccbfb889feaee3d12463facf1313fff84f2d7", "class_name": "RelatedNodeInfo"}}, "text": "Forward Propagation\n The training example on which this analysis will be based is x 1, x 2 ,x  3 which can be represented in form of 1X3 matrix as:\n                                                      X   = [   x 1    x  2    x 3 ]\n The weights can also be represented in form of a 3x2 matrix as\n                                                              \u23a1  W   11     W  12  \u23a4\n                                                     W    =   \u23a2W     21     W  22  \u23a5\n                                                              \u23a3  W   31     W  32  \u23a6\n The output before application of sigmoid can easily be found out by multiplying the two matrix to generate a 1x2 output matrix:\n                                      \u23a1 W  11  W  12\u23a4\n                    [x 1  x 2   x3 ] \u2217\u23a2W   21  W  22\u23a5  = [x 1W  11+  x 2W 21 +  x3W 31   x 1W 12 +  x2W  22+x   3W 32]\n                                      \u23a3 W  31  W  32\u23a6\n Note that the output matrix is a 1x2 matrix with two elements of which one belongs to the first node and other to the second node (of\n the pre sigmoidal output layer)for a single training example.Let this matrix be represented as:\n             y = [   x 1 W  11  +   x 2 W  21  +   x 3 W  31     x 1 W  12  +   x 2 W  22  +   x 3W   32  ] \u2261 [   y1     y2  ]\n where  y 1 ,y 2 ,y  are placeholders to facilitate understanding.\n Applying sigmoid on this matrix we get:\n                              y o  = [   sigmoid(y       1 )    sigmoid(y        2 ) ] \u2261 [   y o1    y o2  ]\n where  y o1 , yo2 , y o are placeholders.\n Cost function\n y o1 , y o2 obtained from forward propagation are used in cost function.Here we are using   cross-entropycost function which is given\n by:\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                                         2/74/5/24, 8:25 PM                                             Further-into-backpropagation\n                                            C  = \u2212     1   \u2211    p i \u2217 log(q  i)\n                                                       N     i\n where   is the number of catagories for classification(equivalent to number of nodes in output unit),is true label of that class and\n        i                                                                                  pi\n qi is predicted value and N is total number of training examples.\n For this system       (as number of nodes in output layer=2 i.e. 2 classification classes).", "start_char_idx": 4374, "end_char_idx": 6814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5e528b2f-1d9c-4385-91a9-c0a0fb039ef1": {"__data__": {"id_": "5e528b2f-1d9c-4385-91a9-c0a0fb039ef1", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "a5014e6bb579857adafb84d3cc339a36e602937d12d7d0882b0782b9482a4348", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e931283-9daf-498b-a1cf-10f00e6b3519", "node_type": "1", "metadata": {}, "hash": "28b7ca23c0bf6f6d42a321b6f0883e225bc233e4a16a111c284309fe1200682e", "class_name": "RelatedNodeInfo"}}, "text": "and            as they are\n               i = 2                                                              q 1 =   yo1     q 2 =   y o2\n the predicted value of the two classes.When we train our network against training data,the label corresponding to a training example\n would be known.The label represents the class that the training example belongs to.Here let us assume that the training example\n                                                                                                                p   = 1\n belongs to the first class.This would make the label values of all other classes to be zero and of the first class as 1.Thus here1\n and           .Expanding our cost function for    ,", "start_char_idx": 0, "end_char_idx": 705, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3e931283-9daf-498b-a1cf-10f00e6b3519": {"__data__": {"id_": "3e931283-9daf-498b-a1cf-10f00e6b3519", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "a5014e6bb579857adafb84d3cc339a36e602937d12d7d0882b0782b9482a4348", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e528b2f-1d9c-4385-91a9-c0a0fb039ef1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "eee7c6ed29d17d835df8db191a8d879277208f17236762852961136701141113", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2dff36fc-0a82-4c3d-90af-14072fa277f9", "node_type": "1", "metadata": {}, "hash": "acb201a7ca9f47cda84745d5d20aff38a14e9cf113f2129b124f70a6a6dc4756", "class_name": "RelatedNodeInfo"}}, "text": "we get:\n     p 2 = 0                                i = 2\n Putting corresponding values we get:C  = \u2212    N1   \u2217 (p 1 log(q  1) +   p2 log(q  2))\n Backpropagation                    C   = \u2212(p    1 \u2217 log(y   o1) +   p2  \u2217 log(y  o2 ))\n Let us first revisit the matrices that we have: A pre sigmoidal output matrix\n A post sigmoidal output matrix                      y = [  y 1    y2 ]\n Our weight matrix                                 yo  = [  yo1    y o2 ]\n                                                   \u23a1  W   11    W  12 \u23a4\n                                                   \u23a2  W   21    W  22 \u23a5\n Our input matrix                                  \u23a3  W   31    W  32 \u23a6\n                                                X   = [  x 1    x 2    x 3]\n Remember that we had to find the value of\u2202C   to put into update rule which would decrease the cost.", "start_char_idx": 705, "end_char_idx": 1556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2dff36fc-0a82-4c3d-90af-14072fa277f9": {"__data__": {"id_": "2dff36fc-0a82-4c3d-90af-14072fa277f9", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "a5014e6bb579857adafb84d3cc339a36e602937d12d7d0882b0782b9482a4348", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e931283-9daf-498b-a1cf-10f00e6b3519", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "1117b43597255954f54ac3ce3840df60cb7a27ae3b8b659c09dccb4b726cb545", "class_name": "RelatedNodeInfo"}}, "text": "We will find this value by\n                                         \u2202Wij\n applying chain rule as we have done in previous posts but the only di\u25aference here will be that we would be dealing with matrices\n instead of individual variables.While applying chain rule we will focus on the parallelism in dealing with matrices and variables thereby\n making the transition to matrices smoother and intuitive.\n If we look at our network figure and our cost function,we notice that our cost function is a function ofwhich is represented\n                                                                                            y o1, y o2\n in    matrix which is a function of     which is represented inmatrix which is function of inputs and weights.\n    yo                           y1 ,y 2                      y\n Let us move back through the system from cost function to input layer and write the chain rule.First we will move back from one node\n to other and alongside it we will represent layerwise movement in terms of matrices.", "start_char_idx": 1556, "end_char_idx": 2581, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "eb932cc1-0060-4770-a948-5dba645c3f86": {"__data__": {"id_": "eb932cc1-0060-4770-a948-5dba645c3f86", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "a5014e6bb579857adafb84d3cc339a36e602937d12d7d0882b0782b9482a4348", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2dd516de-dd92-4d5f-9dd4-054d5ddcbad7", "node_type": "1", "metadata": {}, "hash": "fc0f4e359fc84eac80b09cb9f42ec55fe0c06a5253fa47f057c0d44ec5f232b6", "class_name": "RelatedNodeInfo"}}, "text": "and            as they are\n               i = 2                                                              q 1 =   yo1     q 2 =   y o2\n the predicted value of the two classes.When we train our network against training data,the label corresponding to a training example\n would be known.The label represents the class that the training example belongs to.Here let us assume that the training example\n                                                                                                                p   = 1\n belongs to the first class.This would make the label values of all other classes to be zero and of the first class as 1.Thus here1\n and           .Expanding our cost function for    ,we get:\n     p 2 = 0                                i = 2\n Putting corresponding values we get:C  = \u2212    N1   \u2217 (p 1 log(q  1) +   p2 log(q  2))\n Backpropagation                    C   = \u2212(p    1 \u2217 log(y   o1) +   p2  \u2217 log(y  o2 ))\n Let us first revisit the matrices that we have: A pre sigmoidal output matrix\n A post sigmoidal output matrix                      y = [  y 1    y2 ]\n Our weight matrix                                 yo  = [  yo1    y o2 ]\n                                                   \u23a1  W   11    W  12 \u23a4\n                                                   \u23a2  W   21    W  22 \u23a5\n Our input matrix                                  \u23a3  W   31    W  32 \u23a6\n                                                X   = [  x 1    x 2    x 3]\n Remember that we had to find the value of\u2202C   to put into update rule which would decrease the cost.We will find this value by\n                                         \u2202Wij\n applying chain rule as we have done in previous posts but the only di\u25aference here will be that we would be dealing with matrices\n instead of individual variables.While applying chain rule we will focus on the parallelism in dealing with matrices and variables thereby\n making the transition to matrices smoother and intuitive.\n If we look at our network figure and our cost function,we notice that our cost function is a function ofwhich is represented\n                                                                                            y o1, y o2\n in    matrix which is a function of     which is represented inmatrix which is function of inputs and weights.", "start_char_idx": 0, "end_char_idx": 2297, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2dd516de-dd92-4d5f-9dd4-054d5ddcbad7": {"__data__": {"id_": "2dd516de-dd92-4d5f-9dd4-054d5ddcbad7", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "a5014e6bb579857adafb84d3cc339a36e602937d12d7d0882b0782b9482a4348", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb932cc1-0060-4770-a948-5dba645c3f86", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "f281cfd80c65c0378dd28c0ba168c29ca33c0586598a63d1506c0cfe5741590b", "class_name": "RelatedNodeInfo"}}, "text": "yo                           y1 ,y 2                      y\n Let us move back through the system from cost function to input layer and write the chain rule.First we will move back from one node\n to other and alongside it we will represent layerwise movement in terms of matrices.", "start_char_idx": 2302, "end_char_idx": 2581, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-2": {"__data__": {"id_": "node-2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ff6e428-f8cf-4eeb-804e-19f93cab97d4", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "04b91a201da822821acf57c9f425f56accbc39a6b42b39e09cd3d706cee6897d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4069e59-9588-402f-b94d-d5eb9f52434a", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "9cc8941baeabb09c2684d36820a9311cebe19fd51acd8df553c89f891bfc9c5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6190969d-d209-4912-8c24-aad8ffccf989", "node_type": "1", "metadata": {}, "hash": "8d59c6d08fa2de4217f5a09ba9fd26488d2a8bb8a9b7aab9601f05ee2e10b297", "class_name": "RelatedNodeInfo"}}, "text": "and            as they are\n               i = 2                                                              q 1 =   yo1     q 2 =   y o2\n the predicted value of the two classes.When we train our network against training data,the label corresponding to a training example\n would be known.The label represents the class that the training example belongs to.Here let us assume that the training example\n                                                                                                                p   = 1\n belongs to the first class.This would make the label values of all other classes to be zero and of the first class as 1.Thus here1\n and           .Expanding our cost function for    ,we get:\n     p 2 = 0                                i = 2\n Putting corresponding values we get:C  = \u2212    N1   \u2217 (p 1 log(q  1) +   p2 log(q  2))\n Backpropagation                    C   = \u2212(p    1 \u2217 log(y   o1) +   p2  \u2217 log(y  o2 ))\n Let us first revisit the matrices that we have: A pre sigmoidal output matrix\n A post sigmoidal output matrix                      y = [  y 1    y2 ]\n Our weight matrix                                 yo  = [  yo1    y o2 ]\n                                                   \u23a1  W   11    W  12 \u23a4\n                                                   \u23a2  W   21    W  22 \u23a5\n Our input matrix                                  \u23a3  W   31    W  32 \u23a6\n                                                X   = [  x 1    x 2    x 3]\n Remember that we had to find the value of\u2202C   to put into update rule which would decrease the cost.We will find this value by\n                                         \u2202Wij\n applying chain rule as we have done in previous posts but the only di\u25aference here will be that we would be dealing with matrices\n instead of individual variables.While applying chain rule we will focus on the parallelism in dealing with matrices and variables thereby\n making the transition to matrices smoother and intuitive.\n If we look at our network figure and our cost function,we notice that our cost function is a function ofwhich is represented\n                                                                                            y o1, y o2\n in    matrix which is a function of     which is represented inmatrix which is function of inputs and weights.\n    yo                           y1 ,y 2                      y\n Let us move back through the system from cost function to input layer and write the chain rule.First we will move back from one node\n to other and alongside it we will represent layerwise movement in terms of matrices.", "start_char_idx": 6817, "end_char_idx": 9398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "fa38b574-d6b7-42ea-ab84-1ad8adff3e1a": {"__data__": {"id_": "fa38b574-d6b7-42ea-ab84-1ad8adff3e1a", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "4e61b429f6e40ce63865d810b19030939521b8409490e1f780cfd3ed5ac7902c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "204a8a0c-f79d-4dc5-8b65-bf51c153d65e", "node_type": "1", "metadata": {}, "hash": "32c549e60f2ce7c22d604b99e36bc4948f9c44d6882f45fb990acaf4e64342a6", "class_name": "RelatedNodeInfo"}}, "text": "From cost function towards sigmoid layer\n                                     C   = \u2212((p    1 \u2217  log(y  o1 ) +  p 2 \u2217 log(y   o2)))\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                          3/74/5/24, 8:25 PM                                         Further-into-backpropagation\n     We can easily write the derivatives:         \u2202C             p1\n                                                  \u2202y o1  = \u2212(    yo1 )", "start_char_idx": 0, "end_char_idx": 479, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "204a8a0c-f79d-4dc5-8b65-bf51c153d65e": {"__data__": {"id_": "204a8a0c-f79d-4dc5-8b65-bf51c153d65e", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "4e61b429f6e40ce63865d810b19030939521b8409490e1f780cfd3ed5ac7902c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa38b574-d6b7-42ea-ab84-1ad8adff3e1a", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "24e8826b98a3d4ef310062e61df2e04b1e55f3e933e0eb265e06518905efdcd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "271e2be6-222c-4499-9703-dbb07c8c8e16", "node_type": "1", "metadata": {}, "hash": "9958f799c0f55d34daeb04f0980ce5925230ef8cdb7ac1b411a2f9d18a1d9219", "class_name": "RelatedNodeInfo"}}, "text": "\u2202C             p 2\n     We can represent this in form of a matrix:   \u2202y o2  = \u2212(    yo2 )\n                                           \u2202C    =  [ \u2212(   p1 )    \u2212(  p2 ) ]\n                                           \u2202yo             yo1         yo2\n     Through the sigmoid layer- From our experiences in previous posts we can easily write the sigmoid derivatives", "start_char_idx": 530, "end_char_idx": 887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "271e2be6-222c-4499-9703-dbb07c8c8e16": {"__data__": {"id_": "271e2be6-222c-4499-9703-dbb07c8c8e16", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "4e61b429f6e40ce63865d810b19030939521b8409490e1f780cfd3ed5ac7902c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "204a8a0c-f79d-4dc5-8b65-bf51c153d65e", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "5bce2a837bc1ce552e71c730cf1af1268a0236bdc390ffbd9df7fb3794017d75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b851c39c-3cb3-4288-a388-be0ffab814a4", "node_type": "1", "metadata": {}, "hash": "85381b8952c6240b9c01a136a10fc3a33e5fdb0c6a62a78ad4ee09ecd509c30a", "class_name": "RelatedNodeInfo"}}, "text": "as:\n                                          \u2202yo1   =   \u03c3(y  1) \u2217 (1 \u2212   \u03c3(y  1))\n                                           \u2202y1\n                                          \u2202yo2   =   \u03c3(y  2) \u2217 (1 \u2212   \u03c3(y  2))\n     We can represent this in matrix form: \u2202y2\n                           \u2202yo   = [  \u03c3(y", "start_char_idx": 888, "end_char_idx": 1185, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b851c39c-3cb3-4288-a388-be0ffab814a4": {"__data__": {"id_": "b851c39c-3cb3-4288-a388-be0ffab814a4", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "4e61b429f6e40ce63865d810b19030939521b8409490e1f780cfd3ed5ac7902c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "271e2be6-222c-4499-9703-dbb07c8c8e16", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "3f86982c1ebfe9ef1603028aab35afc4b0353c5e962166a1aa52cfb903d61377", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eec3195e-b2af-4122-97aa-30af58baec20", "node_type": "1", "metadata": {}, "hash": "dbba7e8903e150e2229385ca9e64827b53e95d262b701895e2c9ee59cc878be4", "class_name": "RelatedNodeInfo"}}, "text": "1) \u2217 (1 \u2212   \u03c3(y  1))    \u03c3(y 2) \u2217 (1 \u2212   \u03c3(y  2)) ]\n                            \u2202y\n     Through the pre sigmoidal output layer towards the weights We know the relations\n     and                                y1 =   x 1W  11 +  x 2W  21 +   x3 W  31\n                                        y2 =   x 1W  12 +  x 2W  22 +   x3 W  32\n     .", "start_char_idx": 1187, "end_char_idx": 1523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "eec3195e-b2af-4122-97aa-30af58baec20": {"__data__": {"id_": "eec3195e-b2af-4122-97aa-30af58baec20", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "4e61b429f6e40ce63865d810b19030939521b8409490e1f780cfd3ed5ac7902c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b851c39c-3cb3-4288-a388-be0ffab814a4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "124f6470d35fadc1efdb90da7bcdb4ea5c361c7d1b71bb035fbdf222f9a0abea", "class_name": "RelatedNodeInfo"}}, "text": "From these we can easily find the derivatives:  \u2202y1    =   x1\n                                                     \u2202W11\n                                                      \u2202y1    =   x2\n                                                     \u2202W21\n                                                      \u2202y1    =   x3\n                                                     \u2202W31\n                                                      \u2202y2    =   x1\n                                                     \u2202W12\n                                                      \u2202y2    =   x2\n                                                     \u2202W22\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                   4/74/5/24, 8:25 PM                                        Further-into-backpropagation\n                                                    \u2202y2     =  x3\n                                                   \u2202W32\n  Let us chain all the derivatives elementwise first.Then we will convert it into matrix representation.", "start_char_idx": 1523, "end_char_idx": 2574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "14faa69d-f366-4491-a511-3758f463eca7": {"__data__": {"id_": "14faa69d-f366-4491-a511-3758f463eca7", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "4e61b429f6e40ce63865d810b19030939521b8409490e1f780cfd3ed5ac7902c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58a3edee-3a09-45b1-9e83-c5676d068f5c", "node_type": "1", "metadata": {}, "hash": "dbba7e8903e150e2229385ca9e64827b53e95d262b701895e2c9ee59cc878be4", "class_name": "RelatedNodeInfo"}}, "text": "From cost function towards sigmoid layer\n                                     C   = \u2212((p    1 \u2217  log(y  o1 ) +  p 2 \u2217 log(y   o2)))\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                          3/74/5/24, 8:25 PM                                         Further-into-backpropagation\n     We can easily write the derivatives:         \u2202C             p1\n                                                  \u2202y o1  = \u2212(    yo1 )\n                                                  \u2202C             p 2\n     We can represent this in form of a matrix:   \u2202y o2  = \u2212(    yo2 )\n                                           \u2202C    =  [ \u2212(   p1 )    \u2212(  p2 ) ]\n                                           \u2202yo             yo1         yo2\n     Through the sigmoid layer- From our experiences in previous posts we can easily write the sigmoid derivatives as:\n                                          \u2202yo1   =   \u03c3(y  1) \u2217 (1 \u2212   \u03c3(y  1))\n                                           \u2202y1\n                                          \u2202yo2   =   \u03c3(y  2) \u2217 (1 \u2212   \u03c3(y  2))\n     We can represent this in matrix form: \u2202y2\n                           \u2202yo   = [  \u03c3(y  1) \u2217 (1 \u2212   \u03c3(y  1))    \u03c3(y 2) \u2217 (1 \u2212   \u03c3(y  2)) ]\n                            \u2202y\n     Through the pre sigmoidal output layer towards the weights We know the relations\n     and                                y1 =   x 1W  11 +  x 2W  21 +   x3 W  31\n                                        y2 =   x 1W  12 +  x 2W  22 +   x3 W  32\n     .", "start_char_idx": 0, "end_char_idx": 1523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "58a3edee-3a09-45b1-9e83-c5676d068f5c": {"__data__": {"id_": "58a3edee-3a09-45b1-9e83-c5676d068f5c", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "4e61b429f6e40ce63865d810b19030939521b8409490e1f780cfd3ed5ac7902c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14faa69d-f366-4491-a511-3758f463eca7", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "1f50f3fd041175a163653fbd9a0e174383f67c7b47e8e8dde37cfaad0a7e377a", "class_name": "RelatedNodeInfo"}}, "text": "From these we can easily find the derivatives:  \u2202y1    =   x1\n                                                     \u2202W11\n                                                      \u2202y1    =   x2\n                                                     \u2202W21\n                                                      \u2202y1    =   x3\n                                                     \u2202W31\n                                                      \u2202y2    =   x1\n                                                     \u2202W12\n                                                      \u2202y2    =   x2\n                                                     \u2202W22\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                   4/74/5/24, 8:25 PM                                        Further-into-backpropagation\n                                                    \u2202y2     =  x3\n                                                   \u2202W32\n  Let us chain all the derivatives elementwise first.Then we will convert it into matrix representation.", "start_char_idx": 1523, "end_char_idx": 2574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-3": {"__data__": {"id_": "node-3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ff6e428-f8cf-4eeb-804e-19f93cab97d4", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "04b91a201da822821acf57c9f425f56accbc39a6b42b39e09cd3d706cee6897d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c428b34a-9673-46f4-baf2-20d5e9ad5dcd", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "a5014e6bb579857adafb84d3cc339a36e602937d12d7d0882b0782b9482a4348", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b217e96-7acf-486d-8156-c9e12dc3720a", "node_type": "1", "metadata": {}, "hash": "a4830b1216cc4e0511e5a9cb10e246a365ebdd99a09f719aed1b609096818d2f", "class_name": "RelatedNodeInfo"}}, "text": "From cost function towards sigmoid layer\n                                     C   = \u2212((p    1 \u2217  log(y  o1 ) +  p 2 \u2217 log(y   o2)))\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                          3/74/5/24, 8:25 PM                                         Further-into-backpropagation\n     We can easily write the derivatives:         \u2202C             p1\n                                                  \u2202y o1  = \u2212(    yo1 )\n                                                  \u2202C             p 2\n     We can represent this in form of a matrix:   \u2202y o2  = \u2212(    yo2 )\n                                           \u2202C    =  [ \u2212(   p1 )    \u2212(  p2 ) ]\n                                           \u2202yo             yo1         yo2\n     Through the sigmoid layer- From our experiences in previous posts we can easily write the sigmoid derivatives as:\n                                          \u2202yo1   =   \u03c3(y  1) \u2217 (1 \u2212   \u03c3(y  1))\n                                           \u2202y1\n                                          \u2202yo2   =   \u03c3(y  2) \u2217 (1 \u2212   \u03c3(y  2))\n     We can represent this in matrix form: \u2202y2\n                           \u2202yo   = [  \u03c3(y  1) \u2217 (1 \u2212   \u03c3(y  1))    \u03c3(y 2) \u2217 (1 \u2212   \u03c3(y  2)) ]\n                            \u2202y\n     Through the pre sigmoidal output layer towards the weights We know the relations\n     and                                y1 =   x 1W  11 +  x 2W  21 +   x3 W  31\n                                        y2 =   x 1W  12 +  x 2W  22 +   x3 W  32\n     .From these we can easily find the derivatives:  \u2202y1    =   x1\n                                                     \u2202W11\n                                                      \u2202y1    =   x2\n                                                     \u2202W21\n                                                      \u2202y1    =   x3\n                                                     \u2202W31\n                                                      \u2202y2    =   x1\n                                                     \u2202W12\n                                                      \u2202y2    =   x2\n                                                     \u2202W22\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                   4/74/5/24, 8:25 PM                                        Further-into-backpropagation\n                                                    \u2202y2     =  x3\n                                                   \u2202W32\n  Let us chain all the derivatives elementwise first.Then we will convert it into matrix representation.", "start_char_idx": 9405, "end_char_idx": 11979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e50adabb-1984-4574-bb4f-691efe6ccc01": {"__data__": {"id_": "e50adabb-1984-4574-bb4f-691efe6ccc01", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "2a671606438c10f66f1c1259c82018e9a9d79b6276c61da10692547d09eef270", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d49c0ae8-bb63-4a13-bf0e-2ac2395feb24", "node_type": "1", "metadata": {}, "hash": "2f7075f0ae038204164d29e14b6254ee2d78c13de2afcb0ff73414f0046d7bc5", "class_name": "RelatedNodeInfo"}}, "text": "\u2202C          \u2202C       \u2202y o1      \u2202y 1\n                                     \u2202W  11  =   \u2202y o1  \u2217  \u2202y  1  \u2217 \u2202W   11\n  Putting the respective values we get-\n                                \u2202C         \u2212p1\n                               \u2202W  11  =    yo1   \u2217 \u03c3(y 1 ) \u2217 (1 \u2212  \u03c3(y  1)) \u2217 x 1\n  Similarly we can write this for", "start_char_idx": 0, "end_char_idx": 316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-4", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d49c0ae8-bb63-4a13-bf0e-2ac2395feb24": {"__data__": {"id_": "d49c0ae8-bb63-4a13-bf0e-2ac2395feb24", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "2a671606438c10f66f1c1259c82018e9a9d79b6276c61da10692547d09eef270", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e50adabb-1984-4574-bb4f-691efe6ccc01", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "b8485672465b0880d867a197f950008df07c54aac24934fb85710fb67ca4884d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cb07794-933b-4694-8298-9d4dd68a3a6c", "node_type": "1", "metadata": {}, "hash": "27f6a99f38dee2ce2a65f38346e2c51ec4da356be2540fa742336e9d7c6f2922", "class_name": "RelatedNodeInfo"}}, "text": "all the W\u02bcs and place them in a matrix as-\n                       p1                                          p2\n                \u23a1 \u2212(  yo1 ) \u2217 \u03c3(y 1) \u2217 (1 \u2212   \u03c3(y 1 )) \u2217 x 1   \u2212(  yo2) \u2217 \u03c3(y  2) \u2217 (1 \u2212   \u03c3(y 2)) \u2217 x 1 \u23a4\n      \u2202C        \u23a2 \u2212(   p1 ) \u2217 \u03c3(y  ) \u2217 (1 \u2212   \u03c3(y   )) \u2217 x     \u2212(  p2 ) \u2217 \u03c3(y   ) \u2217 (1 \u2212   \u03c3(y", "start_char_idx": 317, "end_char_idx": 631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-4", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0cb07794-933b-4694-8298-9d4dd68a3a6c": {"__data__": {"id_": "0cb07794-933b-4694-8298-9d4dd68a3a6c", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "2a671606438c10f66f1c1259c82018e9a9d79b6276c61da10692547d09eef270", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d49c0ae8-bb63-4a13-bf0e-2ac2395feb24", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "3aece44253f1df78704ebf30edc1ee6efe03ac36fc059bcd877c24fd88b2e902", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38709b63-cc3d-48cb-a100-2f0988dd64ea", "node_type": "1", "metadata": {}, "hash": "348723146f6b70adf301fb93bd3fe7737fd731b990ee3df2409463116d298613", "class_name": "RelatedNodeInfo"}}, "text": ")) \u2217 x   \u23a5\n            =   \u23a2     yo1         1               1        2       yo2         2               2       2 \u23a5\n      \u2202W        \u23a2      p1                                          p2                                    \u23a5\n                \u23a3", "start_char_idx": 633, "end_char_idx": 875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-4", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "38709b63-cc3d-48cb-a100-2f0988dd64ea": {"__data__": {"id_": "38709b63-cc3d-48cb-a100-2f0988dd64ea", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "2a671606438c10f66f1c1259c82018e9a9d79b6276c61da10692547d09eef270", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cb07794-933b-4694-8298-9d4dd68a3a6c", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "bfbf9b9729fc8ebd65caed7a62125ed5f16f30e79f591f86d71d5dd16d21873d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49a49168-e921-4fd1-8c1d-65de5e9175ad", "node_type": "1", "metadata": {}, "hash": "c8de181b10f98c9dcbc23d7077c54fb5bdc1d238ba5ebfce6bd15200db5ff672", "class_name": "RelatedNodeInfo"}}, "text": "\u2212(  yo1 ) \u2217 \u03c3(y 1) \u2217 (1 \u2212   \u03c3(y 1 )) \u2217 x 3   \u2212(  yo2) \u2217 \u03c3(y  2) \u2217 (1 \u2212   \u03c3(y 2)) \u2217 x 3 \u23a6\n  Observe the above matrix.It is nothing but the combination\n                                           X  T \u00d7 (   \u2202C   \u2299   \u2202yo  )\n                                                     \u2202y  o     \u2202y\n  where  denotes transpose of matrix anddenotes matrix product whiledenotes element-wise product. Now we can put this\n       T                            \u00d7                          \u2299\n  expression in our update rule:  W   =  W   \u2212  h  \u2217 X  T \u00d7 (  \u2202C   \u2299   \u2202yo  )\n  The python representation can be given as:                   \u2202y o      \u2202y\n  import  numpy as np\n  import  random\n  def sigmoid(x):\n           return 1/(1+np.", "start_char_idx": 876, "end_char_idx": 1583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-4", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "49a49168-e921-4fd1-8c1d-65de5e9175ad": {"__data__": {"id_": "49a49168-e921-4fd1-8c1d-65de5e9175ad", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "2a671606438c10f66f1c1259c82018e9a9d79b6276c61da10692547d09eef270", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38709b63-cc3d-48cb-a100-2f0988dd64ea", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "b184d74f3f22b28a7842b4283ec4dfa74bb3c43017d8c44f345f88370bcdd3f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e73be0ac-ed64-44da-8fbb-468a7bf4dc19", "node_type": "1", "metadata": {}, "hash": "b82bd5f3a4354c181362f07b7c14e15c8209878b83c01f6c9a2026d4a48b33c9", "class_name": "RelatedNodeInfo"}}, "text": "exp(-x))\n  def derivative_sigmoid(x):\n           return np.multiply(sigmoid(x),(1-sigmoid(x)))\n  #initialization\n  X=np.matrix(\"2,4,-2\")\n  W=np.random.normal(size=(3,2))\n  #label\n  ycap=[0]\n  #number of training of examples\n  num_examples=1\n  #step size\n  h=.01\n  #forward-propogation\n  y=np.dot(X,W)\n  y_o=sigmoid(y)\n  #loss calculation\n  loss=-np.sum(np.log(y_o[range(num_examples),ycap]))\n  print(loss)      #outputs 7.87 (for you it would be different due to random initialization of weights.)\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                               5/74/5/24, 8:25 PM                                                         Further-into-backpropagation\n  #backprop starts\n  temp1=np.", "start_char_idx": 1583, "end_char_idx": 2325, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-4", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e73be0ac-ed64-44da-8fbb-468a7bf4dc19": {"__data__": {"id_": "e73be0ac-ed64-44da-8fbb-468a7bf4dc19", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "2a671606438c10f66f1c1259c82018e9a9d79b6276c61da10692547d09eef270", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49a49168-e921-4fd1-8c1d-65de5e9175ad", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "e7fcf47a64032a582f40faf1613a132e6e6c64a2445f1d8514f11b7607842036", "class_name": "RelatedNodeInfo"}}, "text": "copy(y_o)\n  #implementation of derivative of cost function with respect to y_o\n  temp1[range(num_examples),ycap]=1/-(temp1[range(num_examples),ycap])\n  temp=np.zeros_like(y_o)\n  temp[range(num_examples),ycap]=1\n  #derivative of cost with respect to y_o\n  dcost=np.multiply(temp,temp1)\n  #derivative of y_o with respect to y\n  dy_o=derivative_sigmoid(y)\n  #element-wise multiplication\n  dgrad=np.multiply(dcost,dy_o)\n  dw=np.dot(X.T,dgrad)\n  #weight-update\n  W-=h*dw\n  #forward prop again with updated weight to find new loss\n  y=np.dot(X,W)\n  yo=sigmoid(y)\n  loss=-np.sum(np.log(yo[range(num_examples),ycap]))\n  print(loss)            #outpus 7.63 (again for you it would be different!)", "start_char_idx": 2325, "end_char_idx": 3011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-4", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3cf28b88-2d41-4c91-9e2c-c11847a6a053": {"__data__": {"id_": "3cf28b88-2d41-4c91-9e2c-c11847a6a053", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "2a671606438c10f66f1c1259c82018e9a9d79b6276c61da10692547d09eef270", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3916ef68-de36-4fbb-8fe2-d072df0a7d72", "node_type": "1", "metadata": {}, "hash": "e55c50e604f20437f92380223069b25060583cd27b803fc44a16ea830857ce3c", "class_name": "RelatedNodeInfo"}}, "text": "\u2202C          \u2202C       \u2202y o1      \u2202y 1\n                                     \u2202W  11  =   \u2202y o1  \u2217  \u2202y  1  \u2217 \u2202W   11\n  Putting the respective values we get-\n                                \u2202C         \u2212p1\n                               \u2202W  11  =    yo1   \u2217 \u03c3(y 1 ) \u2217 (1 \u2212  \u03c3(y  1)) \u2217 x 1\n  Similarly we can write this for all the W\u02bcs and place them in a matrix as-\n                       p1                                          p2\n                \u23a1 \u2212(  yo1 ) \u2217 \u03c3(y 1) \u2217 (1 \u2212   \u03c3(y 1 )) \u2217 x 1   \u2212(  yo2) \u2217 \u03c3(y  2) \u2217 (1 \u2212   \u03c3(y 2)) \u2217 x 1 \u23a4\n      \u2202C        \u23a2 \u2212(   p1 ) \u2217 \u03c3(y  ) \u2217 (1 \u2212   \u03c3(y   )) \u2217 x     \u2212(  p2 ) \u2217 \u03c3(y   ) \u2217 (1 \u2212   \u03c3(y  )) \u2217 x   \u23a5\n            =   \u23a2     yo1         1               1        2       yo2         2               2       2 \u23a5\n      \u2202W        \u23a2      p1                                          p2                                    \u23a5\n                \u23a3 \u2212(  yo1 ) \u2217 \u03c3(y 1) \u2217 (1 \u2212   \u03c3(y 1 )) \u2217 x 3   \u2212(  yo2) \u2217 \u03c3(y  2) \u2217 (1 \u2212   \u03c3(y 2)) \u2217 x 3 \u23a6\n  Observe the above matrix.It is nothing but the combination\n                                           X  T \u00d7 (   \u2202C   \u2299   \u2202yo  )\n                                                     \u2202y  o     \u2202y\n  where  denotes transpose of matrix anddenotes matrix product whiledenotes element-wise product.", "start_char_idx": 0, "end_char_idx": 1259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-4", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3916ef68-de36-4fbb-8fe2-d072df0a7d72": {"__data__": {"id_": "3916ef68-de36-4fbb-8fe2-d072df0a7d72", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "2a671606438c10f66f1c1259c82018e9a9d79b6276c61da10692547d09eef270", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3cf28b88-2d41-4c91-9e2c-c11847a6a053", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "49a850c9d051acc7a00d8ca5b775e3a548432240234c64b0de50348c2b58fa14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4e79706-075a-4e11-8b32-22ed35d9ac19", "node_type": "1", "metadata": {}, "hash": "d47892a085c3a84c6b781f01afc647719213196749b1cab6ed23519a55d11c3e", "class_name": "RelatedNodeInfo"}}, "text": "Now we can put this\n       T                            \u00d7                          \u2299\n  expression in our update rule:  W   =  W   \u2212  h  \u2217 X  T \u00d7 (  \u2202C   \u2299   \u2202yo  )\n  The python representation can be given as:                   \u2202y o      \u2202y\n  import  numpy as np\n  import  random\n  def sigmoid(x):\n           return 1/(1+np.exp(-x))\n  def derivative_sigmoid(x):\n           return np.multiply(sigmoid(x),(1-sigmoid(x)))\n  #initialization\n  X=np.matrix(\"2,4,-2\")\n  W=np.random.normal(size=(3,2))\n  #label\n  ycap=[0]\n  #number of training of examples\n  num_examples=1\n  #step size\n  h=.01\n  #forward-propogation\n  y=np.dot(X,W)\n  y_o=sigmoid(y)\n  #loss calculation\n  loss=-np.sum(np.log(y_o[range(num_examples),ycap]))\n  print(loss)      #outputs 7.87 (for you it would be different due to random initialization of weights.)", "start_char_idx": 1260, "end_char_idx": 2080, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-4", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f4e79706-075a-4e11-8b32-22ed35d9ac19": {"__data__": {"id_": "f4e79706-075a-4e11-8b32-22ed35d9ac19", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "2a671606438c10f66f1c1259c82018e9a9d79b6276c61da10692547d09eef270", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3916ef68-de36-4fbb-8fe2-d072df0a7d72", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "bf8deb754d80ee0160b93b94526b0eb6db4d790329bdd943b3287c899caf5449", "class_name": "RelatedNodeInfo"}}, "text": "https://jasdeep06.github.io/posts/further-into-backpropagation/                                               5/74/5/24, 8:25 PM                                                         Further-into-backpropagation\n  #backprop starts\n  temp1=np.copy(y_o)\n  #implementation of derivative of cost function with respect to y_o\n  temp1[range(num_examples),ycap]=1/-(temp1[range(num_examples),ycap])\n  temp=np.zeros_like(y_o)\n  temp[range(num_examples),ycap]=1\n  #derivative of cost with respect to y_o\n  dcost=np.multiply(temp,temp1)\n  #derivative of y_o with respect to y\n  dy_o=derivative_sigmoid(y)\n  #element-wise multiplication\n  dgrad=np.multiply(dcost,dy_o)\n  dw=np.dot(X.T,dgrad)\n  #weight-update\n  W-=h*dw\n  #forward prop again with updated weight to find new loss\n  y=np.dot(X,W)\n  yo=sigmoid(y)\n  loss=-np.sum(np.log(yo[range(num_examples),ycap]))\n  print(loss)            #outpus 7.63 (again for you it would be different!)", "start_char_idx": 2081, "end_char_idx": 3011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-4", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-4": {"__data__": {"id_": "node-4", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ff6e428-f8cf-4eeb-804e-19f93cab97d4", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "04b91a201da822821acf57c9f425f56accbc39a6b42b39e09cd3d706cee6897d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6190969d-d209-4912-8c24-aad8ffccf989", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "4e61b429f6e40ce63865d810b19030939521b8409490e1f780cfd3ed5ac7902c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4def8c10-6fd3-483b-9e35-792ebca91539", "node_type": "1", "metadata": {}, "hash": "39d7fc1b8758de1fd7eb3b445ffadc14c18c3934c33f88435ecc7459a9c62602", "class_name": "RelatedNodeInfo"}}, "text": "\u2202C          \u2202C       \u2202y o1      \u2202y 1\n                                     \u2202W  11  =   \u2202y o1  \u2217  \u2202y  1  \u2217 \u2202W   11\n  Putting the respective values we get-\n                                \u2202C         \u2212p1\n                               \u2202W  11  =    yo1   \u2217 \u03c3(y 1 ) \u2217 (1 \u2212  \u03c3(y  1)) \u2217 x 1\n  Similarly we can write this for all the W\u02bcs and place them in a matrix as-\n                       p1                                          p2\n                \u23a1 \u2212(  yo1 ) \u2217 \u03c3(y 1) \u2217 (1 \u2212   \u03c3(y 1 )) \u2217 x 1   \u2212(  yo2) \u2217 \u03c3(y  2) \u2217 (1 \u2212   \u03c3(y 2)) \u2217 x 1 \u23a4\n      \u2202C        \u23a2 \u2212(   p1 ) \u2217 \u03c3(y  ) \u2217 (1 \u2212   \u03c3(y   )) \u2217 x     \u2212(  p2 ) \u2217 \u03c3(y   ) \u2217 (1 \u2212   \u03c3(y  )) \u2217 x   \u23a5\n            =   \u23a2     yo1         1               1        2       yo2         2               2       2 \u23a5\n      \u2202W        \u23a2      p1                                          p2                                    \u23a5\n                \u23a3 \u2212(  yo1 ) \u2217 \u03c3(y 1) \u2217 (1 \u2212   \u03c3(y 1 )) \u2217 x 3   \u2212(  yo2) \u2217 \u03c3(y  2) \u2217 (1 \u2212   \u03c3(y 2)) \u2217 x 3 \u23a6\n  Observe the above matrix.It is nothing but the combination\n                                           X  T \u00d7 (   \u2202C   \u2299   \u2202yo  )\n                                                     \u2202y  o     \u2202y\n  where  denotes transpose of matrix anddenotes matrix product whiledenotes element-wise product. Now we can put this\n       T                            \u00d7                          \u2299\n  expression in our update rule:  W   =  W   \u2212  h  \u2217 X  T \u00d7 (  \u2202C   \u2299   \u2202yo  )\n  The python representation can be given as:                   \u2202y o      \u2202y\n  import  numpy as np\n  import  random\n  def sigmoid(x):\n           return 1/(1+np.exp(-x))\n  def derivative_sigmoid(x):\n           return np.multiply(sigmoid(x),(1-sigmoid(x)))\n  #initialization\n  X=np.matrix(\"2,4,-2\")\n  W=np.random.normal(size=(3,2))\n  #label\n  ycap=[0]\n  #number of training of examples\n  num_examples=1\n  #step size\n  h=.01\n  #forward-propogation\n  y=np.dot(X,W)\n  y_o=sigmoid(y)\n  #loss calculation\n  loss=-np.sum(np.log(y_o[range(num_examples),ycap]))\n  print(loss)      #outputs 7.87 (for you it would be different due to random initialization of weights.)\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                               5/74/5/24, 8:25 PM                                                         Further-into-backpropagation\n  #backprop starts\n  temp1=np.copy(y_o)\n  #implementation of derivative of cost function with respect to y_o\n  temp1[range(num_examples),ycap]=1/-(temp1[range(num_examples),ycap])\n  temp=np.zeros_like(y_o)\n  temp[range(num_examples),ycap]=1\n  #derivative of cost with respect to y_o\n  dcost=np.multiply(temp,temp1)\n  #derivative of y_o with respect to y\n  dy_o=derivative_sigmoid(y)\n  #element-wise multiplication\n  dgrad=np.multiply(dcost,dy_o)\n  dw=np.dot(X.T,dgrad)\n  #weight-update\n  W-=h*dw\n  #forward prop again with updated weight to find new loss\n  y=np.dot(X,W)\n  yo=sigmoid(y)\n  loss=-np.sum(np.log(yo[range(num_examples),ycap]))\n  print(loss)            #outpus 7.63 (again for you it would be different!)", "start_char_idx": 12018, "end_char_idx": 15029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-4", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "8afee5d1-d768-4861-af91-66751790cf65": {"__data__": {"id_": "8afee5d1-d768-4861-af91-66751790cf65", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-5", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "5e6a3541c5e470c98ac67f4727e04baeffad4a5a9a52d9108addd36f1bae2cd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edb06376-eee7-4d47-b83c-d7d9ca7137b9", "node_type": "1", "metadata": {}, "hash": "57141195bbb65edc4cbf006298a8152e078cab2f322642b1253cbaeb80e32dd5", "class_name": "RelatedNodeInfo"}}, "text": "Our cost function decreases from 7.87 to 7.63 a\u25afer one iteration of backpropagation.Above program shows only one iteration of\n  backpropagation and can be extended to multiple iterations to minimize the cost function.All the above matrix representations are\n  valid for multiple inputs too.With increase in number of inputs,number of rows in input matrix would increase.\n  My aim for writing this post was to enable you to apply backpropagation to neural networks.Also I wanted you to see the transition\n  between dealing with variables and dealing with matrices.Enough for this time!Enjoy!\n   ALSO ON JASDEEP06                                       Posted on 19 January,", "start_char_idx": 0, "end_char_idx": 671, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "edb06376-eee7-4d47-b83c-d7d9ca7137b9": {"__data__": {"id_": "edb06376-eee7-4d47-b83c-d7d9ca7137b9", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-5", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "5e6a3541c5e470c98ac67f4727e04baeffad4a5a9a52d9108addd36f1bae2cd7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8afee5d1-d768-4861-af91-66751790cf65", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "ed83a4c93939c996c4cf40fd09e6d80c62568fd2da95d807b17bde8e91cb75e1", "class_name": "RelatedNodeInfo"}}, "text": "2017\n    Undcrstanding LSTMin         Lets-Practice               Variable sharing-in-        Getting started with\n    Understanding LSTM in\n    Understanding LSTM in        Lets-Practice-\n                                 Lets-Practice-              Variable-sharing-in-\n                                                             Variable-sharing-in-        Getting started with\n                                                                                         Getting started with\n    ensorflow                    Backpropagation             ensorflov                   Tensorflov\n    Tensor\u25afow\n    Tensor\u25afow                    Backpropagation\n                                 Backpropagation             Tensor\u25afow\n                                                             Tensor\u25afow                   Tensor\u25afow\n    7 years ago32 comments                                                               Tensor\u25afow\n              \u2022                  7 years ago4 comments\n                                          \u2022                  7 years ago14 comments\n                                                                      \u2022                  7 years ago3 comments\n                                                                                                  \u2022\n    CNNs in Tensorflow(cifar-    Lets-practice-              Tensorflow: Variable sharingTensorflow : Getting Started\n    10)                          backpropagation             in Tensorflow               with Tensorflow\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                                       6/7", "start_char_idx": 671, "end_char_idx": 2304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b2ae3759-ac8c-44cf-ba47-7777faea228b": {"__data__": {"id_": "b2ae3759-ac8c-44cf-ba47-7777faea228b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-5", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "5e6a3541c5e470c98ac67f4727e04baeffad4a5a9a52d9108addd36f1bae2cd7", "class_name": "RelatedNodeInfo"}}, "text": "Our cost function decreases from 7.87 to 7.63 a\u25afer one iteration of backpropagation.Above program shows only one iteration of\n  backpropagation and can be extended to multiple iterations to minimize the cost function.All the above matrix representations are\n  valid for multiple inputs too.With increase in number of inputs,number of rows in input matrix would increase.\n  My aim for writing this post was to enable you to apply backpropagation to neural networks.Also I wanted you to see the transition\n  between dealing with variables and dealing with matrices.Enough for this time!Enjoy!\n   ALSO ON JASDEEP06                                       Posted on 19 January,2017\n    Undcrstanding LSTMin         Lets-Practice               Variable sharing-in-        Getting started with\n    Understanding LSTM in\n    Understanding LSTM in        Lets-Practice-\n                                 Lets-Practice-              Variable-sharing-in-\n                                                             Variable-sharing-in-        Getting started with\n                                                                                         Getting started with\n    ensorflow                    Backpropagation             ensorflov                   Tensorflov\n    Tensor\u25afow\n    Tensor\u25afow                    Backpropagation\n                                 Backpropagation             Tensor\u25afow\n                                                             Tensor\u25afow                   Tensor\u25afow\n    7 years ago32 comments                                                               Tensor\u25afow\n              \u2022                  7 years ago4 comments\n                                          \u2022                  7 years ago14 comments\n                                                                      \u2022                  7 years ago3 comments\n                                                                                                  \u2022\n    CNNs in Tensorflow(cifar-    Lets-practice-              Tensorflow: Variable sharingTensorflow : Getting Started\n    10)                          backpropagation             in Tensorflow               with Tensorflow\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                                       6/7", "start_char_idx": 0, "end_char_idx": 2304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-5": {"__data__": {"id_": "node-5", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ff6e428-f8cf-4eeb-804e-19f93cab97d4", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "04b91a201da822821acf57c9f425f56accbc39a6b42b39e09cd3d706cee6897d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b217e96-7acf-486d-8156-c9e12dc3720a", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}, "hash": "2a671606438c10f66f1c1259c82018e9a9d79b6276c61da10692547d09eef270", "class_name": "RelatedNodeInfo"}}, "text": "Our cost function decreases from 7.87 to 7.63 a\u25afer one iteration of backpropagation.Above program shows only one iteration of\n  backpropagation and can be extended to multiple iterations to minimize the cost function.All the above matrix representations are\n  valid for multiple inputs too.With increase in number of inputs,number of rows in input matrix would increase.\n  My aim for writing this post was to enable you to apply backpropagation to neural networks.Also I wanted you to see the transition\n  between dealing with variables and dealing with matrices.Enough for this time!Enjoy!\n   ALSO ON JASDEEP06                                       Posted on 19 January,2017\n    Undcrstanding LSTMin         Lets-Practice               Variable sharing-in-        Getting started with\n    Understanding LSTM in\n    Understanding LSTM in        Lets-Practice-\n                                 Lets-Practice-              Variable-sharing-in-\n                                                             Variable-sharing-in-        Getting started with\n                                                                                         Getting started with\n    ensorflow                    Backpropagation             ensorflov                   Tensorflov\n    Tensor\u25afow\n    Tensor\u25afow                    Backpropagation\n                                 Backpropagation             Tensor\u25afow\n                                                             Tensor\u25afow                   Tensor\u25afow\n    7 years ago32 comments                                                               Tensor\u25afow\n              \u2022                  7 years ago4 comments\n                                          \u2022                  7 years ago14 comments\n                                                                      \u2022                  7 years ago3 comments\n                                                                                                  \u2022\n    CNNs in Tensorflow(cifar-    Lets-practice-              Tensorflow: Variable sharingTensorflow : Getting Started\n    10)                          backpropagation             in Tensorflow               with Tensorflow\nhttps://jasdeep06.github.io/posts/further-into-backpropagation/                                                                       6/7", "start_char_idx": 15032, "end_char_idx": 17336, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f859dcd1-69c1-4735-ba0e-b85f23d56f74": {"__data__": {"id_": "f859dcd1-69c1-4735-ba0e-b85f23d56f74", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Further into backpropagation", "path": "cache\\blogposts\\Further-into-backpropagation\\parsed\\images\\abd1d523-758c-4765-845b-172ebabcd2d9-img_p0_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being discussed is a two-layered neural network, which is a fundamental structure in machine learning for pattern recognition and data classification. The network described has an input layer with three nodes, which can be thought of as the entry points for data with three dimensions or features. These nodes are connected to a second layer, which has two nodes. The connections between the nodes represent weights, which are adjustable parameters that the network learns during training.\n\nIn this network, the sigmoid function plays a crucial role. It is a type of activation function that takes the weighted sum of the inputs and normalizes the output to a value between 0 and 1. This is important for binary classification problems, where the output is often interpreted as a probability.\n\nThe process of training the network involves adjusting the weights to minimize the difference between the predicted output and the actual output. This difference is quantified by a cost function, and in the context being discussed, the cross-entropy cost function is used. The cross-entropy cost function is particularly useful when the outputs can be interpreted as probabilities, as it penalizes predictions that are confident but wrong.\n\nThe aim of the network is to classify inputs into different classes, which are represented by the nodes in the output layer. During training, each input is associated with a label indicating its true class, and the network's job is to predict these labels as accurately as possible. The backpropagation algorithm is used to efficiently compute the gradient of the cost function with respect to the", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e590ce6a-e0f1-4741-acb7-c4b0831b852f": {"__data__": {"id_": "e590ce6a-e0f1-4741-acb7-c4b0831b852f", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Further into backpropagation", "path": "cache\\blogposts\\Further-into-backpropagation\\parsed\\images\\abd1d523-758c-4765-845b-172ebabcd2d9-img_p5_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you provided seems to be a tutorial or an educational post about backpropagation, which is a fundamental algorithm used to train neural networks. The text includes a snippet of Python code that demonstrates how backpropagation is implemented to update the weights of a neural network in order to minimize the cost function. The code uses numpy for matrix operations and implements the derivative of the cost function with respect to the output, the derivative of the output with respect to the inputs (using the derivative of the sigmoid function), and the weight update rule.\n\nThe code snippet concludes with a forward propagation using the updated weights to calculate the new loss, which shows a decrease from 7.87 to 7.63, indicating that the backpropagation algorithm is working as intended and the neural network is learning.\n\nThe post also mentions that the matrix representations in the code are valid for multiple inputs, meaning that the code can handle batch training where multiple examples are processed at once. The increase in the number of inputs would correspond to an increase in the number of rows in the input matrix.\n\nAdditionally, the text includes links to other related topics on the same website, such as understanding Long Short-Term Memory (LSTM) networks in TensorFlow, practicing backpropagation, and getting started with TensorFlow. These links suggest that the website provides a series of tutorials on machine learning and neural networks using TensorFlow, which is a popular open-source library for numerical computation and machine learning.\n\nThe date and the number of comments on various posts indicate", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"21c10818-190a-4f44-8b72-57d60e7a4848": {"doc_hash": "3c929206a355253e76165f345cd07bd7881ea6431e03f204851ee39405ae1eae", "ref_doc_id": "node-0"}, "30b203de-aac4-48dd-9bac-387073c4f3e9": {"doc_hash": "7e52512aa311d825e33bd81cdd9c82758e6de97014cc8b656b2ed5c189c38b24", "ref_doc_id": "node-0"}, "6ded9423-6975-4869-b9c9-dadad69ee987": {"doc_hash": "2d87907044ff99156c539a372d99da40d3336c3ab3453347ab1d0d171202b026", "ref_doc_id": "node-0"}, "f89de763-2c31-4a29-873e-f5f38be7e3be": {"doc_hash": "830d09cac0271ee9f2040c8ce300596261825309a22b1f4dc88bf07b4831804c", "ref_doc_id": "node-0"}, "16f135a9-0e8c-492c-b42e-bf03b88dbbbf": {"doc_hash": "1b3c144c50bf62eb75a01468cdf86194945a0ffb327849a1f61281c98f53407f", "ref_doc_id": "node-0"}, "80b9fe56-26ad-40d0-bc43-a8e394a7c4fd": {"doc_hash": "beeab6275cf3f3d6c7f79ae1d155fc87a60d5f33ecdce3149458a1a2783ad8ef", "ref_doc_id": "node-0"}, "7601ec8b-65f7-4cdb-885a-18115363d231": {"doc_hash": "8f61538787bb3988aada95f884440f902a135a271583c11cf51fbc2b04297100", "ref_doc_id": "node-0"}, "5c4657f1-b2d9-4e0a-9383-77eb9cffdc88": {"doc_hash": "1b3c144c50bf62eb75a01468cdf86194945a0ffb327849a1f61281c98f53407f", "ref_doc_id": "node-0"}, "node-0": {"doc_hash": "0954b0c789c1cfcc3c326a568e402b163f195797f14739cb62e39ad029fc0414", "ref_doc_id": "3ff6e428-f8cf-4eeb-804e-19f93cab97d4"}, "3a5e81ec-b17c-43db-a9b4-358172d27636": {"doc_hash": "1b811dd8865fda4c9071c3b4887ee6135c29fe90ac61ed4badb27baff346c36a", "ref_doc_id": "node-1"}, "ea93c57e-c214-418d-bd69-3eb6773b4bb4": {"doc_hash": "a8c014578a1d2835ac527fab232fff37d173b56308b7f570d02b42a1c5a93cfd", "ref_doc_id": "node-1"}, "278a326d-1033-4b3e-8697-3d7d0ceb456e": {"doc_hash": "24df268717e4753b4f98c05f99f4ef6f68f7103e2f2ae108090669f3936b7781", "ref_doc_id": "node-1"}, "faa5511e-4faf-425f-8a64-bb7015069bc2": {"doc_hash": "e3b8bbea820ae1d2dab96ab670e56fe613e3e812d015bb5c5a7d8fb973cfc5e6", "ref_doc_id": "node-1"}, "327c823a-c057-497f-9def-d120b4e3cc77": {"doc_hash": "3b0e194fef42b1de2b5093d0a69701354e890ab92f53d4a39567f223203cdf02", "ref_doc_id": "node-1"}, "ef858ddc-60b2-4a6f-a16e-e5ee92ee1343": {"doc_hash": "5cce55ae03c561119f8bd37896d411f30965ce79b0c0a19d9a7a1558a309ead5", "ref_doc_id": "node-1"}, "28bd0c05-db02-499d-9030-1d46c7ad1cf8": {"doc_hash": "3b0e194fef42b1de2b5093d0a69701354e890ab92f53d4a39567f223203cdf02", "ref_doc_id": "node-1"}, "node-1": {"doc_hash": "9cc8941baeabb09c2684d36820a9311cebe19fd51acd8df553c89f891bfc9c5a", "ref_doc_id": "3ff6e428-f8cf-4eeb-804e-19f93cab97d4"}, "5e528b2f-1d9c-4385-91a9-c0a0fb039ef1": {"doc_hash": "eee7c6ed29d17d835df8db191a8d879277208f17236762852961136701141113", "ref_doc_id": "node-2"}, "3e931283-9daf-498b-a1cf-10f00e6b3519": {"doc_hash": "1117b43597255954f54ac3ce3840df60cb7a27ae3b8b659c09dccb4b726cb545", "ref_doc_id": "node-2"}, "2dff36fc-0a82-4c3d-90af-14072fa277f9": {"doc_hash": "7b8c12b1bae244efc2bf035124e7cca5aad3f413328734f18f92ec497db427cb", "ref_doc_id": "node-2"}, "eb932cc1-0060-4770-a948-5dba645c3f86": {"doc_hash": "f281cfd80c65c0378dd28c0ba168c29ca33c0586598a63d1506c0cfe5741590b", "ref_doc_id": "node-2"}, "2dd516de-dd92-4d5f-9dd4-054d5ddcbad7": {"doc_hash": "7cd52eee0fcb79dbc42e32a262f2f92b44c08bc7875350cb0899721a9a54bda0", "ref_doc_id": "node-2"}, "node-2": {"doc_hash": "a5014e6bb579857adafb84d3cc339a36e602937d12d7d0882b0782b9482a4348", "ref_doc_id": "3ff6e428-f8cf-4eeb-804e-19f93cab97d4"}, "fa38b574-d6b7-42ea-ab84-1ad8adff3e1a": {"doc_hash": "24e8826b98a3d4ef310062e61df2e04b1e55f3e933e0eb265e06518905efdcd8", "ref_doc_id": "node-3"}, "204a8a0c-f79d-4dc5-8b65-bf51c153d65e": {"doc_hash": "5bce2a837bc1ce552e71c730cf1af1268a0236bdc390ffbd9df7fb3794017d75", "ref_doc_id": "node-3"}, "271e2be6-222c-4499-9703-dbb07c8c8e16": {"doc_hash": "3f86982c1ebfe9ef1603028aab35afc4b0353c5e962166a1aa52cfb903d61377", "ref_doc_id": "node-3"}, "b851c39c-3cb3-4288-a388-be0ffab814a4": {"doc_hash": "124f6470d35fadc1efdb90da7bcdb4ea5c361c7d1b71bb035fbdf222f9a0abea", "ref_doc_id": "node-3"}, "eec3195e-b2af-4122-97aa-30af58baec20": {"doc_hash": "e95200f86f19c3ab7ba03bebd87c493e206fe984387a0994c2b07f4cb9f8909c", "ref_doc_id": "node-3"}, "14faa69d-f366-4491-a511-3758f463eca7": {"doc_hash": "1f50f3fd041175a163653fbd9a0e174383f67c7b47e8e8dde37cfaad0a7e377a", "ref_doc_id": "node-3"}, "58a3edee-3a09-45b1-9e83-c5676d068f5c": {"doc_hash": "e95200f86f19c3ab7ba03bebd87c493e206fe984387a0994c2b07f4cb9f8909c", "ref_doc_id": "node-3"}, "node-3": {"doc_hash": "4e61b429f6e40ce63865d810b19030939521b8409490e1f780cfd3ed5ac7902c", "ref_doc_id": "3ff6e428-f8cf-4eeb-804e-19f93cab97d4"}, "e50adabb-1984-4574-bb4f-691efe6ccc01": {"doc_hash": "b8485672465b0880d867a197f950008df07c54aac24934fb85710fb67ca4884d", "ref_doc_id": "node-4"}, "d49c0ae8-bb63-4a13-bf0e-2ac2395feb24": {"doc_hash": "3aece44253f1df78704ebf30edc1ee6efe03ac36fc059bcd877c24fd88b2e902", "ref_doc_id": "node-4"}, "0cb07794-933b-4694-8298-9d4dd68a3a6c": {"doc_hash": "bfbf9b9729fc8ebd65caed7a62125ed5f16f30e79f591f86d71d5dd16d21873d", "ref_doc_id": "node-4"}, "38709b63-cc3d-48cb-a100-2f0988dd64ea": {"doc_hash": "b184d74f3f22b28a7842b4283ec4dfa74bb3c43017d8c44f345f88370bcdd3f9", "ref_doc_id": "node-4"}, "49a49168-e921-4fd1-8c1d-65de5e9175ad": {"doc_hash": "e7fcf47a64032a582f40faf1613a132e6e6c64a2445f1d8514f11b7607842036", "ref_doc_id": "node-4"}, "e73be0ac-ed64-44da-8fbb-468a7bf4dc19": {"doc_hash": "a251709a0fa5e27978d7235b74ed8710fe170158c0e78b4c2819a21dfc27faf0", "ref_doc_id": "node-4"}, "3cf28b88-2d41-4c91-9e2c-c11847a6a053": {"doc_hash": "49a850c9d051acc7a00d8ca5b775e3a548432240234c64b0de50348c2b58fa14", "ref_doc_id": "node-4"}, "3916ef68-de36-4fbb-8fe2-d072df0a7d72": {"doc_hash": "bf8deb754d80ee0160b93b94526b0eb6db4d790329bdd943b3287c899caf5449", "ref_doc_id": "node-4"}, "f4e79706-075a-4e11-8b32-22ed35d9ac19": {"doc_hash": "31cb99a7e6e689c4210dd61367564bcbb4d2753439fe06aad85a0f90f5cb143f", "ref_doc_id": "node-4"}, "node-4": {"doc_hash": "2a671606438c10f66f1c1259c82018e9a9d79b6276c61da10692547d09eef270", "ref_doc_id": "3ff6e428-f8cf-4eeb-804e-19f93cab97d4"}, "8afee5d1-d768-4861-af91-66751790cf65": {"doc_hash": "ed83a4c93939c996c4cf40fd09e6d80c62568fd2da95d807b17bde8e91cb75e1", "ref_doc_id": "node-5"}, "edb06376-eee7-4d47-b83c-d7d9ca7137b9": {"doc_hash": "ee46cd5b422738739eef1d2062b9bbed31fb60ebba33c573a84fec4f25fab333", "ref_doc_id": "node-5"}, "b2ae3759-ac8c-44cf-ba47-7777faea228b": {"doc_hash": "5e6a3541c5e470c98ac67f4727e04baeffad4a5a9a52d9108addd36f1bae2cd7", "ref_doc_id": "node-5"}, "node-5": {"doc_hash": "5e6a3541c5e470c98ac67f4727e04baeffad4a5a9a52d9108addd36f1bae2cd7", "ref_doc_id": "3ff6e428-f8cf-4eeb-804e-19f93cab97d4"}, "f859dcd1-69c1-4735-ba0e-b85f23d56f74": {"doc_hash": "2ee92a3f687d806cbe8d64e33ee46e2e9ab04a81c3c72be8ae83b0d854e005e1"}, "e590ce6a-e0f1-4741-acb7-c4b0831b852f": {"doc_hash": "7270ee0cd31e0d1a66d215a127ddac20d72fa691c29eebaa97df9dbd43732d57"}}, "docstore/ref_doc_info": {"node-0": {"node_ids": ["21c10818-190a-4f44-8b72-57d60e7a4848", "30b203de-aac4-48dd-9bac-387073c4f3e9", "6ded9423-6975-4869-b9c9-dadad69ee987", "f89de763-2c31-4a29-873e-f5f38be7e3be", "16f135a9-0e8c-492c-b42e-bf03b88dbbbf", "80b9fe56-26ad-40d0-bc43-a8e394a7c4fd", "7601ec8b-65f7-4cdb-885a-18115363d231", "5c4657f1-b2d9-4e0a-9383-77eb9cffdc88"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}}, "3ff6e428-f8cf-4eeb-804e-19f93cab97d4": {"node_ids": ["node-0", "node-1", "node-2", "node-3", "node-4", "node-5"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}}, "node-1": {"node_ids": ["3a5e81ec-b17c-43db-a9b4-358172d27636", "ea93c57e-c214-418d-bd69-3eb6773b4bb4", "278a326d-1033-4b3e-8697-3d7d0ceb456e", "faa5511e-4faf-425f-8a64-bb7015069bc2", "327c823a-c057-497f-9def-d120b4e3cc77", "ef858ddc-60b2-4a6f-a16e-e5ee92ee1343", "28bd0c05-db02-499d-9030-1d46c7ad1cf8"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}}, "node-2": {"node_ids": ["5e528b2f-1d9c-4385-91a9-c0a0fb039ef1", "3e931283-9daf-498b-a1cf-10f00e6b3519", "2dff36fc-0a82-4c3d-90af-14072fa277f9", "eb932cc1-0060-4770-a948-5dba645c3f86", "2dd516de-dd92-4d5f-9dd4-054d5ddcbad7"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}}, "node-3": {"node_ids": ["fa38b574-d6b7-42ea-ab84-1ad8adff3e1a", "204a8a0c-f79d-4dc5-8b65-bf51c153d65e", "271e2be6-222c-4499-9703-dbb07c8c8e16", "b851c39c-3cb3-4288-a388-be0ffab814a4", "eec3195e-b2af-4122-97aa-30af58baec20", "14faa69d-f366-4491-a511-3758f463eca7", "58a3edee-3a09-45b1-9e83-c5676d068f5c"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}}, "node-4": {"node_ids": ["e50adabb-1984-4574-bb4f-691efe6ccc01", "d49c0ae8-bb63-4a13-bf0e-2ac2395feb24", "0cb07794-933b-4694-8298-9d4dd68a3a6c", "38709b63-cc3d-48cb-a100-2f0988dd64ea", "49a49168-e921-4fd1-8c1d-65de5e9175ad", "e73be0ac-ed64-44da-8fbb-468a7bf4dc19", "3cf28b88-2d41-4c91-9e2c-c11847a6a053", "3916ef68-de36-4fbb-8fe2-d072df0a7d72", "f4e79706-075a-4e11-8b32-22ed35d9ac19"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}}, "node-5": {"node_ids": ["8afee5d1-d768-4861-af91-66751790cf65", "edb06376-eee7-4d47-b83c-d7d9ca7137b9", "b2ae3759-ac8c-44cf-ba47-7777faea228b"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Further into backpropagation"}}}}