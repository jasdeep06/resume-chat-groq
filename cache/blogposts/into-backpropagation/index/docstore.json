{"docstore/data": {"5ebeed2e-7082-46b6-be37-027975ddc369": {"__data__": {"id_": "5ebeed2e-7082-46b6-be37-027975ddc369", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "10d093599ee056744b395c9809937279bc764227efc95b6aa3d17c0cd3fb5d61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "329b576d-a741-42d5-937c-435611ef3247", "node_type": "1", "metadata": {}, "hash": "06a1446139f5885228ca4d906d9e3cb11a65c13ed28c31f780b8dfb20b775d65", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:22 PM                                 into-backpropagation\n Backpropagation                                                                      HOME\n Into-Backpropagation\n In the previous post,we learnt to appreciate the beauty of derivatives and their e\u25afect\n on update rule which is given by-a  =   a  +   h  \u2217 \u2202a\u2202f\n                                                      \u2202f\n where                            b  =   b  +  h   \u2217 \u2202b\n                                      f  =   f(a,    b)\n Although the case of single node system helps us capture the intuition behind the\n e\u25afect of change in input on the output of node,it is pretty useless if considered in\n isolation.We need to scale up our network.Let us consider case of nested nodes as\n shown below-\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                     1/74/5/24, 8:22 PM                                        into-backpropagation\n  Here there are two nodes,", "start_char_idx": 0, "end_char_idx": 961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "329b576d-a741-42d5-937c-435611ef3247": {"__data__": {"id_": "329b576d-a741-42d5-937c-435611ef3247", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "10d093599ee056744b395c9809937279bc764227efc95b6aa3d17c0cd3fb5d61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ebeed2e-7082-46b6-be37-027975ddc369", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "71d1d97b093c095a377123add52c89a52cf773e9627ccd5bf82f1cf5af46d709", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc34d39b-de01-46b3-89a6-8b8abe2bc834", "node_type": "1", "metadata": {}, "hash": "66ad21e13f78f1f20ba1b1f2614044881f08154f4e512e8c43db7176051b2127", "class_name": "RelatedNodeInfo"}}, "text": "out of which the first one(from the le\u25af) accepts two inputsa\n  and  b and performs addition operation on them to return the outputd.The second\n  node accepts    d and a new input     c as inputs and performs product operation on them to\n  gives f as final output. This system can be represented in python:\n  def   product(x,y):\n               return     x*y\n  def   addition(x,y):\n  a=5          return     x+y\n  b=-3\n  c=-2\n  d=addition(a,b)\n  f=product(d,c)           #outputs -4\n  Aim\n  Our aim is still the same as was in last post viz;we want to manipulate the values of our\n  inputs  a,b,c  in such a way that the value of output      f  increases.", "start_char_idx": 961, "end_char_idx": 1615, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "cc34d39b-de01-46b3-89a6-8b8abe2bc834": {"__data__": {"id_": "cc34d39b-de01-46b3-89a6-8b8abe2bc834", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "10d093599ee056744b395c9809937279bc764227efc95b6aa3d17c0cd3fb5d61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "329b576d-a741-42d5-937c-435611ef3247", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "44f26904514a01a751621f944d47b60a9fc9baeaac21fd271c1bd60f7789b776", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d9a6889-0b47-4010-b9d2-7decd31218a0", "node_type": "1", "metadata": {}, "hash": "f1adbd21c2c1be0f216ec9986de6e507c30fd0ec1b04172276f323ef41bc54b7", "class_name": "RelatedNodeInfo"}}, "text": "Not only will we achieve the above aim but in that process we will slowly slide into\n  backpropagation and go through the concept intuitively.Note that this post will be\n  slightly more mathematical than the last one but all the concepts used are described\n  intuitively in the  previous post.\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                        2/74/5/24, 8:22 PM                                    into-backpropagation\n Lets get started!!", "start_char_idx": 1618, "end_char_idx": 2101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "8d9a6889-0b47-4010-b9d2-7decd31218a0": {"__data__": {"id_": "8d9a6889-0b47-4010-b9d2-7decd31218a0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "10d093599ee056744b395c9809937279bc764227efc95b6aa3d17c0cd3fb5d61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc34d39b-de01-46b3-89a6-8b8abe2bc834", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "e8faa51785c1e56a44a06a280d03d722b26fe4508f79112a5f4fe63135fd1055", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "346f59a2-4b6d-4d2a-80bc-bdfa0dc37825", "node_type": "1", "metadata": {}, "hash": "f8f49c0cdd06568997c25e3af89ce23bc333bebfbf0e8c9318fdcafb87c69c9b", "class_name": "RelatedNodeInfo"}}, "text": "This nested system might seem a bit intimidating at first.Where do we start?Well,we\n know the update rules from the last post that involve derivatives of output with respect\n to input.Let us list down these update rules for our inputs     a,band   c-\n                                   a   =   a  +   h   \u2217 \u2202a\u2202f\n                                                         \u2202f\n                                    b  =   b  +   h   \u2217 \u2202b\n                                                         \u2202f\n                                    c  =   c  +   h   \u2217 \u2202c  \u2202f    \u2202f        \u2202f\n We somehow want to compute the three derivatives           \u2202a  , \u2202b  and   \u2202c\n Let us look at the relations among various variables in our system.We can easily write-\n                                          f  =   d  \u2217  c\n                                         d  =    a  +   b\n Now let us use the analytical gradient to calculate derivatives from the above relations.\n (Refer  Derivative rules).", "start_char_idx": 2103, "end_char_idx": 3074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "346f59a2-4b6d-4d2a-80bc-bdfa0dc37825": {"__data__": {"id_": "346f59a2-4b6d-4d2a-80bc-bdfa0dc37825", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "10d093599ee056744b395c9809937279bc764227efc95b6aa3d17c0cd3fb5d61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d9a6889-0b47-4010-b9d2-7decd31218a0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "1467c89c395cebfcc958216b07b17d295c364b898750ea8287f09e83810db6d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6864ad0d-835b-4acb-930c-35d08ee17fcd", "node_type": "1", "metadata": {}, "hash": "e35cb3cea5a4b490932774da064af957808f41af48b9834411417174f9a18d85", "class_name": "RelatedNodeInfo"}}, "text": "Consider the relation   f   =   d  \u2217  c\n Di\u25aferentiating this relation we get-      \u2202f\n                                            \u2202d    =    c\n                                           \u2202f\n                                            \u2202c    =   d\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                     3/74/5/24, 8:22 PM                                  into-backpropagation\n Di\u25aferentiating the relation  d   =   a  +\u2202d b we get-\n                                          \u2202a     = 1\n                                          \u2202d     = 1\n                                           \u2202b\n Observe that derivatives for addition node is 1.This makes intuitive sense too.If you try\n to increase input to an addition node by a quantity h,then the output value will\n increase by same quantity.Thus normalised change i.e. the derivative is 1.", "start_char_idx": 3076, "end_char_idx": 3941, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6864ad0d-835b-4acb-930c-35d08ee17fcd": {"__data__": {"id_": "6864ad0d-835b-4acb-930c-35d08ee17fcd", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "10d093599ee056744b395c9809937279bc764227efc95b6aa3d17c0cd3fb5d61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "346f59a2-4b6d-4d2a-80bc-bdfa0dc37825", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "f4a7eaafdba97841cf978a98747e2ce524db57999215488f434f53c63db66728", "class_name": "RelatedNodeInfo"}}, "text": "We now have the values of     \u2202f  ,\u2202f  , \u2202d  and  \u2202d  .We somehow have to use these\n                               \u2202d    \u2202c   \u2202a       \u2202b\n                                     \u2202f   \u2202f        \u2202f\n values to compute the values of     \u2202a  ,\u2202b   and  \u2202c  .", "start_char_idx": 3943, "end_char_idx": 4193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "68e60a50-9ea2-4300-91a2-e51cb7ef29d7": {"__data__": {"id_": "68e60a50-9ea2-4300-91a2-e51cb7ef29d7", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "10d093599ee056744b395c9809937279bc764227efc95b6aa3d17c0cd3fb5d61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba73a067-3b50-4839-8ed2-67f30a86712a", "node_type": "1", "metadata": {}, "hash": "dfaf1c7cf29539bfc0563c0eddd2329504cb5bbf9735d50750ee3e82532c1305", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:22 PM                                 into-backpropagation\n Backpropagation                                                                      HOME\n Into-Backpropagation\n In the previous post,we learnt to appreciate the beauty of derivatives and their e\u25afect\n on update rule which is given by-a  =   a  +   h  \u2217 \u2202a\u2202f\n                                                      \u2202f\n where                            b  =   b  +  h   \u2217 \u2202b\n                                      f  =   f(a,    b)\n Although the case of single node system helps us capture the intuition behind the\n e\u25afect of change in input on the output of node,it is pretty useless if considered in\n isolation.We need to scale up our network.Let us consider case of nested nodes as\n shown below-\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                     1/74/5/24, 8:22 PM                                        into-backpropagation\n  Here there are two nodes,out of which the first one(from the le\u25af) accepts two inputsa\n  and  b and performs addition operation on them to return the outputd.The second\n  node accepts    d and a new input     c as inputs and performs product operation on them to\n  gives f as final output. This system can be represented in python:\n  def   product(x,y):\n               return     x*y\n  def   addition(x,y):\n  a=5          return     x+y\n  b=-3\n  c=-2\n  d=addition(a,b)\n  f=product(d,c)           #outputs -4\n  Aim\n  Our aim is still the same as was in last post viz;we want to manipulate the values of our\n  inputs  a,b,c  in such a way that the value of output      f  increases.\n  Not only will we achieve the above aim but in that process we will slowly slide into\n  backpropagation and go through the concept intuitively.Note that this post will be\n  slightly more mathematical than the last one but all the concepts used are described\n  intuitively in the  previous post.\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                        2/74/5/24, 8:22 PM                                    into-backpropagation\n Lets get started!!", "start_char_idx": 0, "end_char_idx": 2101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ba73a067-3b50-4839-8ed2-67f30a86712a": {"__data__": {"id_": "ba73a067-3b50-4839-8ed2-67f30a86712a", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "10d093599ee056744b395c9809937279bc764227efc95b6aa3d17c0cd3fb5d61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68e60a50-9ea2-4300-91a2-e51cb7ef29d7", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "0ed6f2548080a057df66cad916f9d4ae9ae3edbedfe24f2ff656f11cf5d2082e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dec2bf1d-acdd-4afa-bd57-4f5de085f5de", "node_type": "1", "metadata": {}, "hash": "e35cb3cea5a4b490932774da064af957808f41af48b9834411417174f9a18d85", "class_name": "RelatedNodeInfo"}}, "text": "This nested system might seem a bit intimidating at first.Where do we start?Well,we\n know the update rules from the last post that involve derivatives of output with respect\n to input.Let us list down these update rules for our inputs     a,band   c-\n                                   a   =   a  +   h   \u2217 \u2202a\u2202f\n                                                         \u2202f\n                                    b  =   b  +   h   \u2217 \u2202b\n                                                         \u2202f\n                                    c  =   c  +   h   \u2217 \u2202c  \u2202f    \u2202f        \u2202f\n We somehow want to compute the three derivatives           \u2202a  , \u2202b  and   \u2202c\n Let us look at the relations among various variables in our system.We can easily write-\n                                          f  =   d  \u2217  c\n                                         d  =    a  +   b\n Now let us use the analytical gradient to calculate derivatives from the above relations.\n (Refer  Derivative rules).\n Consider the relation   f   =   d  \u2217  c\n Di\u25aferentiating this relation we get-      \u2202f\n                                            \u2202d    =    c\n                                           \u2202f\n                                            \u2202c    =   d\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                     3/74/5/24, 8:22 PM                                  into-backpropagation\n Di\u25aferentiating the relation  d   =   a  +\u2202d b we get-\n                                          \u2202a     = 1\n                                          \u2202d     = 1\n                                           \u2202b\n Observe that derivatives for addition node is 1.This makes intuitive sense too.If you try\n to increase input to an addition node by a quantity h,then the output value will\n increase by same quantity.Thus normalised change i.e. the derivative is 1.", "start_char_idx": 2103, "end_char_idx": 3941, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "dec2bf1d-acdd-4afa-bd57-4f5de085f5de": {"__data__": {"id_": "dec2bf1d-acdd-4afa-bd57-4f5de085f5de", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "10d093599ee056744b395c9809937279bc764227efc95b6aa3d17c0cd3fb5d61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba73a067-3b50-4839-8ed2-67f30a86712a", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "8076f3b0b59a6c289a64fd0bbbd35cd8341fcf7e0bf7fafde7d238c8c9b884da", "class_name": "RelatedNodeInfo"}}, "text": "We now have the values of     \u2202f  ,\u2202f  , \u2202d  and  \u2202d  .We somehow have to use these\n                               \u2202d    \u2202c   \u2202a       \u2202b\n                                     \u2202f   \u2202f        \u2202f\n values to compute the values of     \u2202a  ,\u2202b   and  \u2202c  .", "start_char_idx": 3943, "end_char_idx": 4193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-0": {"__data__": {"id_": "node-0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6b23c667-1796-4259-b4d6-c287bdb34ea5", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "42e8c0b4be3b36797ef2b88effa058d648e1e7c244b9b71e41068b4ade14adbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c78fc1f6-f023-418f-9adc-655bbf24d645", "node_type": "1", "metadata": {}, "hash": "28463a4092a41864fa190189e2ecfb1156ae9fad179013c87a9bed06e629c19e", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:22 PM                                 into-backpropagation\n Backpropagation                                                                      HOME\n Into-Backpropagation\n In the previous post,we learnt to appreciate the beauty of derivatives and their e\u25afect\n on update rule which is given by-a  =   a  +   h  \u2217 \u2202a\u2202f\n                                                      \u2202f\n where                            b  =   b  +  h   \u2217 \u2202b\n                                      f  =   f(a,    b)\n Although the case of single node system helps us capture the intuition behind the\n e\u25afect of change in input on the output of node,it is pretty useless if considered in\n isolation.We need to scale up our network.Let us consider case of nested nodes as\n shown below-\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                     1/74/5/24, 8:22 PM                                        into-backpropagation\n  Here there are two nodes,out of which the first one(from the le\u25af) accepts two inputsa\n  and  b and performs addition operation on them to return the outputd.The second\n  node accepts    d and a new input     c as inputs and performs product operation on them to\n  gives f as final output. This system can be represented in python:\n  def   product(x,y):\n               return     x*y\n  def   addition(x,y):\n  a=5          return     x+y\n  b=-3\n  c=-2\n  d=addition(a,b)\n  f=product(d,c)           #outputs -4\n  Aim\n  Our aim is still the same as was in last post viz;we want to manipulate the values of our\n  inputs  a,b,c  in such a way that the value of output      f  increases.\n  Not only will we achieve the above aim but in that process we will slowly slide into\n  backpropagation and go through the concept intuitively.Note that this post will be\n  slightly more mathematical than the last one but all the concepts used are described\n  intuitively in the  previous post.\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                        2/74/5/24, 8:22 PM                                    into-backpropagation\n Lets get started!!\n This nested system might seem a bit intimidating at first.Where do we start?Well,we\n know the update rules from the last post that involve derivatives of output with respect\n to input.Let us list down these update rules for our inputs     a,band   c-\n                                   a   =   a  +   h   \u2217 \u2202a\u2202f\n                                                         \u2202f\n                                    b  =   b  +   h   \u2217 \u2202b\n                                                         \u2202f\n                                    c  =   c  +   h   \u2217 \u2202c  \u2202f    \u2202f        \u2202f\n We somehow want to compute the three derivatives           \u2202a  , \u2202b  and   \u2202c\n Let us look at the relations among various variables in our system.We can easily write-\n                                          f  =   d  \u2217  c\n                                         d  =    a  +   b\n Now let us use the analytical gradient to calculate derivatives from the above relations.\n (Refer  Derivative rules).\n Consider the relation   f   =   d  \u2217  c\n Di\u25aferentiating this relation we get-      \u2202f\n                                            \u2202d    =    c\n                                           \u2202f\n                                            \u2202c    =   d\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                     3/74/5/24, 8:22 PM                                  into-backpropagation\n Di\u25aferentiating the relation  d   =   a  +\u2202d b we get-\n                                          \u2202a     = 1\n                                          \u2202d     = 1\n                                           \u2202b\n Observe that derivatives for addition node is 1.This makes intuitive sense too.If you try\n to increase input to an addition node by a quantity h,then the output value will\n increase by same quantity.Thus normalised change i.e. the derivative is 1.\n We now have the values of     \u2202f  ,\u2202f  , \u2202d  and  \u2202d  .We somehow have to use these\n                               \u2202d    \u2202c   \u2202a       \u2202b\n                                     \u2202f   \u2202f        \u2202f\n values to compute the values of     \u2202a  ,\u2202b   and  \u2202c  .", "start_char_idx": 0, "end_char_idx": 4193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b02a1438-8be4-4a67-8c4b-2ac17fb130cc": {"__data__": {"id_": "b02a1438-8be4-4a67-8c4b-2ac17fb130cc", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "dde2c8a148298254439ac7443200b2cd489a6a71f34c240c5ec5f8d2b04938fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "671e64c7-4287-4ded-b7b8-1b9f5b748b8d", "node_type": "1", "metadata": {}, "hash": "59d39b4fa860c5b70758a36fe252ab98f60be5d9497d414568604481ec24f9b7", "class_name": "RelatedNodeInfo"}}, "text": "\u2202f                                                                     \u2202f\n The value of   \u2202c  is already known.This leaves us with two unknown values viz:       \u2202a\n       \u2202f\n and   \u2202b  .\n Backpropagation\n Its time to introduce  Chain rule.No need to be intimidated by the name.Its pretty easy\n                                                                         \u2202f\n and straightforward.We know the derivative of      fwith respect to  d( \u2202d  ) and we also\n know the derivative of   dwith respect to  a( \u2202d  ).Chain rule tells us how we can combine\n                                               \u2202a                       \u2202f\n these two derivatives to find the derivative of  f with respect to  a( \u2202a  ).It simply states\n to multiply these two derivatives(or to chain them together)to get the derivative of     f\n with respect to  a.Mathematically- \u2202f          \u2202f      \u2202d\n                                    \u2202a     =    \u2202d   \u2217  \u2202a\nhttps://jasdeep06.github.", "start_char_idx": 0, "end_char_idx": 957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "671e64c7-4287-4ded-b7b8-1b9f5b748b8d": {"__data__": {"id_": "671e64c7-4287-4ded-b7b8-1b9f5b748b8d", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "dde2c8a148298254439ac7443200b2cd489a6a71f34c240c5ec5f8d2b04938fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b02a1438-8be4-4a67-8c4b-2ac17fb130cc", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "b42005074213c18c6689e64d80434f233423c42381058c68752831ab775f1ff1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89d7a00f-ecd2-4389-8cfd-0c3d57c79ab9", "node_type": "1", "metadata": {}, "hash": "5963c25b99300eff1f80c9514f4a309dcda552fc72d58a270de13d430cb303de", "class_name": "RelatedNodeInfo"}}, "text": "io/posts/into-backpropagation/                                        4/74/5/24, 8:22 PM                           into-backpropagation\n Similarly,                    \u2202f       \u2202f     \u2202d\n                               \u2202b   =   \u2202d   \u2217  \u2202b\n For this nested system we have already found the values of di\u25aferent derivatives.", "start_char_idx": 957, "end_char_idx": 1275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "89d7a00f-ecd2-4389-8cfd-0c3d57c79ab9": {"__data__": {"id_": "89d7a00f-ecd2-4389-8cfd-0c3d57c79ab9", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "dde2c8a148298254439ac7443200b2cd489a6a71f34c240c5ec5f8d2b04938fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "671e64c7-4287-4ded-b7b8-1b9f5b748b8d", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "0eff34990d0570078aae89edbecfe075f9c0710dfd6594b8e6eaeeaa334b2f7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bdee22d-4a9b-4f92-be5d-679db65e2b22", "node_type": "1", "metadata": {}, "hash": "8d6738ce288740b56776e5f329c8da9ff7a04031f8807f92ea4b475a368c518f", "class_name": "RelatedNodeInfo"}}, "text": "Thus:\n                                 \u2202f\n                                 \u2202a    =   c \u2217 1\n                                 \u2202f\n                                  \u2202b   =   c \u2217 1\n The update rules can now be generated as follows:\n                                       \u2202f\n                     a  =  a  +  h  \u2217  \u2202a    =  a  +  h  \u2217  c\n                                       \u2202f\n                     b  =  b  +  h  \u2217  \u2202b    =  b +   h \u2217  c\n                     c  =  c +   h  \u2217  \u2202f   =   c +   h \u2217  d\n                                       \u2202c\n Now that we have our update rules we can express this in python:\n  def product(x,y):\n           return   x*y\n  def addition(x,y):\n  a=5      return   x+y\n  b=-3\n  c=-2\n  d=addition(a,b)\n  h=0.", "start_char_idx": 1275, "end_char_idx": 2005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6bdee22d-4a9b-4f92-be5d-679db65e2b22": {"__data__": {"id_": "6bdee22d-4a9b-4f92-be5d-679db65e2b22", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "dde2c8a148298254439ac7443200b2cd489a6a71f34c240c5ec5f8d2b04938fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89d7a00f-ecd2-4389-8cfd-0c3d57c79ab9", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "a5bb7081a3abefeffd74fa5190fbb01de862a802465c32f322dce6df1d693662", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e067f06-e838-480e-a71f-8c416a14875b", "node_type": "1", "metadata": {}, "hash": "36bdced163bcac4c6bca04c76223ac740dca7adafa58f51d9f2a681ae4476146", "class_name": "RelatedNodeInfo"}}, "text": "01\n  derivative_f_wrt_d=c\n  derivative_f_wrt_c=d\nhttps://jasdeep06.github.io/posts/into-backpropagation/                         5/74/5/24, 8:22 PM                                        into-backpropagation\n  derivative_d_wrt_a=1\n  derivative_d_wrt_b=1\n  derivative_f_wrt_a=derivative_f_wrt_d*derivative_d_wrt_a\n  derivative_f_wrt_b=derivative_f_wrt_d*derivative_d_wrt_b\n  a=a+h*derivative_f_wrt_a\n  b=b+h*derivative_f_wrt_b\n  c=c+h*derivative_f_wrt_c\n  d=addition(a,b)\n  f=product(d,c)               #outputs -3.88\n  The output of above program is -3.88 which is greater than -4.It worked!!!\n  Why did it work?", "start_char_idx": 2005, "end_char_idx": 2617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "9e067f06-e838-480e-a71f-8c416a14875b": {"__data__": {"id_": "9e067f06-e838-480e-a71f-8c416a14875b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "dde2c8a148298254439ac7443200b2cd489a6a71f34c240c5ec5f8d2b04938fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bdee22d-4a9b-4f92-be5d-679db65e2b22", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "f5e5d57c5a9d30da0cc57ed75ccda4a0b57a94712e82611fc7296672282d0864", "class_name": "RelatedNodeInfo"}}, "text": "Let us step back a bit and try to gain intuition of stu\u25af       that is happening here.In order to\n  analyse,first let us traverse through nodes from input to output i.e. in forward\n  direction.We know the input value of        a=5,b=-3,c=-2.We can easily find out value of        d  to\n  be 2 which in turn makes      f=-4.This is essentially known as forward pass through the\n  network.This forward traversal is important because we would want to know the values\n  of intermediate variables like     dthus making it possible to analyse and find the\n  derivatives.\n  Let us now traverse through the nested nodes from output to input i.e.", "start_char_idx": 2620, "end_char_idx": 3257, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "fdaa6846-70ad-4730-9d45-207dcd59b361": {"__data__": {"id_": "fdaa6846-70ad-4730-9d45-207dcd59b361", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "dde2c8a148298254439ac7443200b2cd489a6a71f34c240c5ec5f8d2b04938fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d463a35-7ecb-4f01-abe6-00f78713f306", "node_type": "1", "metadata": {}, "hash": "e4b25aabab357669be8a5c0dd1d9675de2dfe159cef1933c0f9abe9707bdb095", "class_name": "RelatedNodeInfo"}}, "text": "\u2202f                                                                     \u2202f\n The value of   \u2202c  is already known.This leaves us with two unknown values viz:       \u2202a\n       \u2202f\n and   \u2202b  .\n Backpropagation\n Its time to introduce  Chain rule.No need to be intimidated by the name.Its pretty easy\n                                                                         \u2202f\n and straightforward.We know the derivative of      fwith respect to  d( \u2202d  ) and we also\n know the derivative of   dwith respect to  a( \u2202d  ).Chain rule tells us how we can combine\n                                               \u2202a                       \u2202f\n these two derivatives to find the derivative of  f with respect to  a( \u2202a  ).It simply states\n to multiply these two derivatives(or to chain them together)to get the derivative of     f\n with respect to  a.Mathematically- \u2202f          \u2202f      \u2202d\n                                    \u2202a     =    \u2202d   \u2217  \u2202a\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                        4/74/5/24, 8:22 PM                           into-backpropagation\n Similarly,                    \u2202f       \u2202f     \u2202d\n                               \u2202b   =   \u2202d   \u2217  \u2202b\n For this nested system we have already found the values of di\u25aferent derivatives.Thus:\n                                 \u2202f\n                                 \u2202a    =   c \u2217 1\n                                 \u2202f\n                                  \u2202b   =   c \u2217 1\n The update rules can now be generated as follows:\n                                       \u2202f\n                     a  =  a  +  h  \u2217  \u2202a    =  a  +  h  \u2217  c\n                                       \u2202f\n                     b  =  b  +  h  \u2217  \u2202b    =  b +   h \u2217  c\n                     c  =  c +   h  \u2217  \u2202f   =   c +   h \u2217  d\n                                       \u2202c\n Now that we have our update rules we can express this in python:\n  def product(x,y):\n           return   x*y\n  def addition(x,", "start_char_idx": 0, "end_char_idx": 1939, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2d463a35-7ecb-4f01-abe6-00f78713f306": {"__data__": {"id_": "2d463a35-7ecb-4f01-abe6-00f78713f306", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "dde2c8a148298254439ac7443200b2cd489a6a71f34c240c5ec5f8d2b04938fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fdaa6846-70ad-4730-9d45-207dcd59b361", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "f17e8adfd7530baa99c6e89d4278134da2f2f7fb1459b6ed733d5f998d0a9414", "class_name": "RelatedNodeInfo"}}, "text": "y):\n  a=5      return   x+y\n  b=-3\n  c=-2\n  d=addition(a,b)\n  h=0.01\n  derivative_f_wrt_d=c\n  derivative_f_wrt_c=d\nhttps://jasdeep06.github.io/posts/into-backpropagation/                         5/74/5/24, 8:22 PM                                        into-backpropagation\n  derivative_d_wrt_a=1\n  derivative_d_wrt_b=1\n  derivative_f_wrt_a=derivative_f_wrt_d*derivative_d_wrt_a\n  derivative_f_wrt_b=derivative_f_wrt_d*derivative_d_wrt_b\n  a=a+h*derivative_f_wrt_a\n  b=b+h*derivative_f_wrt_b\n  c=c+h*derivative_f_wrt_c\n  d=addition(a,b)\n  f=product(d,c)               #outputs -3.88\n  The output of above program is -3.88 which is greater than -4.It worked!!!\n  Why did it work?\n  Let us step back a bit and try to gain intuition of stu\u25af       that is happening here.In order to\n  analyse,first let us traverse through nodes from input to output i.e. in forward\n  direction.We know the input value of        a=5,b=-3,c=-2.We can easily find out value of        d  to\n  be 2 which in turn makes      f=-4.This is essentially known as forward pass through the\n  network.This forward traversal is important because we would want to know the values\n  of intermediate variables like     dthus making it possible to analyse and find the\n  derivatives.\n  Let us now traverse through the nested nodes from output to input i.e.", "start_char_idx": 1939, "end_char_idx": 3257, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-1": {"__data__": {"id_": "node-1", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6b23c667-1796-4259-b4d6-c287bdb34ea5", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "42e8c0b4be3b36797ef2b88effa058d648e1e7c244b9b71e41068b4ade14adbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64ff2ac8-d437-444d-8b66-3e14ae778cb1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "10d093599ee056744b395c9809937279bc764227efc95b6aa3d17c0cd3fb5d61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a61edf9-fc15-4841-898c-91f90562a1fb", "node_type": "1", "metadata": {}, "hash": "bc401f6fd5372041dc4b2bb7bfa0d548c181d59500f1a840f4250238b7d938c1", "class_name": "RelatedNodeInfo"}}, "text": "\u2202f                                                                     \u2202f\n The value of   \u2202c  is already known.This leaves us with two unknown values viz:       \u2202a\n       \u2202f\n and   \u2202b  .\n Backpropagation\n Its time to introduce  Chain rule.No need to be intimidated by the name.Its pretty easy\n                                                                         \u2202f\n and straightforward.We know the derivative of      fwith respect to  d( \u2202d  ) and we also\n know the derivative of   dwith respect to  a( \u2202d  ).Chain rule tells us how we can combine\n                                               \u2202a                       \u2202f\n these two derivatives to find the derivative of  f with respect to  a( \u2202a  ).It simply states\n to multiply these two derivatives(or to chain them together)to get the derivative of     f\n with respect to  a.Mathematically- \u2202f          \u2202f      \u2202d\n                                    \u2202a     =    \u2202d   \u2217  \u2202a\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                        4/74/5/24, 8:22 PM                           into-backpropagation\n Similarly,                    \u2202f       \u2202f     \u2202d\n                               \u2202b   =   \u2202d   \u2217  \u2202b\n For this nested system we have already found the values of di\u25aferent derivatives.Thus:\n                                 \u2202f\n                                 \u2202a    =   c \u2217 1\n                                 \u2202f\n                                  \u2202b   =   c \u2217 1\n The update rules can now be generated as follows:\n                                       \u2202f\n                     a  =  a  +  h  \u2217  \u2202a    =  a  +  h  \u2217  c\n                                       \u2202f\n                     b  =  b  +  h  \u2217  \u2202b    =  b +   h \u2217  c\n                     c  =  c +   h  \u2217  \u2202f   =   c +   h \u2217  d\n                                       \u2202c\n Now that we have our update rules we can express this in python:\n  def product(x,y):\n           return   x*y\n  def addition(x,y):\n  a=5      return   x+y\n  b=-3\n  c=-2\n  d=addition(a,b)\n  h=0.01\n  derivative_f_wrt_d=c\n  derivative_f_wrt_c=d\nhttps://jasdeep06.github.io/posts/into-backpropagation/                         5/74/5/24, 8:22 PM                                        into-backpropagation\n  derivative_d_wrt_a=1\n  derivative_d_wrt_b=1\n  derivative_f_wrt_a=derivative_f_wrt_d*derivative_d_wrt_a\n  derivative_f_wrt_b=derivative_f_wrt_d*derivative_d_wrt_b\n  a=a+h*derivative_f_wrt_a\n  b=b+h*derivative_f_wrt_b\n  c=c+h*derivative_f_wrt_c\n  d=addition(a,b)\n  f=product(d,c)               #outputs -3.88\n  The output of above program is -3.88 which is greater than -4.It worked!!!\n  Why did it work?\n  Let us step back a bit and try to gain intuition of stu\u25af       that is happening here.In order to\n  analyse,first let us traverse through nodes from input to output i.e. in forward\n  direction.We know the input value of        a=5,b=-3,c=-2.We can easily find out value of        d  to\n  be 2 which in turn makes      f=-4.This is essentially known as forward pass through the\n  network.This forward traversal is important because we would want to know the values\n  of intermediate variables like     dthus making it possible to analyse and find the\n  derivatives.\n  Let us now traverse through the nested nodes from output to input i.e.", "start_char_idx": 4210, "end_char_idx": 7467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4e3bca92-a5f7-4cc8-a418-c16ae070a50d": {"__data__": {"id_": "4e3bca92-a5f7-4cc8-a418-c16ae070a50d", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "ba32f0b9ce2d80ff4cd592b121fd0bb11afd42df7156ee0c6494b147642e4b4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dad30422-231f-466f-a488-d01907981296", "node_type": "1", "metadata": {}, "hash": "7e4e59f93ac2e7da7236a2e92213299bc733b065da661c570b339ffb5c199103", "class_name": "RelatedNodeInfo"}}, "text": "in backward\n  direction.Our aim was to increase the value of         f by manipulating     a,b and   c.In order to\n  increase the value of    f,asc   is negative(-2) andd    is positive(2),we would want to\n  increase the value of c(with sign) and decrease the value of d.This e\u25afect is essentially\n                                  \u2202f\n  captured by the derivative       \u2202c  being positive(thus increasing value ofc         in update rule)\n                    \u2202f\n  and derivative    \u2202d   being negative(thus decreasing the value of          d in update rule).Now\n  as we traverse furthur back through the addition node,in order to decrease value of\n  d,both  a and   b have to decrease.Although the derivatives          \u2202d   and   \u2202d   are positive(1),\n                                                                       \u2202a         \u2202b\n                                                                       \u2202f\n  the decreasing e\u25afect is captured by the negative value of            \u2202d   thus making the\nhttps://jasdeep06.github.", "start_char_idx": 0, "end_char_idx": 1027, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "dad30422-231f-466f-a488-d01907981296": {"__data__": {"id_": "dad30422-231f-466f-a488-d01907981296", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "ba32f0b9ce2d80ff4cd592b121fd0bb11afd42df7156ee0c6494b147642e4b4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e3bca92-a5f7-4cc8-a418-c16ae070a50d", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "a9b6971529e85ce2a745f3ad689785ac1d574978c7ea87a9119cbd6fdc6f024c", "class_name": "RelatedNodeInfo"}}, "text": "io/posts/into-backpropagation/                                                  6/74/5/24, 8:22 PM                                     into-backpropagation\n  products   \u2202d  * \u2202f   and   \u2202f  * \u2202d   negative and thus decreasing the value in update\n             \u2202a    \u2202d         \u2202d    \u2202b\n  rules of a and  b.\n  This pass through the network from output to input is known as backward pass and\n  this process of transfer of gradients or derivatives through the network from output to\n  input is known as backpropogation.\n  I will stop here.I hope that you have captured the intuition behind backpropagation\n  and its nothing but chain rule applied over and over again.In the next post we will\n  apply this algorithm to a standard neural network and develop an intuition of how\n  things work there.                Posted on 14 January,2017\n  maintained by   jasdeep06\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                    7/7", "start_char_idx": 1027, "end_char_idx": 1983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "77f6e27e-e901-42a0-bc55-e766013b740f": {"__data__": {"id_": "77f6e27e-e901-42a0-bc55-e766013b740f", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "ba32f0b9ce2d80ff4cd592b121fd0bb11afd42df7156ee0c6494b147642e4b4e", "class_name": "RelatedNodeInfo"}}, "text": "in backward\n  direction.Our aim was to increase the value of         f by manipulating     a,b and   c.In order to\n  increase the value of    f,asc   is negative(-2) andd    is positive(2),we would want to\n  increase the value of c(with sign) and decrease the value of d.This e\u25afect is essentially\n                                  \u2202f\n  captured by the derivative       \u2202c  being positive(thus increasing value ofc         in update rule)\n                    \u2202f\n  and derivative    \u2202d   being negative(thus decreasing the value of          d in update rule).Now\n  as we traverse furthur back through the addition node,in order to decrease value of\n  d,both  a and   b have to decrease.Although the derivatives          \u2202d   and   \u2202d   are positive(1),\n                                                                       \u2202a         \u2202b\n                                                                       \u2202f\n  the decreasing e\u25afect is captured by the negative value of            \u2202d   thus making the\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                                  6/74/5/24, 8:22 PM                                     into-backpropagation\n  products   \u2202d  * \u2202f   and   \u2202f  * \u2202d   negative and thus decreasing the value in update\n             \u2202a    \u2202d         \u2202d    \u2202b\n  rules of a and  b.\n  This pass through the network from output to input is known as backward pass and\n  this process of transfer of gradients or derivatives through the network from output to\n  input is known as backpropogation.\n  I will stop here.I hope that you have captured the intuition behind backpropagation\n  and its nothing but chain rule applied over and over again.In the next post we will\n  apply this algorithm to a standard neural network and develop an intuition of how\n  things work there.                Posted on 14 January,2017\n  maintained by   jasdeep06\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                    7/7", "start_char_idx": 0, "end_char_idx": 1983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-2": {"__data__": {"id_": "node-2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6b23c667-1796-4259-b4d6-c287bdb34ea5", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "42e8c0b4be3b36797ef2b88effa058d648e1e7c244b9b71e41068b4ade14adbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c78fc1f6-f023-418f-9adc-655bbf24d645", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}, "hash": "dde2c8a148298254439ac7443200b2cd489a6a71f34c240c5ec5f8d2b04938fc", "class_name": "RelatedNodeInfo"}}, "text": "in backward\n  direction.Our aim was to increase the value of         f by manipulating     a,b and   c.In order to\n  increase the value of    f,asc   is negative(-2) andd    is positive(2),we would want to\n  increase the value of c(with sign) and decrease the value of d.This e\u25afect is essentially\n                                  \u2202f\n  captured by the derivative       \u2202c  being positive(thus increasing value ofc         in update rule)\n                    \u2202f\n  and derivative    \u2202d   being negative(thus decreasing the value of          d in update rule).Now\n  as we traverse furthur back through the addition node,in order to decrease value of\n  d,both  a and   b have to decrease.Although the derivatives          \u2202d   and   \u2202d   are positive(1),\n                                                                       \u2202a         \u2202b\n                                                                       \u2202f\n  the decreasing e\u25afect is captured by the negative value of            \u2202d   thus making the\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                                  6/74/5/24, 8:22 PM                                     into-backpropagation\n  products   \u2202d  * \u2202f   and   \u2202f  * \u2202d   negative and thus decreasing the value in update\n             \u2202a    \u2202d         \u2202d    \u2202b\n  rules of a and  b.\n  This pass through the network from output to input is known as backward pass and\n  this process of transfer of gradients or derivatives through the network from output to\n  input is known as backpropogation.\n  I will stop here.I hope that you have captured the intuition behind backpropagation\n  and its nothing but chain rule applied over and over again.In the next post we will\n  apply this algorithm to a standard neural network and develop an intuition of how\n  things work there.                Posted on 14 January,2017\n  maintained by   jasdeep06\nhttps://jasdeep06.github.io/posts/into-backpropagation/                                    7/7", "start_char_idx": 7468, "end_char_idx": 9451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "916b2f98-7c5f-4f98-a83c-3becdf3bc146": {"__data__": {"id_": "916b2f98-7c5f-4f98-a83c-3becdf3bc146", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "into backpropagation", "path": "cache\\blogposts\\into-backpropagation\\parsed\\images\\8351cf45-92df-4157-be3f-b6c8535bcae2-img_p1_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The representation you're referring to illustrates a simple computational graph, which is a common way to visualize mathematical operations in the context of neural networks or programming. In this graph, there are two operations depicted as circles. The first operation takes two inputs, labeled \"a\" and \"b,\" and applies an addition operation to them, resulting in an intermediate output labeled \"d.\" The second operation takes this intermediate output \"d\" along with another input labeled \"c\" and applies a multiplication operation, yielding the final output labeled \"f.\"\n\nThe graph is a visual representation of a sequence of operations where the output of one operation becomes the input to the next. This kind of graph is particularly useful in understanding how changes to the input values affect the final output, which is a key concept in optimization and backpropagation algorithms used in machine learning to adjust input weights to achieve desired outputs. The goal, as mentioned, is to manipulate the input values \"a,\" \"b,\" and \"c\" in such a way that the final output \"f\" is maximized. This process will lead into an exploration of backpropagation, which is a method for calculating gradients that are used to update the inputs or weights in a neural network.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"5ebeed2e-7082-46b6-be37-027975ddc369": {"doc_hash": "71d1d97b093c095a377123add52c89a52cf773e9627ccd5bf82f1cf5af46d709", "ref_doc_id": "node-0"}, "329b576d-a741-42d5-937c-435611ef3247": {"doc_hash": "44f26904514a01a751621f944d47b60a9fc9baeaac21fd271c1bd60f7789b776", "ref_doc_id": "node-0"}, "cc34d39b-de01-46b3-89a6-8b8abe2bc834": {"doc_hash": "e8faa51785c1e56a44a06a280d03d722b26fe4508f79112a5f4fe63135fd1055", "ref_doc_id": "node-0"}, "8d9a6889-0b47-4010-b9d2-7decd31218a0": {"doc_hash": "1467c89c395cebfcc958216b07b17d295c364b898750ea8287f09e83810db6d5", "ref_doc_id": "node-0"}, "346f59a2-4b6d-4d2a-80bc-bdfa0dc37825": {"doc_hash": "f4a7eaafdba97841cf978a98747e2ce524db57999215488f434f53c63db66728", "ref_doc_id": "node-0"}, "6864ad0d-835b-4acb-930c-35d08ee17fcd": {"doc_hash": "be0a13bd37d05a17dc607c3a1fe4e9c3bc538d0c181b183d9137412e2ac4441d", "ref_doc_id": "node-0"}, "68e60a50-9ea2-4300-91a2-e51cb7ef29d7": {"doc_hash": "0ed6f2548080a057df66cad916f9d4ae9ae3edbedfe24f2ff656f11cf5d2082e", "ref_doc_id": "node-0"}, "ba73a067-3b50-4839-8ed2-67f30a86712a": {"doc_hash": "8076f3b0b59a6c289a64fd0bbbd35cd8341fcf7e0bf7fafde7d238c8c9b884da", "ref_doc_id": "node-0"}, "dec2bf1d-acdd-4afa-bd57-4f5de085f5de": {"doc_hash": "be0a13bd37d05a17dc607c3a1fe4e9c3bc538d0c181b183d9137412e2ac4441d", "ref_doc_id": "node-0"}, "node-0": {"doc_hash": "10d093599ee056744b395c9809937279bc764227efc95b6aa3d17c0cd3fb5d61", "ref_doc_id": "6b23c667-1796-4259-b4d6-c287bdb34ea5"}, "b02a1438-8be4-4a67-8c4b-2ac17fb130cc": {"doc_hash": "b42005074213c18c6689e64d80434f233423c42381058c68752831ab775f1ff1", "ref_doc_id": "node-1"}, "671e64c7-4287-4ded-b7b8-1b9f5b748b8d": {"doc_hash": "0eff34990d0570078aae89edbecfe075f9c0710dfd6594b8e6eaeeaa334b2f7b", "ref_doc_id": "node-1"}, "89d7a00f-ecd2-4389-8cfd-0c3d57c79ab9": {"doc_hash": "a5bb7081a3abefeffd74fa5190fbb01de862a802465c32f322dce6df1d693662", "ref_doc_id": "node-1"}, "6bdee22d-4a9b-4f92-be5d-679db65e2b22": {"doc_hash": "f5e5d57c5a9d30da0cc57ed75ccda4a0b57a94712e82611fc7296672282d0864", "ref_doc_id": "node-1"}, "9e067f06-e838-480e-a71f-8c416a14875b": {"doc_hash": "e06612e4efb54f62c01e9481e5261a46acd2e9eabd44bc87dcf6ac87113d0bff", "ref_doc_id": "node-1"}, "fdaa6846-70ad-4730-9d45-207dcd59b361": {"doc_hash": "f17e8adfd7530baa99c6e89d4278134da2f2f7fb1459b6ed733d5f998d0a9414", "ref_doc_id": "node-1"}, "2d463a35-7ecb-4f01-abe6-00f78713f306": {"doc_hash": "3337c2871cd1259a148e2bd899ce16269968f1af5f5e15e825792f225eab4417", "ref_doc_id": "node-1"}, "node-1": {"doc_hash": "dde2c8a148298254439ac7443200b2cd489a6a71f34c240c5ec5f8d2b04938fc", "ref_doc_id": "6b23c667-1796-4259-b4d6-c287bdb34ea5"}, "4e3bca92-a5f7-4cc8-a418-c16ae070a50d": {"doc_hash": "a9b6971529e85ce2a745f3ad689785ac1d574978c7ea87a9119cbd6fdc6f024c", "ref_doc_id": "node-2"}, "dad30422-231f-466f-a488-d01907981296": {"doc_hash": "536756008b441aa21626cd0f44787037638ac5488960c4764915bb51aae9bd87", "ref_doc_id": "node-2"}, "77f6e27e-e901-42a0-bc55-e766013b740f": {"doc_hash": "ba32f0b9ce2d80ff4cd592b121fd0bb11afd42df7156ee0c6494b147642e4b4e", "ref_doc_id": "node-2"}, "node-2": {"doc_hash": "ba32f0b9ce2d80ff4cd592b121fd0bb11afd42df7156ee0c6494b147642e4b4e", "ref_doc_id": "6b23c667-1796-4259-b4d6-c287bdb34ea5"}, "916b2f98-7c5f-4f98-a83c-3becdf3bc146": {"doc_hash": "b0bf16a1a17a956d1388132feeb11f40d25bf9fa4acb1eaa2ec410385f6fc17b"}}, "docstore/ref_doc_info": {"node-0": {"node_ids": ["5ebeed2e-7082-46b6-be37-027975ddc369", "329b576d-a741-42d5-937c-435611ef3247", "cc34d39b-de01-46b3-89a6-8b8abe2bc834", "8d9a6889-0b47-4010-b9d2-7decd31218a0", "346f59a2-4b6d-4d2a-80bc-bdfa0dc37825", "6864ad0d-835b-4acb-930c-35d08ee17fcd", "68e60a50-9ea2-4300-91a2-e51cb7ef29d7", "ba73a067-3b50-4839-8ed2-67f30a86712a", "dec2bf1d-acdd-4afa-bd57-4f5de085f5de"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}}, "6b23c667-1796-4259-b4d6-c287bdb34ea5": {"node_ids": ["node-0", "node-1", "node-2"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}}, "node-1": {"node_ids": ["b02a1438-8be4-4a67-8c4b-2ac17fb130cc", "671e64c7-4287-4ded-b7b8-1b9f5b748b8d", "89d7a00f-ecd2-4389-8cfd-0c3d57c79ab9", "6bdee22d-4a9b-4f92-be5d-679db65e2b22", "9e067f06-e838-480e-a71f-8c416a14875b", "fdaa6846-70ad-4730-9d45-207dcd59b361", "2d463a35-7ecb-4f01-abe6-00f78713f306"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}}, "node-2": {"node_ids": ["4e3bca92-a5f7-4cc8-a418-c16ae070a50d", "dad30422-231f-466f-a488-d01907981296", "77f6e27e-e901-42a0-bc55-e766013b740f"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "into backpropagation"}}}}