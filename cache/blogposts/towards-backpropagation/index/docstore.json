{"docstore/data": {"18dfb1af-a419-4182-b3d9-c568c719086a": {"__data__": {"id_": "18dfb1af-a419-4182-b3d9-c568c719086a", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "7f9b3a059947f021637eff222ef3a51ecf034dec3b140be86020655c3b46fc4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8fc9a285-c54b-4ac5-9bee-30e30d88c0c8", "node_type": "1", "metadata": {}, "hash": "7027676356afd374edb3e3958392e3f65f07792fdc9c6ef514f94d391ff46210", "class_name": "RelatedNodeInfo"}}, "text": "4/2/24, 1:23 AM                                        towards-backpropagation\n  Backpropagation                                                                                  HOME\n  Towards-Backpropagation\n  Backpropagation is by far the most important algorithm for training a neural\n  network.Although alternatives such as           Genetic Algorithmor Exhaustive search exist but\n  their performance is vastly inferior as compared to backpropagation.Many resources\n  are scattered across web that explain backpropagation but they can be pretty\n  intimidating for a beginner due to their immensely mathematical nature.This series of\n  blogposts is aimed to develop an intuition for backpropagation algorithm which would\n  enable the reader to implement backpropagation on the go. In this post we will try to\n  develop an intuition how change in input of a simple node system will a\u25afect its output.\n  Why understand it when Tensorflow/Theano are there?", "start_char_idx": 0, "end_char_idx": 956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "8fc9a285-c54b-4ac5-9bee-30e30d88c0c8": {"__data__": {"id_": "8fc9a285-c54b-4ac5-9bee-30e30d88c0c8", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "7f9b3a059947f021637eff222ef3a51ecf034dec3b140be86020655c3b46fc4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18dfb1af-a419-4182-b3d9-c568c719086a", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "fbb9994e03fa2daee40034c781260cecacb2f609e389ea169f2e3a0427448021", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22b8177f-93ac-4103-b438-476b28d70494", "node_type": "1", "metadata": {}, "hash": "885376e810f2960a0511956aff7d19f19f2e4648ac8ea36f1938c203a73bfaf9", "class_name": "RelatedNodeInfo"}}, "text": "Libraries like Tensorflow and Theano give us a ready-made implementation of\n  backpropagation algorithm and do everything for us in few lines of code.Why to go\n  through all this fuss?Well,for me personally, it feels good to know the intricacies of\n  what I am working with but the \u201cfeel-good\u201d factor for some folks is not a good enough\n  reason.For them,A lot of times while implementing research papers,you would come\n  across structures other than networks to implement backpropagation through.Also\n  problems like Vanishing gradients on sigmoids,Dying ReLUs,Exploding gradients in\n  RNNs can be better understood and prevented if you know intricacies of\n  backpropagation.", "start_char_idx": 959, "end_char_idx": 1635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "22b8177f-93ac-4103-b438-476b28d70494": {"__data__": {"id_": "22b8177f-93ac-4103-b438-476b28d70494", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "7f9b3a059947f021637eff222ef3a51ecf034dec3b140be86020655c3b46fc4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8fc9a285-c54b-4ac5-9bee-30e30d88c0c8", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "c61faf0ee3ecf1dc67094c048a02cab0d0c5438b7a0513eb46f98b5232088202", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "537bc2c8-44e3-43a7-ba16-e713e78aa571", "node_type": "1", "metadata": {}, "hash": "5f9f20815adb6287920c64eccfe9a4711f68152b596c61fb4314a995880a8987", "class_name": "RelatedNodeInfo"}}, "text": "So enough of motivation!Bottom line is that \u201cYou should understand\n  backpropagation.\u201d\n        A lot of times while implementing research papers,you would come across\n        structures other than networks to implement backpropagation through.Also\n        problems like Vanishing gradients on sigmoids,Dying ReLUs,Exploding\n        gradients in RNNs can be better understood and prevented if you know\n        intricacies of backpropagation.\n  So lets get started!\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                               1/94/2/24, 1:23 AM                                      towards-backpropagation\n  Before understanding backpropagation,let us go through the basic case on which\n  majority of this post will be based. It consists of a node accepting two inputs           a,b  and\n  producing an output. It will be referred to as    a*bdefault casefor rest of this post.\n  The node accepts two inputs        a,b  and does a product operation on them and gives           a*b\n  as output.", "start_char_idx": 1636, "end_char_idx": 2672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "537bc2c8-44e3-43a7-ba16-e713e78aa571": {"__data__": {"id_": "537bc2c8-44e3-43a7-ba16-e713e78aa571", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "7f9b3a059947f021637eff222ef3a51ecf034dec3b140be86020655c3b46fc4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22b8177f-93ac-4103-b438-476b28d70494", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "e2240ae630c2763a21626d2e98f39e4330cb29bdf1322d1b69dcd83f131a107d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0e912c8-e9e5-41ab-9478-457a18a69274", "node_type": "1", "metadata": {}, "hash": "f20d7d613a0c48a04c602af884c36a43e4c694e29f1c9f7c772cc9b0718b1b2d", "class_name": "RelatedNodeInfo"}}, "text": "Aim-Our aim is to increase the output by tweaking values of            a and  b.\n  Method#1\n  The first method is the most obvious one.Let us randomly increase the values of               a and\n  b by a small quantity    h times a random number:\n  def   product(a,b):\n  a=4          return     a*b\n  b=3\n  h=0.01\n  a=a+h*(random.random())\n  b=b+h*(random.random())\n  print(product(a,b))\n  The output for above program is        12.042which is greater than        12  as was our\n  aim.Although our aim is achieved but there are problems:\n        This is a good strategy for small problems with few nodes but with millions of\n        inputs and thousands of nodes which is easily possible in modern day networks\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                              2/94/2/24, 1:23 AM                                        towards-backpropagation\n        this strategy of exhaustive search would be too time consuming.", "start_char_idx": 2675, "end_char_idx": 3642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a0e912c8-e9e5-41ab-9478-457a18a69274": {"__data__": {"id_": "a0e912c8-e9e5-41ab-9478-457a18a69274", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "7f9b3a059947f021637eff222ef3a51ecf034dec3b140be86020655c3b46fc4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "537bc2c8-44e3-43a7-ba16-e713e78aa571", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "6cb78046f35be9f1e7577da814cdfe958e2e8daf86ab0e2105238012d5a863b0", "class_name": "RelatedNodeInfo"}}, "text": "This is an unreliable method to increase value of the function.When we randomly\n        increase the values of     a and  b,it might result in decrease in value of function.Have\n        a look at the code below:\n        def    product(a,b):\n        a=-4         return     a*b\n        b=-3\n        h=0.01\n        a=a+h*(random.random())\n        b=b+h*(random.random())\n        print(product(a,b))\n  The above code produces output           11.94   which is less than    12.The only di\u25aference was\n  in initial values of  a and  b.Thus randomly increasing the values of           a and  b won\u02bct su\u25afice\n  our aim.", "start_char_idx": 3651, "end_char_idx": 4261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f919fceb-678f-4e2e-9f8b-8b2f499b527f": {"__data__": {"id_": "f919fceb-678f-4e2e-9f8b-8b2f499b527f", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "7f9b3a059947f021637eff222ef3a51ecf034dec3b140be86020655c3b46fc4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1eb67db3-3b92-4e75-82c1-e07fbe256694", "node_type": "1", "metadata": {}, "hash": "b7c1ef096d4e5a5710573c2149b83c34e9c332d6a006289566c0e00fddca9647", "class_name": "RelatedNodeInfo"}}, "text": "4/2/24, 1:23 AM                                        towards-backpropagation\n  Backpropagation                                                                                  HOME\n  Towards-Backpropagation\n  Backpropagation is by far the most important algorithm for training a neural\n  network.Although alternatives such as           Genetic Algorithmor Exhaustive search exist but\n  their performance is vastly inferior as compared to backpropagation.Many resources\n  are scattered across web that explain backpropagation but they can be pretty\n  intimidating for a beginner due to their immensely mathematical nature.This series of\n  blogposts is aimed to develop an intuition for backpropagation algorithm which would\n  enable the reader to implement backpropagation on the go. In this post we will try to\n  develop an intuition how change in input of a simple node system will a\u25afect its output.\n  Why understand it when Tensorflow/Theano are there?\n  Libraries like Tensorflow and Theano give us a ready-made implementation of\n  backpropagation algorithm and do everything for us in few lines of code.Why to go\n  through all this fuss?Well,for me personally, it feels good to know the intricacies of\n  what I am working with but the \u201cfeel-good\u201d factor for some folks is not a good enough\n  reason.For them,A lot of times while implementing research papers,you would come\n  across structures other than networks to implement backpropagation through.Also\n  problems like Vanishing gradients on sigmoids,Dying ReLUs,Exploding gradients in\n  RNNs can be better understood and prevented if you know intricacies of\n  backpropagation. So enough of motivation!Bottom line is that \u201cYou should understand\n  backpropagation.\u201d\n        A lot of times while implementing research papers,you would come across\n        structures other than networks to implement backpropagation through.Also\n        problems like Vanishing gradients on sigmoids,Dying ReLUs,Exploding\n        gradients in RNNs can be better understood and prevented if you know\n        intricacies of backpropagation.\n  So lets get started!", "start_char_idx": 0, "end_char_idx": 2099, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1eb67db3-3b92-4e75-82c1-e07fbe256694": {"__data__": {"id_": "1eb67db3-3b92-4e75-82c1-e07fbe256694", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "7f9b3a059947f021637eff222ef3a51ecf034dec3b140be86020655c3b46fc4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f919fceb-678f-4e2e-9f8b-8b2f499b527f", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "bd24479f90dd09e1c31fe899419ea843fdf7c48e8924fbff750574e7fcc1963e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59b9cbc5-0f56-4199-bc70-a381abd78209", "node_type": "1", "metadata": {}, "hash": "f20d7d613a0c48a04c602af884c36a43e4c694e29f1c9f7c772cc9b0718b1b2d", "class_name": "RelatedNodeInfo"}}, "text": "https://jasdeep06.github.io/posts/towards-backpropagation/                                               1/94/2/24, 1:23 AM                                      towards-backpropagation\n  Before understanding backpropagation,let us go through the basic case on which\n  majority of this post will be based. It consists of a node accepting two inputs           a,b  and\n  producing an output. It will be referred to as    a*bdefault casefor rest of this post.\n  The node accepts two inputs        a,b  and does a product operation on them and gives           a*b\n  as output.\n  Aim-Our aim is to increase the output by tweaking values of            a and  b.\n  Method#1\n  The first method is the most obvious one.Let us randomly increase the values of               a and\n  b by a small quantity    h times a random number:\n  def   product(a,b):\n  a=4          return     a*b\n  b=3\n  h=0.01\n  a=a+h*(random.random())\n  b=b+h*(random.random())\n  print(product(a,b))\n  The output for above program is        12.042which is greater than        12  as was our\n  aim.Although our aim is achieved but there are problems:\n        This is a good strategy for small problems with few nodes but with millions of\n        inputs and thousands of nodes which is easily possible in modern day networks\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                              2/94/2/24, 1:23 AM                                        towards-backpropagation\n        this strategy of exhaustive search would be too time consuming.", "start_char_idx": 2100, "end_char_idx": 3642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "59b9cbc5-0f56-4199-bc70-a381abd78209": {"__data__": {"id_": "59b9cbc5-0f56-4199-bc70-a381abd78209", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "7f9b3a059947f021637eff222ef3a51ecf034dec3b140be86020655c3b46fc4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1eb67db3-3b92-4e75-82c1-e07fbe256694", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "20369cb9416c8ff6c0ecc93f0af7108517ec6ef7e6470e81e5fc38ce0b9649c4", "class_name": "RelatedNodeInfo"}}, "text": "This is an unreliable method to increase value of the function.When we randomly\n        increase the values of     a and  b,it might result in decrease in value of function.Have\n        a look at the code below:\n        def    product(a,b):\n        a=-4         return     a*b\n        b=-3\n        h=0.01\n        a=a+h*(random.random())\n        b=b+h*(random.random())\n        print(product(a,b))\n  The above code produces output           11.94   which is less than    12.The only di\u25aference was\n  in initial values of  a and  b.Thus randomly increasing the values of           a and  b won\u02bct su\u25afice\n  our aim.", "start_char_idx": 3651, "end_char_idx": 4261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-0": {"__data__": {"id_": "node-0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e57bb7d-5aeb-4da8-9f1a-a2fcc0a2df5f", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "f016698b751fab5f63420a5689678a80a6e3217862f9a6ec5f3938f24798ecbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24f9ee94-a5ea-4f5a-952d-76777e386cc7", "node_type": "1", "metadata": {}, "hash": "84e3ee68bb13ab91b21e12fee0767903be140e71b635d5b6bab59defe050eb4c", "class_name": "RelatedNodeInfo"}}, "text": "4/2/24, 1:23 AM                                        towards-backpropagation\n  Backpropagation                                                                                  HOME\n  Towards-Backpropagation\n  Backpropagation is by far the most important algorithm for training a neural\n  network.Although alternatives such as           Genetic Algorithmor Exhaustive search exist but\n  their performance is vastly inferior as compared to backpropagation.Many resources\n  are scattered across web that explain backpropagation but they can be pretty\n  intimidating for a beginner due to their immensely mathematical nature.This series of\n  blogposts is aimed to develop an intuition for backpropagation algorithm which would\n  enable the reader to implement backpropagation on the go. In this post we will try to\n  develop an intuition how change in input of a simple node system will a\u25afect its output.\n  Why understand it when Tensorflow/Theano are there?\n  Libraries like Tensorflow and Theano give us a ready-made implementation of\n  backpropagation algorithm and do everything for us in few lines of code.Why to go\n  through all this fuss?Well,for me personally, it feels good to know the intricacies of\n  what I am working with but the \u201cfeel-good\u201d factor for some folks is not a good enough\n  reason.For them,A lot of times while implementing research papers,you would come\n  across structures other than networks to implement backpropagation through.Also\n  problems like Vanishing gradients on sigmoids,Dying ReLUs,Exploding gradients in\n  RNNs can be better understood and prevented if you know intricacies of\n  backpropagation. So enough of motivation!Bottom line is that \u201cYou should understand\n  backpropagation.\u201d\n        A lot of times while implementing research papers,you would come across\n        structures other than networks to implement backpropagation through.Also\n        problems like Vanishing gradients on sigmoids,Dying ReLUs,Exploding\n        gradients in RNNs can be better understood and prevented if you know\n        intricacies of backpropagation.\n  So lets get started!\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                               1/94/2/24, 1:23 AM                                      towards-backpropagation\n  Before understanding backpropagation,let us go through the basic case on which\n  majority of this post will be based. It consists of a node accepting two inputs           a,b  and\n  producing an output. It will be referred to as    a*bdefault casefor rest of this post.\n  The node accepts two inputs        a,b  and does a product operation on them and gives           a*b\n  as output.\n  Aim-Our aim is to increase the output by tweaking values of            a and  b.\n  Method#1\n  The first method is the most obvious one.Let us randomly increase the values of               a and\n  b by a small quantity    h times a random number:\n  def   product(a,b):\n  a=4          return     a*b\n  b=3\n  h=0.01\n  a=a+h*(random.random())\n  b=b+h*(random.random())\n  print(product(a,b))\n  The output for above program is        12.042which is greater than        12  as was our\n  aim.Although our aim is achieved but there are problems:\n        This is a good strategy for small problems with few nodes but with millions of\n        inputs and thousands of nodes which is easily possible in modern day networks\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                              2/94/2/24, 1:23 AM                                        towards-backpropagation\n        this strategy of exhaustive search would be too time consuming.\n        This is an unreliable method to increase value of the function.When we randomly\n        increase the values of     a and  b,it might result in decrease in value of function.Have\n        a look at the code below:\n        def    product(a,b):\n        a=-4         return     a*b\n        b=-3\n        h=0.01\n        a=a+h*(random.random())\n        b=b+h*(random.random())\n        print(product(a,b))\n  The above code produces output           11.94   which is less than    12.The only di\u25aference was\n  in initial values of  a and  b.Thus randomly increasing the values of           a and  b won\u02bct su\u25afice\n  our aim.", "start_char_idx": 0, "end_char_idx": 4261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1142e4ba-7cf9-43a7-8274-8c66de0adc53": {"__data__": {"id_": "1142e4ba-7cf9-43a7-8274-8c66de0adc53", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "4c6622eebcfdef501d8590ba538e2d290ef309999e56bf739dcb862688e95423", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94911def-33d1-41a5-9b64-fc98a7ad26f4", "node_type": "1", "metadata": {}, "hash": "85f031a23b1194176c49bb707414c7fe8e2c85117500fe0bc10b2d7a4dc12f21", "class_name": "RelatedNodeInfo"}}, "text": "Thus we can conclude that:\n        We need more control over increasing the values of input.Essentially it means that\n        there should be more controlled coe\u25aficient of           h.\n        The coe\u25aficient ofh      should be function of input or inputs as changing the initial\n        values of input changed the behaviour of output.\n  Method#2\n  This control that we desire in the coe\u25aficient ofh          is given by derivative.Derivative of a\n  function with respect to a variable is a pretty easy and straightforward concept.Let us\n  understand by modifying our         default case-\n  Let us increase the value of     a  by a small quantityh(same as in method 1) and let us give\n  a name to our product function say f(a,b).Thus:\n                                         f(a,     b) =      a   \u2217  b\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                               3/94/2/24,", "start_char_idx": 0, "end_char_idx": 919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "94911def-33d1-41a5-9b64-fc98a7ad26f4": {"__data__": {"id_": "94911def-33d1-41a5-9b64-fc98a7ad26f4", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "4c6622eebcfdef501d8590ba538e2d290ef309999e56bf739dcb862688e95423", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1142e4ba-7cf9-43a7-8274-8c66de0adc53", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "fe5ea183ccb33d495b93814925faaa4cdd961f2ac86f6ff779b39a3cbaf9f428", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9040cb6-e472-42fd-93e1-8ec11fe89720", "node_type": "1", "metadata": {}, "hash": "d887c448d789e6d9f2fb8975bf89b1515f13c5d8dbae3543af5152c86bd6cbe5", "class_name": "RelatedNodeInfo"}}, "text": "1:23 AM                                         towards-backpropagation\n                 a+h                              (a+h)tb\n  The new output can be calculated easily as           (a+h)*b     which can be expanded as\n  a*b+h*b.Thus the output increases by a value of              h*b  as compared to      default case.We\n  can say that with increase of       h in a the output increases by       h*b,thus with a unit\n  increase in   a the output would have increased byb.This normalised e\u25afect of increase in\n  value of one of the input on output is expressed as derivative of output with respect to\n  that input.Note that when we take out derivative of function with respect to a variable\n  then all other variables are kept constant. The mathematical interpretation of the\n  derivative of function     f with respect to    a is defined as-\n                            \u2202f     =     f((a      +   h),    b) \u2212     f(a,     b)\n                            \u2202a                               h\n  The above formula is nothing but mathematical interpretation of definition of\n  derivative as mentioned above.", "start_char_idx": 920, "end_char_idx": 2024, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c9040cb6-e472-42fd-93e1-8ec11fe89720": {"__data__": {"id_": "c9040cb6-e472-42fd-93e1-8ec11fe89720", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "4c6622eebcfdef501d8590ba538e2d290ef309999e56bf739dcb862688e95423", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94911def-33d1-41a5-9b64-fc98a7ad26f4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "8cc49837490d7f9bc555f05cac0577981ae02694276cad7534b162facc67dc4e", "class_name": "RelatedNodeInfo"}}, "text": "(For those familiar with multivariate calculus,this is the\n  partial derivative of    f with respect to    a )\n  Similarly,the derivative of function       f with respect to    b can be found by increasing        b by a\n  small quantity    h  and normalising the di\u25aference between the final output(a*b+a*h)\n  and initial output(a*b) that gives us        a.\n  This is the numeric method of finding gradients.It has a significant drawback.Although\n  it is less error prone but it takes lot of time to calculate derivatives this way.Time to\n  introduce a new method of finding derivatives:Analytical method.", "start_char_idx": 2024, "end_char_idx": 2628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "69552909-b33c-466c-8516-0a9fc2e18089": {"__data__": {"id_": "69552909-b33c-466c-8516-0a9fc2e18089", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "4c6622eebcfdef501d8590ba538e2d290ef309999e56bf739dcb862688e95423", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bc63c7a-bbbf-4a83-9d0b-8a4c8f325850", "node_type": "1", "metadata": {}, "hash": "d887c448d789e6d9f2fb8975bf89b1515f13c5d8dbae3543af5152c86bd6cbe5", "class_name": "RelatedNodeInfo"}}, "text": "Thus we can conclude that:\n        We need more control over increasing the values of input.Essentially it means that\n        there should be more controlled coe\u25aficient of           h.\n        The coe\u25aficient ofh      should be function of input or inputs as changing the initial\n        values of input changed the behaviour of output.\n  Method#2\n  This control that we desire in the coe\u25aficient ofh          is given by derivative.Derivative of a\n  function with respect to a variable is a pretty easy and straightforward concept.Let us\n  understand by modifying our         default case-\n  Let us increase the value of     a  by a small quantityh(same as in method 1) and let us give\n  a name to our product function say f(a,b).Thus:\n                                         f(a,     b) =      a   \u2217  b\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                               3/94/2/24, 1:23 AM                                         towards-backpropagation\n                 a+h                              (a+h)tb\n  The new output can be calculated easily as           (a+h)*b     which can be expanded as\n  a*b+h*b.Thus the output increases by a value of              h*b  as compared to      default case.We\n  can say that with increase of       h in a the output increases by       h*b,thus with a unit\n  increase in   a the output would have increased byb.This normalised e\u25afect of increase in\n  value of one of the input on output is expressed as derivative of output with respect to\n  that input.Note that when we take out derivative of function with respect to a variable\n  then all other variables are kept constant. The mathematical interpretation of the\n  derivative of function     f with respect to    a is defined as-\n                            \u2202f     =     f((a      +   h),    b) \u2212     f(a,     b)\n                            \u2202a                               h\n  The above formula is nothing but mathematical interpretation of definition of\n  derivative as mentioned above.", "start_char_idx": 0, "end_char_idx": 2024, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1bc63c7a-bbbf-4a83-9d0b-8a4c8f325850": {"__data__": {"id_": "1bc63c7a-bbbf-4a83-9d0b-8a4c8f325850", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "4c6622eebcfdef501d8590ba538e2d290ef309999e56bf739dcb862688e95423", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69552909-b33c-466c-8516-0a9fc2e18089", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "867d398e09292c2960633c9bd1d20ce4825ae0605dbf9b707575940312e7defb", "class_name": "RelatedNodeInfo"}}, "text": "(For those familiar with multivariate calculus,this is the\n  partial derivative of    f with respect to    a )\n  Similarly,the derivative of function       f with respect to    b can be found by increasing        b by a\n  small quantity    h  and normalising the di\u25aference between the final output(a*b+a*h)\n  and initial output(a*b) that gives us        a.\n  This is the numeric method of finding gradients.It has a significant drawback.Although\n  it is less error prone but it takes lot of time to calculate derivatives this way.Time to\n  introduce a new method of finding derivatives:Analytical method.", "start_char_idx": 2024, "end_char_idx": 2628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-1": {"__data__": {"id_": "node-1", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e57bb7d-5aeb-4da8-9f1a-a2fcc0a2df5f", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "f016698b751fab5f63420a5689678a80a6e3217862f9a6ec5f3938f24798ecbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b90b10b5-6927-4933-a111-4cbc51aa08d9", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "7f9b3a059947f021637eff222ef3a51ecf034dec3b140be86020655c3b46fc4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6422019-692e-463b-b943-8ac4e426a498", "node_type": "1", "metadata": {}, "hash": "f4250820b142d2d661a26af905c448d84724a18b4c7cbde5463800c939ee53c6", "class_name": "RelatedNodeInfo"}}, "text": "Thus we can conclude that:\n        We need more control over increasing the values of input.Essentially it means that\n        there should be more controlled coe\u25aficient of           h.\n        The coe\u25aficient ofh      should be function of input or inputs as changing the initial\n        values of input changed the behaviour of output.\n  Method#2\n  This control that we desire in the coe\u25aficient ofh          is given by derivative.Derivative of a\n  function with respect to a variable is a pretty easy and straightforward concept.Let us\n  understand by modifying our         default case-\n  Let us increase the value of     a  by a small quantityh(same as in method 1) and let us give\n  a name to our product function say f(a,b).Thus:\n                                         f(a,     b) =      a   \u2217  b\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                               3/94/2/24, 1:23 AM                                         towards-backpropagation\n                 a+h                              (a+h)tb\n  The new output can be calculated easily as           (a+h)*b     which can be expanded as\n  a*b+h*b.Thus the output increases by a value of              h*b  as compared to      default case.We\n  can say that with increase of       h in a the output increases by       h*b,thus with a unit\n  increase in   a the output would have increased byb.This normalised e\u25afect of increase in\n  value of one of the input on output is expressed as derivative of output with respect to\n  that input.Note that when we take out derivative of function with respect to a variable\n  then all other variables are kept constant. The mathematical interpretation of the\n  derivative of function     f with respect to    a is defined as-\n                            \u2202f     =     f((a      +   h),    b) \u2212     f(a,     b)\n                            \u2202a                               h\n  The above formula is nothing but mathematical interpretation of definition of\n  derivative as mentioned above.(For those familiar with multivariate calculus,this is the\n  partial derivative of    f with respect to    a )\n  Similarly,the derivative of function       f with respect to    b can be found by increasing        b by a\n  small quantity    h  and normalising the di\u25aference between the final output(a*b+a*h)\n  and initial output(a*b) that gives us        a.\n  This is the numeric method of finding gradients.It has a significant drawback.Although\n  it is less error prone but it takes lot of time to calculate derivatives this way.Time to\n  introduce a new method of finding derivatives:Analytical method.", "start_char_idx": 4264, "end_char_idx": 6892, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "dd505949-27a5-4f7d-8df9-92393becd847": {"__data__": {"id_": "dd505949-27a5-4f7d-8df9-92393becd847", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "f767038812fa02c493c4b72c4c4510ce00d75754d0563cf7019bb3b85fe49fbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0fad1ba-84d5-4ede-8be2-7dbfb8358c58", "node_type": "1", "metadata": {}, "hash": "faf987c094b6e1fe2f0c199d2b69b67cd57d53656bcfd992284c604db3c807d8", "class_name": "RelatedNodeInfo"}}, "text": "Method#3\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                   4/94/2/24, 1:23 AM                                  towards-backpropagation\n Analytical gradient method:In order to calculate gradient using analytical gradient\n method we need to remember few basic rules of calculus(Refer          Derivative-rules).These\n rules are derived from the numerical gradient method and are committed to memory\n so that they can be used directly.This saves us the computation time and space\n required for calculating derivatives.For   f=a*b,the following can be directly stated-\n                                           \u2202f\n                                           \u2202a     =   b\n This can easily be derived from the mathematical interpretation of derivative as stated\n above.", "start_char_idx": 0, "end_char_idx": 822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e0fad1ba-84d5-4ede-8be2-7dbfb8358c58": {"__data__": {"id_": "e0fad1ba-84d5-4ede-8be2-7dbfb8358c58", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "f767038812fa02c493c4b72c4c4510ce00d75754d0563cf7019bb3b85fe49fbf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd505949-27a5-4f7d-8df9-92393becd847", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "42c5860c4231ecff4a45fee97afdfa4e66d57ba9002dcd68e85a97fb7b5c6a76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6396cba9-2001-44d4-96da-5830de97025b", "node_type": "1", "metadata": {}, "hash": "071e13d94f77a3a47c870b9ff494141195c9de4bcb2c7e235a7ca1fc52b803a9", "class_name": "RelatedNodeInfo"}}, "text": "Putting values in interpretation-\n                           \u2202f          (a   +   h) \u2217    b  \u2212   a  \u2217  b\n                           \u2202a     =                   h\n                        \u2202f          (a   \u2217  b  +   h  \u2217  b) \u2212     a  \u2217  b\n                         \u2202a    =           \u2202f         h\n Similarly one can derive:                 \u2202a     =   b\n                                           \u2202f\n                                           \u2202b    =    a\n With these two derivatives in hand we have our coe\u25aficients of h in a-update and b-\n update.So our modified update rules will be-\n                         a  =   a   +   h  \u2217   \u2202f     =   a  +   h  \u2217  b\n                                               \u2202a\n                                               \u2202f\n                         b  =   b  +   h   \u2217   \u2202b    =    b +   h   \u2217  a\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                      5/94/2/24,", "start_char_idx": 822, "end_char_idx": 1753, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6396cba9-2001-44d4-96da-5830de97025b": {"__data__": {"id_": "6396cba9-2001-44d4-96da-5830de97025b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "f767038812fa02c493c4b72c4c4510ce00d75754d0563cf7019bb3b85fe49fbf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0fad1ba-84d5-4ede-8be2-7dbfb8358c58", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "ab9216dddfbd6e4d310d86d4e8174bf7c85e014828e814f1e9b8a6facf60fd7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1b12def-3b82-44da-9067-cbdad1db516f", "node_type": "1", "metadata": {}, "hash": "ced1c4cd32feba69a4f69b51965bef24773e67a4b3b25e298c196378e10e8fd2", "class_name": "RelatedNodeInfo"}}, "text": "1:23 AM                                      towards-backpropagation\n  The modified code will be:\n  def   product(a,b):\n  a=-4         return     a*b\n  b=-3\n  h=0.01\n  a=a+h*b\n  b=b+h*a\n  print(product(a,b))\n  The output of above code is 12.252 which is greater than 12 as was our aim.\n  Why does this approach work?", "start_char_idx": 1754, "end_char_idx": 2070, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f1b12def-3b82-44da-9067-cbdad1db516f": {"__data__": {"id_": "f1b12def-3b82-44da-9067-cbdad1db516f", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "f767038812fa02c493c4b72c4c4510ce00d75754d0563cf7019bb3b85fe49fbf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6396cba9-2001-44d4-96da-5830de97025b", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "e7a7514936cf0935c740cac04d2b0c3a18adf5c8795044417b21a3d75baff15d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3238851a-6ce8-4333-beff-e350d3d478b4", "node_type": "1", "metadata": {}, "hash": "577347303a6567d9a208192b5f1659dbca416d868ea280042c8a4467cc2e3d4e", "class_name": "RelatedNodeInfo"}}, "text": "To appreciate the beauty of derivative in updates of inputs          a and  b we have to dive into\n  geometrical interpretation of derivative.Geometrically,derivative of a function with\n  respect to a variable tells us the rate at which that function changes with respect to that\n  variable.While reading ahead keep in mind those update rules given by-\n                                      a   =    a  +   h   \u2217 \u2202a\u2202f\n                                       b  =    b  +   h   \u2217 \u2202b\u2202f\n       Derivative of a function with respect to a variable tells us the rate at which\n       that function changes with respect to that variable keeping all other variables\n       constant.\n  Let us understand the e\u25afect of derivative in update rules in a single-variable system to\n  get an intuition of what is happenning-\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                            6/94/2/24, 1:23 AM                                         towards-backpropagation\n                                                             y-f(x)\n Above figure represents a function in single variable             x.", "start_char_idx": 2073, "end_char_idx": 3201, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3238851a-6ce8-4333-beff-e350d3d478b4": {"__data__": {"id_": "3238851a-6ce8-4333-beff-e350d3d478b4", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "f767038812fa02c493c4b72c4c4510ce00d75754d0563cf7019bb3b85fe49fbf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1b12def-3b82-44da-9067-cbdad1db516f", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "88f53b62d2d47f1f6939e59c9a7acb8bc7d03678dacac04d7272a9bad1f9d0a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66010583-0630-4331-a850-7a8abf4dedf9", "node_type": "1", "metadata": {}, "hash": "f065b1f67734c11403c74dad80ebd0b825f9167b2696cacc92046d3ae42dd94a", "class_name": "RelatedNodeInfo"}}, "text": "As  x increases(from le\u25af       to right)\n the function increases till point B and then decreases as we furthur increase                   x.Let us\n imagine two cases-\n Case#1\n Imagine we are at point A as shown in figure and we want to change the value of                      x  in\n such a way that the value of funcion increases.From the figure it is clear that if we\n increase the value of      x,the value of function increases.Now let us take a look at our\n update rule and see how it comes to this conclusion-\n                                        x   =    x   +    h   \u2217   dy\n                                                                  dx\n We know that our       h>0   so whether    x  increases or decreases depends upon derivative of\n function with respect to       x.Recall that derivative is nothing but the rate of change of\n function.At point A we can see that the function increases as                x increases so the rate of\n change of function with respect to          x at A is positive.This means that at point A the\n derivative of function with respect to         x is positive.", "start_char_idx": 3201, "end_char_idx": 4308, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "66010583-0630-4331-a850-7a8abf4dedf9": {"__data__": {"id_": "66010583-0630-4331-a850-7a8abf4dedf9", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "f767038812fa02c493c4b72c4c4510ce00d75754d0563cf7019bb3b85fe49fbf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3238851a-6ce8-4333-beff-e350d3d478b4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "6764bd2122cb26b8b56113cb00d60db38ca9fe334dca631f6f0766b16cf4a4bf", "class_name": "RelatedNodeInfo"}}, "text": "A positive derivative will make          x\n increase through the update rule.", "start_char_idx": 4308, "end_char_idx": 4385, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "30a159b3-b0cc-4f8d-bd0e-54d313d6a795": {"__data__": {"id_": "30a159b3-b0cc-4f8d-bd0e-54d313d6a795", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "f767038812fa02c493c4b72c4c4510ce00d75754d0563cf7019bb3b85fe49fbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70db4d0c-99bd-43e9-bfb4-a3801e4cfb80", "node_type": "1", "metadata": {}, "hash": "2a76cbe9c07b84439d9362a6bd6d64bcf07bbde6bdd3712dbfc59bd448b7c071", "class_name": "RelatedNodeInfo"}}, "text": "Method#3\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                   4/94/2/24, 1:23 AM                                  towards-backpropagation\n Analytical gradient method:In order to calculate gradient using analytical gradient\n method we need to remember few basic rules of calculus(Refer          Derivative-rules).These\n rules are derived from the numerical gradient method and are committed to memory\n so that they can be used directly.This saves us the computation time and space\n required for calculating derivatives.For   f=a*b,the following can be directly stated-\n                                           \u2202f\n                                           \u2202a     =   b\n This can easily be derived from the mathematical interpretation of derivative as stated\n above.Putting values in interpretation-\n                           \u2202f          (a   +   h) \u2217    b  \u2212   a  \u2217  b\n                           \u2202a     =                   h\n                        \u2202f          (a   \u2217  b  +   h  \u2217  b) \u2212     a  \u2217  b\n                         \u2202a    =           \u2202f         h\n Similarly one can derive:                 \u2202a     =   b\n                                           \u2202f\n                                           \u2202b    =    a\n With these two derivatives in hand we have our coe\u25aficients of h in a-update and b-\n update.So our modified update rules will be-\n                         a  =   a   +   h  \u2217   \u2202f     =   a  +   h  \u2217  b\n                                               \u2202a\n                                               \u2202f\n                         b  =   b  +   h   \u2217   \u2202b    =    b +   h   \u2217  a\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                      5/94/2/24, 1:23 AM                                      towards-backpropagation\n  The modified code will be:\n  def   product(a,b):\n  a=-4         return     a*b\n  b=-3\n  h=0.01\n  a=a+h*b\n  b=b+h*a\n  print(product(a,b))\n  The output of above code is 12.252 which is greater than 12 as was our aim.\n  Why does this approach work?", "start_char_idx": 0, "end_char_idx": 2070, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "70db4d0c-99bd-43e9-bfb4-a3801e4cfb80": {"__data__": {"id_": "70db4d0c-99bd-43e9-bfb4-a3801e4cfb80", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "f767038812fa02c493c4b72c4c4510ce00d75754d0563cf7019bb3b85fe49fbf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30a159b3-b0cc-4f8d-bd0e-54d313d6a795", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "3c7b5f4a0b2dc0e9139558746601f2505d86ec696afa32bf4c8398d560d12217", "class_name": "RelatedNodeInfo"}}, "text": "To appreciate the beauty of derivative in updates of inputs          a and  b we have to dive into\n  geometrical interpretation of derivative.Geometrically,derivative of a function with\n  respect to a variable tells us the rate at which that function changes with respect to that\n  variable.While reading ahead keep in mind those update rules given by-\n                                      a   =    a  +   h   \u2217 \u2202a\u2202f\n                                       b  =    b  +   h   \u2217 \u2202b\u2202f\n       Derivative of a function with respect to a variable tells us the rate at which\n       that function changes with respect to that variable keeping all other variables\n       constant.\n  Let us understand the e\u25afect of derivative in update rules in a single-variable system to\n  get an intuition of what is happenning-\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                            6/94/2/24, 1:23 AM                                         towards-backpropagation\n                                                             y-f(x)\n Above figure represents a function in single variable             x.As  x increases(from le\u25af       to right)\n the function increases till point B and then decreases as we furthur increase                   x.Let us\n imagine two cases-\n Case#1\n Imagine we are at point A as shown in figure and we want to change the value of                      x  in\n such a way that the value of funcion increases.From the figure it is clear that if we\n increase the value of      x,the value of function increases.Now let us take a look at our\n update rule and see how it comes to this conclusion-\n                                        x   =    x   +    h   \u2217   dy\n                                                                  dx\n We know that our       h>0   so whether    x  increases or decreases depends upon derivative of\n function with respect to       x.Recall that derivative is nothing but the rate of change of\n function.At point A we can see that the function increases as                x increases so the rate of\n change of function with respect to          x at A is positive.This means that at point A the\n derivative of function with respect to         x is positive.A positive derivative will make          x\n increase through the update rule.", "start_char_idx": 2073, "end_char_idx": 4385, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-2": {"__data__": {"id_": "node-2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e57bb7d-5aeb-4da8-9f1a-a2fcc0a2df5f", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "f016698b751fab5f63420a5689678a80a6e3217862f9a6ec5f3938f24798ecbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24f9ee94-a5ea-4f5a-952d-76777e386cc7", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "4c6622eebcfdef501d8590ba538e2d290ef309999e56bf739dcb862688e95423", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8be95e0a-939d-4c19-a1a5-b0b43bcc7775", "node_type": "1", "metadata": {}, "hash": "04b4689e7932b72b2904ec33be18557a6ad327f0ae992f3ed2b1403e112f0386", "class_name": "RelatedNodeInfo"}}, "text": "Method#3\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                   4/94/2/24, 1:23 AM                                  towards-backpropagation\n Analytical gradient method:In order to calculate gradient using analytical gradient\n method we need to remember few basic rules of calculus(Refer          Derivative-rules).These\n rules are derived from the numerical gradient method and are committed to memory\n so that they can be used directly.This saves us the computation time and space\n required for calculating derivatives.For   f=a*b,the following can be directly stated-\n                                           \u2202f\n                                           \u2202a     =   b\n This can easily be derived from the mathematical interpretation of derivative as stated\n above.Putting values in interpretation-\n                           \u2202f          (a   +   h) \u2217    b  \u2212   a  \u2217  b\n                           \u2202a     =                   h\n                        \u2202f          (a   \u2217  b  +   h  \u2217  b) \u2212     a  \u2217  b\n                         \u2202a    =           \u2202f         h\n Similarly one can derive:                 \u2202a     =   b\n                                           \u2202f\n                                           \u2202b    =    a\n With these two derivatives in hand we have our coe\u25aficients of h in a-update and b-\n update.So our modified update rules will be-\n                         a  =   a   +   h  \u2217   \u2202f     =   a  +   h  \u2217  b\n                                               \u2202a\n                                               \u2202f\n                         b  =   b  +   h   \u2217   \u2202b    =    b +   h   \u2217  a\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                      5/94/2/24, 1:23 AM                                      towards-backpropagation\n  The modified code will be:\n  def   product(a,b):\n  a=-4         return     a*b\n  b=-3\n  h=0.01\n  a=a+h*b\n  b=b+h*a\n  print(product(a,b))\n  The output of above code is 12.252 which is greater than 12 as was our aim.\n  Why does this approach work?\n  To appreciate the beauty of derivative in updates of inputs          a and  b we have to dive into\n  geometrical interpretation of derivative.Geometrically,derivative of a function with\n  respect to a variable tells us the rate at which that function changes with respect to that\n  variable.While reading ahead keep in mind those update rules given by-\n                                      a   =    a  +   h   \u2217 \u2202a\u2202f\n                                       b  =    b  +   h   \u2217 \u2202b\u2202f\n       Derivative of a function with respect to a variable tells us the rate at which\n       that function changes with respect to that variable keeping all other variables\n       constant.\n  Let us understand the e\u25afect of derivative in update rules in a single-variable system to\n  get an intuition of what is happenning-\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                            6/94/2/24, 1:23 AM                                         towards-backpropagation\n                                                             y-f(x)\n Above figure represents a function in single variable             x.As  x increases(from le\u25af       to right)\n the function increases till point B and then decreases as we furthur increase                   x.Let us\n imagine two cases-\n Case#1\n Imagine we are at point A as shown in figure and we want to change the value of                      x  in\n such a way that the value of funcion increases.From the figure it is clear that if we\n increase the value of      x,the value of function increases.Now let us take a look at our\n update rule and see how it comes to this conclusion-\n                                        x   =    x   +    h   \u2217   dy\n                                                                  dx\n We know that our       h>0   so whether    x  increases or decreases depends upon derivative of\n function with respect to       x.Recall that derivative is nothing but the rate of change of\n function.At point A we can see that the function increases as                x increases so the rate of\n change of function with respect to          x at A is positive.This means that at point A the\n derivative of function with respect to         x is positive.A positive derivative will make          x\n increase through the update rule.", "start_char_idx": 6895, "end_char_idx": 11280, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "24bc835d-662f-44c7-8115-bf3a0fc8d037": {"__data__": {"id_": "24bc835d-662f-44c7-8115-bf3a0fc8d037", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "ca0ed39a89092ee002ee02c7393ca4a0a671996209bd6c855b721e5a9b1dff22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbb82d2c-7271-45f8-890f-a02af41344d0", "node_type": "1", "metadata": {}, "hash": "d05b3505e129c7b6ee8ef123c1759a28cc3ba35771da359cea2c188e9789dd87", "class_name": "RelatedNodeInfo"}}, "text": "Case#2\n Imagine that we are at point C as shown in figure.Everything remains same as case#1\n but now we can see from figure that the value of function increases with decrease in\n value of   x.At this time the derivative of funcion with respect to            x will be  negative   as\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                     7/94/2/24, 1:23 AM                                           towards-backpropagation\n value of function decreases as          x increases(i.e. the rate of change of function with             x is\n negative).When we put a negative derivative in our update rule the value of                       x decreases\n as expected.\n        Thus the derivative captures the nature of our function and gives our update\n Case#3 rule a sense of direction to obtain a higher value.", "start_char_idx": 0, "end_char_idx": 859, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "bbb82d2c-7271-45f8-890f-a02af41344d0": {"__data__": {"id_": "bbb82d2c-7271-45f8-890f-a02af41344d0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "ca0ed39a89092ee002ee02c7393ca4a0a671996209bd6c855b721e5a9b1dff22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24bc835d-662f-44c7-8115-bf3a0fc8d037", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "22d9845df1231eddc8b8a3728d55a887e23f58de454e6401979ca4b1db93a5ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab75b283-d6d7-4650-a160-e21f63fc8862", "node_type": "1", "metadata": {}, "hash": "b90ecd7767901233610446e5c1989f3162222532a9e490765af9f9db6a85a0d5", "class_name": "RelatedNodeInfo"}}, "text": "Imagine that we are somewhere pretty close to point B(say we are on the le\u25af                         of it but\n pretty close to it).The value of derivative will be positive and the update rule would\n want to increase the value of         xin order to increase the value of the function.But a\n condition could arise when value of            h (stepsize) is su\u25aficiently large that it overshoots\n the point B and thus decreasing the value of our function.Then we would have to adjust\n value of our stepsize(make it small of course!) in order to increase value of our\n function.Thus this approach of increasing value of inputs in direction of derivative\n would give us desired result for almost all functions you would encounter.(It fails for\n some poorly defined convex functions, but for now you do not need to worry about\n them.)\n I will stop this post here.I hope that this post helped you develop an intuition about\n derivatives and their involvement in update rule.In the next post we will apply these\n principals in nested nodes and will finally see backpropagation.\n   ALSO ON JASDEEP06                    Posted on 12 January,", "start_char_idx": 861, "end_char_idx": 1991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ab75b283-d6d7-4650-a160-e21f63fc8862": {"__data__": {"id_": "ab75b283-d6d7-4650-a160-e21f63fc8862", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "ca0ed39a89092ee002ee02c7393ca4a0a671996209bd6c855b721e5a9b1dff22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbb82d2c-7271-45f8-890f-a02af41344d0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "3ff1e7068b202c2ec4758086973a7fe51a4ee0583acf2075d05d64f42084c3b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb0b0f30-b14f-4996-a0c1-d6ce6f045b91", "node_type": "1", "metadata": {}, "hash": "58878ca182465bc2c5c393077575d75f39aab635f97e6125c5ab8ed1aef5ec6e", "class_name": "RelatedNodeInfo"}}, "text": "2017\n    Further-into                     Understanding L STMin           Variable-sharing-in-            Getting stt\n    Further-into-                    Understanding LSTM in           Variable-sharing-in-            Getting sta\n    Further-into-                    Understanding LSTM in           Variable-sharing-in-            Getting sta\n    backpropagation                  Tensorflow                      Tensorflow                      Tensorflov\n    backpropagation                  Tensor\u25afow                       Tensor\u25afow                       Tensor\u25afow\n    backpropagation                  Tensor\u25afow                       Tensor\u25afow                       Tensor\u25afow\n               \u2022                               \u2022                                \u2022                               \u2022\n    7 years ago 2 comments           7 years ago32 comments          7 years ago 14comments          7 years ago\n    Backpropagation : Further        CNNs in Tensorflow(cifar-       Tensorflow: Variable sharing    Tensorflow :\n    into Backpropagation             10)                             in Tensorflow                   with Tensorfl\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                        8/94/2/24,", "start_char_idx": 1991, "end_char_idx": 3249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "fb0b0f30-b14f-4996-a0c1-d6ce6f045b91": {"__data__": {"id_": "fb0b0f30-b14f-4996-a0c1-d6ce6f045b91", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "ca0ed39a89092ee002ee02c7393ca4a0a671996209bd6c855b721e5a9b1dff22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab75b283-d6d7-4650-a160-e21f63fc8862", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "f3fd37865b0d9bd81a635d02fed92b6c8a93aaa54e3bd262dfbd4f245f6dc681", "class_name": "RelatedNodeInfo"}}, "text": "1:23 AM                   towards-backpropagation\n 0 Comments                                    \ue603  Jasdeep Singh Chhabra\n  (rJG\n         Start the discussion\u2026\n   \uf109    Share                                        BestNewestOldest\n                           Be thefirst to comment.\n    Subscribe Privacy Do Not Sell My Data\n Backpropagation maintained byjasdeep06\nhttps://jasdeep06.github.io/posts/towards-backpropagation/        9/9", "start_char_idx": 3250, "end_char_idx": 3682, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f163f89b-8c8b-4a71-83c3-433e2d2fa731": {"__data__": {"id_": "f163f89b-8c8b-4a71-83c3-433e2d2fa731", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "ca0ed39a89092ee002ee02c7393ca4a0a671996209bd6c855b721e5a9b1dff22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a223ae33-e2b7-4be2-b3c7-32d3c5f5891e", "node_type": "1", "metadata": {}, "hash": "80331c957ee52688b3e60f955ba704b42054924f5892883a6580ae65cecda91d", "class_name": "RelatedNodeInfo"}}, "text": "Case#2\n Imagine that we are at point C as shown in figure.Everything remains same as case#1\n but now we can see from figure that the value of function increases with decrease in\n value of   x.At this time the derivative of funcion with respect to            x will be  negative   as\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                     7/94/2/24, 1:23 AM                                           towards-backpropagation\n value of function decreases as          x increases(i.e. the rate of change of function with             x is\n negative).When we put a negative derivative in our update rule the value of                       x decreases\n as expected.\n        Thus the derivative captures the nature of our function and gives our update\n Case#3 rule a sense of direction to obtain a higher value.\n Imagine that we are somewhere pretty close to point B(say we are on the le\u25af                         of it but\n pretty close to it).The value of derivative will be positive and the update rule would\n want to increase the value of         xin order to increase the value of the function.But a\n condition could arise when value of            h (stepsize) is su\u25aficiently large that it overshoots\n the point B and thus decreasing the value of our function.Then we would have to adjust\n value of our stepsize(make it small of course!) in order to increase value of our\n function.Thus this approach of increasing value of inputs in direction of derivative\n would give us desired result for almost all functions you would encounter.(It fails for\n some poorly defined convex functions, but for now you do not need to worry about\n them.)\n I will stop this post here.I hope that this post helped you develop an intuition about\n derivatives and their involvement in update rule.In the next post we will apply these\n principals in nested nodes and will finally see backpropagation.", "start_char_idx": 0, "end_char_idx": 1929, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a223ae33-e2b7-4be2-b3c7-32d3c5f5891e": {"__data__": {"id_": "a223ae33-e2b7-4be2-b3c7-32d3c5f5891e", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "ca0ed39a89092ee002ee02c7393ca4a0a671996209bd6c855b721e5a9b1dff22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f163f89b-8c8b-4a71-83c3-433e2d2fa731", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "c229ecb40ae6cc26a9f895a511ee576ae4d6b0f9db82b2b48837108ff5fbf11e", "class_name": "RelatedNodeInfo"}}, "text": "ALSO ON JASDEEP06                    Posted on 12 January,2017\n    Further-into                     Understanding L STMin           Variable-sharing-in-            Getting stt\n    Further-into-                    Understanding LSTM in           Variable-sharing-in-            Getting sta\n    Further-into-                    Understanding LSTM in           Variable-sharing-in-            Getting sta\n    backpropagation                  Tensorflow                      Tensorflow                      Tensorflov\n    backpropagation                  Tensor\u25afow                       Tensor\u25afow                       Tensor\u25afow\n    backpropagation                  Tensor\u25afow                       Tensor\u25afow                       Tensor\u25afow\n               \u2022                               \u2022                                \u2022                               \u2022\n    7 years ago 2 comments           7 years ago32 comments          7 years ago 14comments          7 years ago\n    Backpropagation : Further        CNNs in Tensorflow(cifar-       Tensorflow: Variable sharing    Tensorflow :\n    into Backpropagation             10)                             in Tensorflow                   with Tensorfl\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                        8/94/2/24, 1:23 AM                   towards-backpropagation\n 0 Comments                                    \ue603  Jasdeep Singh Chhabra\n  (rJG\n         Start the discussion\u2026\n   \uf109    Share                                        BestNewestOldest\n                           Be thefirst to comment.\n    Subscribe Privacy Do Not Sell My Data\n Backpropagation maintained byjasdeep06\nhttps://jasdeep06.github.io/posts/towards-backpropagation/        9/9", "start_char_idx": 1933, "end_char_idx": 3682, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-3": {"__data__": {"id_": "node-3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e57bb7d-5aeb-4da8-9f1a-a2fcc0a2df5f", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "f016698b751fab5f63420a5689678a80a6e3217862f9a6ec5f3938f24798ecbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6422019-692e-463b-b943-8ac4e426a498", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}, "hash": "f767038812fa02c493c4b72c4c4510ce00d75754d0563cf7019bb3b85fe49fbf", "class_name": "RelatedNodeInfo"}}, "text": "Case#2\n Imagine that we are at point C as shown in figure.Everything remains same as case#1\n but now we can see from figure that the value of function increases with decrease in\n value of   x.At this time the derivative of funcion with respect to            x will be  negative   as\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                     7/94/2/24, 1:23 AM                                           towards-backpropagation\n value of function decreases as          x increases(i.e. the rate of change of function with             x is\n negative).When we put a negative derivative in our update rule the value of                       x decreases\n as expected.\n        Thus the derivative captures the nature of our function and gives our update\n Case#3 rule a sense of direction to obtain a higher value.\n Imagine that we are somewhere pretty close to point B(say we are on the le\u25af                         of it but\n pretty close to it).The value of derivative will be positive and the update rule would\n want to increase the value of         xin order to increase the value of the function.But a\n condition could arise when value of            h (stepsize) is su\u25aficiently large that it overshoots\n the point B and thus decreasing the value of our function.Then we would have to adjust\n value of our stepsize(make it small of course!) in order to increase value of our\n function.Thus this approach of increasing value of inputs in direction of derivative\n would give us desired result for almost all functions you would encounter.(It fails for\n some poorly defined convex functions, but for now you do not need to worry about\n them.)\n I will stop this post here.I hope that this post helped you develop an intuition about\n derivatives and their involvement in update rule.In the next post we will apply these\n principals in nested nodes and will finally see backpropagation.\n   ALSO ON JASDEEP06                    Posted on 12 January,2017\n    Further-into                     Understanding L STMin           Variable-sharing-in-            Getting stt\n    Further-into-                    Understanding LSTM in           Variable-sharing-in-            Getting sta\n    Further-into-                    Understanding LSTM in           Variable-sharing-in-            Getting sta\n    backpropagation                  Tensorflow                      Tensorflow                      Tensorflov\n    backpropagation                  Tensor\u25afow                       Tensor\u25afow                       Tensor\u25afow\n    backpropagation                  Tensor\u25afow                       Tensor\u25afow                       Tensor\u25afow\n               \u2022                               \u2022                                \u2022                               \u2022\n    7 years ago 2 comments           7 years ago32 comments          7 years ago 14comments          7 years ago\n    Backpropagation : Further        CNNs in Tensorflow(cifar-       Tensorflow: Variable sharing    Tensorflow :\n    into Backpropagation             10)                             in Tensorflow                   with Tensorfl\nhttps://jasdeep06.github.io/posts/towards-backpropagation/                                                        8/94/2/24, 1:23 AM                   towards-backpropagation\n 0 Comments                                    \ue603  Jasdeep Singh Chhabra\n  (rJG\n         Start the discussion\u2026\n   \uf109    Share                                        BestNewestOldest\n                           Be thefirst to comment.\n    Subscribe Privacy Do Not Sell My Data\n Backpropagation maintained byjasdeep06\nhttps://jasdeep06.github.io/posts/towards-backpropagation/        9/9", "start_char_idx": 11282, "end_char_idx": 14964, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "10cf71fe-6195-4631-b158-4357503e4f33": {"__data__": {"id_": "10cf71fe-6195-4631-b158-4357503e4f33", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "towards backpropagation", "path": "cache\\blogposts\\towards-backpropagation\\parsed\\images\\1cd59981-3150-4376-ab82-099b6eb2126c-img_p1_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being discussed here is a fundamental operation in neural networks, where a node performs a multiplication of two inputs. The inputs are labeled as 'a' and 'b', and the operation performed by the node is indicated by an asterisk (*), symbolizing multiplication. The result of this operation is the product of 'a' and 'b', which is then output from the node. This operation is a basic building block for more complex neural network structures and is crucial for understanding how information is processed and transformed within the network.\n\nThe text also describes an aim to optimize the output by adjusting the input values 'a' and 'b'. A method is proposed to increment these values by a small factor, which is a combination of a constant 'h' and a random number. This approach is a simple form of optimization, but the text suggests that it may not be efficient for larger networks with many inputs and nodes, hinting at the need for more sophisticated techniques like backpropagation for such scenarios.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24aba9d8-877a-442c-b884-13fe15d104d0": {"__data__": {"id_": "24aba9d8-877a-442c-b884-13fe15d104d0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "towards backpropagation", "path": "cache\\blogposts\\towards-backpropagation\\parsed\\images\\1cd59981-3150-4376-ab82-099b6eb2126c-img_p3_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being discussed here is related to the calculation of derivatives in the context of functions with multiple variables. The explanation provided outlines how the output of a function changes when one of the input variables is increased by a small amount, h. The increase in output is proportional to the product of h and the other variable, b, when the first variable, a, is increased by h. This relationship is used to define the derivative of the function with respect to the variable a, which is a measure of how much the output changes per unit increase in a, while keeping b constant.\n\nThe formula provided is a representation of the partial derivative of the function f with respect to the variable a. It shows that the derivative is the limit of the difference in the function's output as h approaches zero. This is a fundamental concept in calculus, particularly in multivariate calculus, where the focus is on functions with more than one variable.\n\nThe discussion also touches upon the numerical method of finding gradients, which involves incrementally adjusting one variable and observing the change in the output. While this method is straightforward and less prone to errors, it is noted that it can be time-consuming. This leads to the introduction of the analytical method for finding derivatives, which is a more efficient approach that involves using rules of differentiation to directly calculate the derivative without incrementally adjusting the variables.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0c5f1f5-f1fb-4510-bcd9-3d8fee89300f": {"__data__": {"id_": "f0c5f1f5-f1fb-4510-bcd9-3d8fee89300f", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "towards backpropagation", "path": "cache\\blogposts\\towards-backpropagation\\parsed\\images\\1cd59981-3150-4376-ab82-099b6eb2126c-img_p6_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The scenario described involves a graph of a function y=f(x) with respect to a single variable x. The graph is shaped like an arch, where the function value increases as x increases, reaching a peak at a certain point, and then decreases as x continues to increase beyond that peak.\n\nIn the first case, we consider a point labeled A on the graph where the function is on the rising slope. Here, the function value increases as x increases, indicating that the slope of the function at this point is positive. Therefore, if we were to adjust the value of x in the positive direction, the function value would increase, consistent with the update rule provided, where x is updated by adding a product of a positive constant h and the derivative of y with respect to x (dy/dx). Since the derivative is positive at point A, the update rule will result in an increase in x.\n\nIn the second case, we look at a point labeled C, which is on the descending slope of the graph, past the peak. At this point, the function value increases when x is decreased, which means the slope of the function at point C is negative. Consequently, the derivative of the function with respect to x at point C is negative. According to the update rule, if we apply the same process as in the first case, the negative derivative will result in a decrease in the value of x, as we are adding a negative quantity to x (since h is positive and dy/dx", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"18dfb1af-a419-4182-b3d9-c568c719086a": {"doc_hash": "fbb9994e03fa2daee40034c781260cecacb2f609e389ea169f2e3a0427448021", "ref_doc_id": "node-0"}, "8fc9a285-c54b-4ac5-9bee-30e30d88c0c8": {"doc_hash": "c61faf0ee3ecf1dc67094c048a02cab0d0c5438b7a0513eb46f98b5232088202", "ref_doc_id": "node-0"}, "22b8177f-93ac-4103-b438-476b28d70494": {"doc_hash": "e2240ae630c2763a21626d2e98f39e4330cb29bdf1322d1b69dcd83f131a107d", "ref_doc_id": "node-0"}, "537bc2c8-44e3-43a7-ba16-e713e78aa571": {"doc_hash": "6cb78046f35be9f1e7577da814cdfe958e2e8daf86ab0e2105238012d5a863b0", "ref_doc_id": "node-0"}, "a0e912c8-e9e5-41ab-9478-457a18a69274": {"doc_hash": "9701decbc099ed8bdc929a7ad7829dee552358e3ef2664bb987915970875bd87", "ref_doc_id": "node-0"}, "f919fceb-678f-4e2e-9f8b-8b2f499b527f": {"doc_hash": "bd24479f90dd09e1c31fe899419ea843fdf7c48e8924fbff750574e7fcc1963e", "ref_doc_id": "node-0"}, "1eb67db3-3b92-4e75-82c1-e07fbe256694": {"doc_hash": "20369cb9416c8ff6c0ecc93f0af7108517ec6ef7e6470e81e5fc38ce0b9649c4", "ref_doc_id": "node-0"}, "59b9cbc5-0f56-4199-bc70-a381abd78209": {"doc_hash": "9701decbc099ed8bdc929a7ad7829dee552358e3ef2664bb987915970875bd87", "ref_doc_id": "node-0"}, "node-0": {"doc_hash": "7f9b3a059947f021637eff222ef3a51ecf034dec3b140be86020655c3b46fc4e", "ref_doc_id": "9e57bb7d-5aeb-4da8-9f1a-a2fcc0a2df5f"}, "1142e4ba-7cf9-43a7-8274-8c66de0adc53": {"doc_hash": "fe5ea183ccb33d495b93814925faaa4cdd961f2ac86f6ff779b39a3cbaf9f428", "ref_doc_id": "node-1"}, "94911def-33d1-41a5-9b64-fc98a7ad26f4": {"doc_hash": "8cc49837490d7f9bc555f05cac0577981ae02694276cad7534b162facc67dc4e", "ref_doc_id": "node-1"}, "c9040cb6-e472-42fd-93e1-8ec11fe89720": {"doc_hash": "47d4c30a3a3539eaa5eacc34dbf74af9071e50725fc5d4abdb7efc75e5cd6dae", "ref_doc_id": "node-1"}, "69552909-b33c-466c-8516-0a9fc2e18089": {"doc_hash": "867d398e09292c2960633c9bd1d20ce4825ae0605dbf9b707575940312e7defb", "ref_doc_id": "node-1"}, "1bc63c7a-bbbf-4a83-9d0b-8a4c8f325850": {"doc_hash": "47d4c30a3a3539eaa5eacc34dbf74af9071e50725fc5d4abdb7efc75e5cd6dae", "ref_doc_id": "node-1"}, "node-1": {"doc_hash": "4c6622eebcfdef501d8590ba538e2d290ef309999e56bf739dcb862688e95423", "ref_doc_id": "9e57bb7d-5aeb-4da8-9f1a-a2fcc0a2df5f"}, "dd505949-27a5-4f7d-8df9-92393becd847": {"doc_hash": "42c5860c4231ecff4a45fee97afdfa4e66d57ba9002dcd68e85a97fb7b5c6a76", "ref_doc_id": "node-2"}, "e0fad1ba-84d5-4ede-8be2-7dbfb8358c58": {"doc_hash": "ab9216dddfbd6e4d310d86d4e8174bf7c85e014828e814f1e9b8a6facf60fd7f", "ref_doc_id": "node-2"}, "6396cba9-2001-44d4-96da-5830de97025b": {"doc_hash": "e7a7514936cf0935c740cac04d2b0c3a18adf5c8795044417b21a3d75baff15d", "ref_doc_id": "node-2"}, "f1b12def-3b82-44da-9067-cbdad1db516f": {"doc_hash": "88f53b62d2d47f1f6939e59c9a7acb8bc7d03678dacac04d7272a9bad1f9d0a9", "ref_doc_id": "node-2"}, "3238851a-6ce8-4333-beff-e350d3d478b4": {"doc_hash": "6764bd2122cb26b8b56113cb00d60db38ca9fe334dca631f6f0766b16cf4a4bf", "ref_doc_id": "node-2"}, "66010583-0630-4331-a850-7a8abf4dedf9": {"doc_hash": "d4596083e3c42ebe62e444ec17f62491e1110def4cb264f36a1ff660212dbbc8", "ref_doc_id": "node-2"}, "30a159b3-b0cc-4f8d-bd0e-54d313d6a795": {"doc_hash": "3c7b5f4a0b2dc0e9139558746601f2505d86ec696afa32bf4c8398d560d12217", "ref_doc_id": "node-2"}, "70db4d0c-99bd-43e9-bfb4-a3801e4cfb80": {"doc_hash": "b844229e8bee0a27a80383b9ac83b4dd5dc1b72a0e30206dc422da446a5b6b6d", "ref_doc_id": "node-2"}, "node-2": {"doc_hash": "f767038812fa02c493c4b72c4c4510ce00d75754d0563cf7019bb3b85fe49fbf", "ref_doc_id": "9e57bb7d-5aeb-4da8-9f1a-a2fcc0a2df5f"}, "24bc835d-662f-44c7-8115-bf3a0fc8d037": {"doc_hash": "22d9845df1231eddc8b8a3728d55a887e23f58de454e6401979ca4b1db93a5ba", "ref_doc_id": "node-3"}, "bbb82d2c-7271-45f8-890f-a02af41344d0": {"doc_hash": "3ff1e7068b202c2ec4758086973a7fe51a4ee0583acf2075d05d64f42084c3b6", "ref_doc_id": "node-3"}, "ab75b283-d6d7-4650-a160-e21f63fc8862": {"doc_hash": "f3fd37865b0d9bd81a635d02fed92b6c8a93aaa54e3bd262dfbd4f245f6dc681", "ref_doc_id": "node-3"}, "fb0b0f30-b14f-4996-a0c1-d6ce6f045b91": {"doc_hash": "bb124f8ced6b47020052eb68c441f4c933c21c560a0f0e45f5f50651414bec16", "ref_doc_id": "node-3"}, "f163f89b-8c8b-4a71-83c3-433e2d2fa731": {"doc_hash": "c229ecb40ae6cc26a9f895a511ee576ae4d6b0f9db82b2b48837108ff5fbf11e", "ref_doc_id": "node-3"}, "a223ae33-e2b7-4be2-b3c7-32d3c5f5891e": {"doc_hash": "597f252931956570888a97d13b12081f29dc3f3d443d7083cea96eb6af87e73d", "ref_doc_id": "node-3"}, "node-3": {"doc_hash": "ca0ed39a89092ee002ee02c7393ca4a0a671996209bd6c855b721e5a9b1dff22", "ref_doc_id": "9e57bb7d-5aeb-4da8-9f1a-a2fcc0a2df5f"}, "10cf71fe-6195-4631-b158-4357503e4f33": {"doc_hash": "73c13a3444e147602ee8d41817d4e5e59708e6e92b599b8b638ba7ed4c9d920a"}, "24aba9d8-877a-442c-b884-13fe15d104d0": {"doc_hash": "f9bead676a38f8de32bf64196060a186bd8d48fb7710a6ef73dfab70359ee271"}, "f0c5f1f5-f1fb-4510-bcd9-3d8fee89300f": {"doc_hash": "46864d1f05f4909ee143951db108f3faf3a9bd393fc79c81d7b9234cc3b078e5"}}, "docstore/ref_doc_info": {"node-0": {"node_ids": ["18dfb1af-a419-4182-b3d9-c568c719086a", "8fc9a285-c54b-4ac5-9bee-30e30d88c0c8", "22b8177f-93ac-4103-b438-476b28d70494", "537bc2c8-44e3-43a7-ba16-e713e78aa571", "a0e912c8-e9e5-41ab-9478-457a18a69274", "f919fceb-678f-4e2e-9f8b-8b2f499b527f", "1eb67db3-3b92-4e75-82c1-e07fbe256694", "59b9cbc5-0f56-4199-bc70-a381abd78209"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}}, "9e57bb7d-5aeb-4da8-9f1a-a2fcc0a2df5f": {"node_ids": ["node-0", "node-1", "node-2", "node-3"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}}, "node-1": {"node_ids": ["1142e4ba-7cf9-43a7-8274-8c66de0adc53", "94911def-33d1-41a5-9b64-fc98a7ad26f4", "c9040cb6-e472-42fd-93e1-8ec11fe89720", "69552909-b33c-466c-8516-0a9fc2e18089", "1bc63c7a-bbbf-4a83-9d0b-8a4c8f325850"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}}, "node-2": {"node_ids": ["dd505949-27a5-4f7d-8df9-92393becd847", "e0fad1ba-84d5-4ede-8be2-7dbfb8358c58", "6396cba9-2001-44d4-96da-5830de97025b", "f1b12def-3b82-44da-9067-cbdad1db516f", "3238851a-6ce8-4333-beff-e350d3d478b4", "66010583-0630-4331-a850-7a8abf4dedf9", "30a159b3-b0cc-4f8d-bd0e-54d313d6a795", "70db4d0c-99bd-43e9-bfb4-a3801e4cfb80"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}}, "node-3": {"node_ids": ["24bc835d-662f-44c7-8115-bf3a0fc8d037", "bbb82d2c-7271-45f8-890f-a02af41344d0", "ab75b283-d6d7-4650-a160-e21f63fc8862", "fb0b0f30-b14f-4996-a0c1-d6ce6f045b91", "f163f89b-8c8b-4a71-83c3-433e2d2fa731", "a223ae33-e2b7-4be2-b3c7-32d3c5f5891e"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "towards backpropagation"}}}}