{"docstore/data": {"9c5fc5ac-bdc9-438b-abc3-6b88e77ef6d9": {"__data__": {"id_": "9c5fc5ac-bdc9-438b-abc3-6b88e77ef6d9", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "b9621e3975a7486da56c848f3400831dcffea2c83ff89c69c89b214a0c1a70c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03a86b98-d016-4644-ba9a-4638013c5652", "node_type": "1", "metadata": {}, "hash": "7efb7c6401edbbff248ac0ff94d39a49c5641dd49657c38e409f530f1bbb8f01", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:30 PM                                                             Intuitions in CNNs\n  Tensorflow                                                                                                                  HOME\n  Intuitions in Convolutional Neural Networks\n  In previousblogposts      we explored the basics of tensorflow.In this series of posts we will look into di\u25aferent types of\n  neural networks and their implementation in Tensorflow.In this post we will look into the gory details of convolutional\n  neural networks(CNNs) from motivation to reasoning their e\u25afectiveness. As we will mostly deal with applying CNNs on\n  images,we will start by looking into digital representation of images and how filters work on an image.We will also look\n  into some theoretical aspects of CNNs which would help us to implement them in coming posts.\n  Lets get started!!!\n  Digital representation of images\n  We all have(hopefully!)", "start_char_idx": 0, "end_char_idx": 937, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "03a86b98-d016-4644-ba9a-4638013c5652": {"__data__": {"id_": "03a86b98-d016-4644-ba9a-4638013c5652", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "b9621e3975a7486da56c848f3400831dcffea2c83ff89c69c89b214a0c1a70c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c5fc5ac-bdc9-438b-abc3-6b88e77ef6d9", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "58c7120e2448d3c9f9b4cb2eb3fd59c908aa5f06dd41663407e87af32a52f2d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7a673c8-95b4-4f87-a6c1-09415685ca33", "node_type": "1", "metadata": {}, "hash": "757b7581ced0e1c7ad98e27724658c7d190dedf487a69f86ef104d9395d3181d", "class_name": "RelatedNodeInfo"}}, "text": "drawn pictures on canvas or a sheet of paper at some point in our lives.We would use di\u25aferent\n  colours to accomplish this task.The concept of colour,to most of us,feels pretty abstract.Therefore the representation of\n  images on a screen rather than a piece of paper needed us to express this abstraction in form of a concept.Thus the\n  concept of \u201cpixel\u201d was motivated.\n  Images are digitally represented in form of \u201cpixels\u201d.These are the building blocks of digital images.You can visualize\n  pixels as tiny rectangles of lights that impart properties to an image.\n  To further clarify this representation,let us look at concept of resolution.When we state that an image has resolution\n  (2048X1536) we necessarily mean that it has 2048 pixels in width and 1536 pixels in height and thus has total of\n  2048X1536=3,145,7 pixels or 3.1 megapixels.", "start_char_idx": 938, "end_char_idx": 1786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f7a673c8-95b4-4f87-a6c1-09415685ca33": {"__data__": {"id_": "f7a673c8-95b4-4f87-a6c1-09415685ca33", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "b9621e3975a7486da56c848f3400831dcffea2c83ff89c69c89b214a0c1a70c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03a86b98-d016-4644-ba9a-4638013c5652", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "09c9ddcd22762559649cc22c9503bdc341370c963f227ca8db6960d0c1e03539", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f20ee78-5713-488c-a266-562a3aa8cd71", "node_type": "1", "metadata": {}, "hash": "f5928486d0f5cb95d92efaf7574dde6ff3b6c5be7ed7dab311888329b5880f92", "class_name": "RelatedNodeInfo"}}, "text": "So a 2X2 image will be made up of 4 pixels which can be visualized as:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                                  1/144/5/24, 8:30 PM                                                         Intuitions in CNNs\n                                               2\n                           3                    4\n The content of these pixels depend upon the kind of image that we are dealing with.Here we will talk about the most\n common type of image i.e. RGB image.RGB stands for Red,Green and Blue that constitute the primary colours.\n In a RGB image,each pixel consists of three channels one each for red,green and blue.Our aim is to use these three\n colours(channels) in di\u25aferent amounts to generate rest of the colours which would in turn generate the image.To\n control the colour amount,we divide each channel into something calledbit depth.Usually the value of                   bit-depthis\n 8.Each of these 8 bit consists value of 1 or 0 and thus the value of channel can range from 0 to 2^8-1 i.e.", "start_char_idx": 1787, "end_char_idx": 2877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0f20ee78-5713-488c-a266-562a3aa8cd71": {"__data__": {"id_": "0f20ee78-5713-488c-a266-562a3aa8cd71", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "b9621e3975a7486da56c848f3400831dcffea2c83ff89c69c89b214a0c1a70c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7a673c8-95b4-4f87-a6c1-09415685ca33", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "757c738a1a1a935323da675eb499c3c660a7c08ae35523797ad7081a44553af3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4cd6b04-1898-4615-86c6-93e94f6cfa75", "node_type": "1", "metadata": {}, "hash": "023245ebafecbd98a13a2c03ae8e59bd8d515e81964c26b4da28b6c78ed6ba9a", "class_name": "RelatedNodeInfo"}}, "text": "from 0 to\n 255.The combination of values from 0 to 255 of these three channels gives the value of a pixel and thus imparts it the\n resulting colour.For example,the combination (64,224,208) gives the colour Turquoise:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                                    2/144/5/24, 8:30 PM                                        Intuitions in CNNs\n                                             pixel\n                                                              TURQUOISE\n  Thus any RGB image of resolution AXB can be represented in 3 dimensions in form of AXBX3 where 3 is the number of\n  channels(Red,Green and blue).Below is representation of a 200X200 RGB image splitted in three channels:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                      3/144/5/24, 8:30 PM                                                              Intuitions in CNNs\n  Now that we know basic representation of an image in form of pixels we can move forward to motivate the idea of\n  Convolutional Neural Networks.", "start_char_idx": 2878, "end_char_idx": 4000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b4cd6b04-1898-4615-86c6-93e94f6cfa75": {"__data__": {"id_": "b4cd6b04-1898-4615-86c6-93e94f6cfa75", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "b9621e3975a7486da56c848f3400831dcffea2c83ff89c69c89b214a0c1a70c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f20ee78-5713-488c-a266-562a3aa8cd71", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "a3ebb8dc489a7f7498ebd60594fb314bd14df562d9c2e561f364201885c12081", "class_name": "RelatedNodeInfo"}}, "text": "Motivating CNNs\n  Imagine we are given a set of images and their corresponding labels and are asked to come up with a image\n  classification system.What could be the possible approaches?\n        One of the straightforward approach can be to come up with a vanilla feed forward network with all the pixels of\n        our image as inputs.In theory this should work wonders as we would have all the information we need to correctly\n        classify the images.But this approach is pretty naive and does not scale well for images with standard\n        resolutions.Lets take an example to understand the problem.Say our input images have resolution\n        1280X720(which is really a lesser assumption considering todays cameras!", "start_char_idx": 4003, "end_char_idx": 4727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f11fa4a8-26ce-4bd5-95f1-ab050005ebae": {"__data__": {"id_": "f11fa4a8-26ce-4bd5-95f1-ab050005ebae", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "b9621e3975a7486da56c848f3400831dcffea2c83ff89c69c89b214a0c1a70c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e0a623c-91ea-4081-93c4-64408078ad28", "node_type": "1", "metadata": {}, "hash": "16c2fea65e9021d176d640f071e67609d8e506d1b84229b47f7fdcbfaf4de045", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:30 PM                                                             Intuitions in CNNs\n  Tensorflow                                                                                                                  HOME\n  Intuitions in Convolutional Neural Networks\n  In previousblogposts      we explored the basics of tensorflow.In this series of posts we will look into di\u25aferent types of\n  neural networks and their implementation in Tensorflow.In this post we will look into the gory details of convolutional\n  neural networks(CNNs) from motivation to reasoning their e\u25afectiveness. As we will mostly deal with applying CNNs on\n  images,we will start by looking into digital representation of images and how filters work on an image.We will also look\n  into some theoretical aspects of CNNs which would help us to implement them in coming posts.\n  Lets get started!!!\n  Digital representation of images\n  We all have(hopefully!) drawn pictures on canvas or a sheet of paper at some point in our lives.We would use di\u25aferent\n  colours to accomplish this task.The concept of colour,to most of us,feels pretty abstract.Therefore the representation of\n  images on a screen rather than a piece of paper needed us to express this abstraction in form of a concept.Thus the\n  concept of \u201cpixel\u201d was motivated.\n  Images are digitally represented in form of \u201cpixels\u201d.These are the building blocks of digital images.You can visualize\n  pixels as tiny rectangles of lights that impart properties to an image.\n  To further clarify this representation,let us look at concept of resolution.When we state that an image has resolution\n  (2048X1536) we necessarily mean that it has 2048 pixels in width and 1536 pixels in height and thus has total of\n  2048X1536=3,145,7 pixels or 3.1 megapixels. So a 2X2 image will be made up of 4 pixels which can be visualized as:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                                  1/144/5/24, 8:30 PM                                                         Intuitions in CNNs\n                                               2\n                           3                    4\n The content of these pixels depend upon the kind of image that we are dealing with.Here we will talk about the most\n common type of image i.e.", "start_char_idx": 0, "end_char_idx": 2326, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1e0a623c-91ea-4081-93c4-64408078ad28": {"__data__": {"id_": "1e0a623c-91ea-4081-93c4-64408078ad28", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "b9621e3975a7486da56c848f3400831dcffea2c83ff89c69c89b214a0c1a70c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f11fa4a8-26ce-4bd5-95f1-ab050005ebae", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "94e37474e121171d518c6b203e7a452a98f514c6c609e87fa2a00a298e910b54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab746730-5830-43b7-a4ac-74e7b7c407a7", "node_type": "1", "metadata": {}, "hash": "2cdbaa20baf4763945bc1bea818f2841fec70c1939e3be361bbf8a931cc13f33", "class_name": "RelatedNodeInfo"}}, "text": "RGB image.RGB stands for Red,Green and Blue that constitute the primary colours.\n In a RGB image,each pixel consists of three channels one each for red,green and blue.Our aim is to use these three\n colours(channels) in di\u25aferent amounts to generate rest of the colours which would in turn generate the image.To\n control the colour amount,we divide each channel into something calledbit depth.Usually the value of                   bit-depthis\n 8.Each of these 8 bit consists value of 1 or 0 and thus the value of channel can range from 0 to 2^8-1 i.e. from 0 to\n 255.The combination of values from 0 to 255 of these three channels gives the value of a pixel and thus imparts it the\n resulting colour.For example,the combination (64,224,208) gives the colour Turquoise:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                                    2/144/5/24, 8:30 PM                                        Intuitions in CNNs\n                                             pixel\n                                                              TURQUOISE\n  Thus any RGB image of resolution AXB can be represented in 3 dimensions in form of AXBX3 where 3 is the number of\n  channels(Red,Green and blue).Below is representation of a 200X200 RGB image splitted in three channels:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                      3/144/5/24, 8:30 PM                                                              Intuitions in CNNs\n  Now that we know basic representation of an image in form of pixels we can move forward to motivate the idea of\n  Convolutional Neural Networks.\n  Motivating CNNs\n  Imagine we are given a set of images and their corresponding labels and are asked to come up with a image\n  classification system.What could be the possible approaches?", "start_char_idx": 2327, "end_char_idx": 4189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ab746730-5830-43b7-a4ac-74e7b7c407a7": {"__data__": {"id_": "ab746730-5830-43b7-a4ac-74e7b7c407a7", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "b9621e3975a7486da56c848f3400831dcffea2c83ff89c69c89b214a0c1a70c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e0a623c-91ea-4081-93c4-64408078ad28", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "9d44d5439bc1ac9aaaf4909ab796abebdc7396e7d9e3051ede1fd27409e6f2e9", "class_name": "RelatedNodeInfo"}}, "text": "One of the straightforward approach can be to come up with a vanilla feed forward network with all the pixels of\n        our image as inputs.In theory this should work wonders as we would have all the information we need to correctly\n        classify the images.But this approach is pretty naive and does not scale well for images with standard\n        resolutions.Lets take an example to understand the problem.Say our input images have resolution\n        1280X720(which is really a lesser assumption considering todays cameras!", "start_char_idx": 4198, "end_char_idx": 4727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-0": {"__data__": {"id_": "node-0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a8a2edd-962c-4b7d-8b73-4124f977c048", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "23b398f45bbc207b8ed8902a254925923e0496b13660344ec8b7dede767ad4b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43f53caf-aead-4853-b0d0-15103bb0ccab", "node_type": "1", "metadata": {}, "hash": "b40d0c251780e61934dd96801402487b5465033658192e41661f86410038b7d3", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:30 PM                                                             Intuitions in CNNs\n  Tensorflow                                                                                                                  HOME\n  Intuitions in Convolutional Neural Networks\n  In previousblogposts      we explored the basics of tensorflow.In this series of posts we will look into di\u25aferent types of\n  neural networks and their implementation in Tensorflow.In this post we will look into the gory details of convolutional\n  neural networks(CNNs) from motivation to reasoning their e\u25afectiveness. As we will mostly deal with applying CNNs on\n  images,we will start by looking into digital representation of images and how filters work on an image.We will also look\n  into some theoretical aspects of CNNs which would help us to implement them in coming posts.\n  Lets get started!!!\n  Digital representation of images\n  We all have(hopefully!) drawn pictures on canvas or a sheet of paper at some point in our lives.We would use di\u25aferent\n  colours to accomplish this task.The concept of colour,to most of us,feels pretty abstract.Therefore the representation of\n  images on a screen rather than a piece of paper needed us to express this abstraction in form of a concept.Thus the\n  concept of \u201cpixel\u201d was motivated.\n  Images are digitally represented in form of \u201cpixels\u201d.These are the building blocks of digital images.You can visualize\n  pixels as tiny rectangles of lights that impart properties to an image.\n  To further clarify this representation,let us look at concept of resolution.When we state that an image has resolution\n  (2048X1536) we necessarily mean that it has 2048 pixels in width and 1536 pixels in height and thus has total of\n  2048X1536=3,145,7 pixels or 3.1 megapixels. So a 2X2 image will be made up of 4 pixels which can be visualized as:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                                  1/144/5/24, 8:30 PM                                                         Intuitions in CNNs\n                                               2\n                           3                    4\n The content of these pixels depend upon the kind of image that we are dealing with.Here we will talk about the most\n common type of image i.e. RGB image.RGB stands for Red,Green and Blue that constitute the primary colours.\n In a RGB image,each pixel consists of three channels one each for red,green and blue.Our aim is to use these three\n colours(channels) in di\u25aferent amounts to generate rest of the colours which would in turn generate the image.To\n control the colour amount,we divide each channel into something calledbit depth.Usually the value of                   bit-depthis\n 8.Each of these 8 bit consists value of 1 or 0 and thus the value of channel can range from 0 to 2^8-1 i.e. from 0 to\n 255.The combination of values from 0 to 255 of these three channels gives the value of a pixel and thus imparts it the\n resulting colour.For example,the combination (64,224,208) gives the colour Turquoise:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                                    2/144/5/24, 8:30 PM                                        Intuitions in CNNs\n                                             pixel\n                                                              TURQUOISE\n  Thus any RGB image of resolution AXB can be represented in 3 dimensions in form of AXBX3 where 3 is the number of\n  channels(Red,Green and blue).Below is representation of a 200X200 RGB image splitted in three channels:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                      3/144/5/24, 8:30 PM                                                              Intuitions in CNNs\n  Now that we know basic representation of an image in form of pixels we can move forward to motivate the idea of\n  Convolutional Neural Networks.\n  Motivating CNNs\n  Imagine we are given a set of images and their corresponding labels and are asked to come up with a image\n  classification system.What could be the possible approaches?\n        One of the straightforward approach can be to come up with a vanilla feed forward network with all the pixels of\n        our image as inputs.In theory this should work wonders as we would have all the information we need to correctly\n        classify the images.But this approach is pretty naive and does not scale well for images with standard\n        resolutions.Lets take an example to understand the problem.Say our input images have resolution\n        1280X720(which is really a lesser assumption considering todays cameras!", "start_char_idx": 0, "end_char_idx": 4727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a966622b-0d36-436f-b849-f6bc2e884851": {"__data__": {"id_": "a966622b-0d36-436f-b849-f6bc2e884851", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0668c141264ac707b2fa6fe58b2f4af4eb0e4e95499e27502b00ce7c9698a0db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aeca82bf-6a35-48f0-a02b-258d6bd17270", "node_type": "1", "metadata": {}, "hash": "f114b8398119b45e7adc2e9e76a6d9031d0381aeb8149840f8b19c21d0b8bfaf", "class_name": "RelatedNodeInfo"}}, "text": ").Thus our input images have around 1\n        million pixels and same will be number of input nodes of our neural network.If we assume that our hidden layer\n        has 10,000 nodes(resonable assumption),the size of our weight matrix will shoot up to 1 millionX10,000 trainable\n        elements!!This is just the first hidden layer!This is pure crazy!!!Thus for even lesser resolution images using a\n        vanilla neural network is computationally too expensive.\n        Another approach which can eliminate the problem of expensive computations is if somehow we are able to\n        decrease number of pixels from input image.If somehow we are able to eliminate the noisy pixels from our image\n        and keep only those that are essential for image recognition.We are talking about manual feature\n        selection.Although manual feature selection may prove to be handy in designing of systems such as anomaly\n        detection but while dealing with images it does not help much.You would not know how a network distinguishes\n        between an image of your and your friend\u02bcs face.", "start_char_idx": 0, "end_char_idx": 1088, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "aeca82bf-6a35-48f0-a02b-258d6bd17270": {"__data__": {"id_": "aeca82bf-6a35-48f0-a02b-258d6bd17270", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0668c141264ac707b2fa6fe58b2f4af4eb0e4e95499e27502b00ce7c9698a0db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a966622b-0d36-436f-b849-f6bc2e884851", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0ac46702fc039507153fdaa79c15752c08d91a39fd306b53a1b2ca306881b6a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "749d43b9-6f2c-40c4-b01b-7488c391c665", "node_type": "1", "metadata": {}, "hash": "cc88715bc8e3517741925773d3a752e9d7f8f5df6566b3ad37e02c5cc218d090", "class_name": "RelatedNodeInfo"}}, "text": "Thus we can now say that we need a network arrangement that limits the number of parameters and also learns the\n  necessary features by itself.Enter CNNs\n  Convolutional neural network\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                          4/144/5/24, 8:30 PM                                                                Intuitions in CNNs\n  Working of a filter\n  The basic structure of a CNN is a little di\u25aferent from vanilla feed forward neural network.Instead of consecutive layers\n  being fully connected,we introduce the concept of \u201cfilters\u201d.When we talk about \u201cfiltering\u201d in image processing we are\n  talking about emphasizing or removing certain features.A filter can be visualized as multidimensional matrix of\n  weights.These weights are usually governed by some pattern or principle suitably selected to emphasize or remove\n  features.Filters are usually smaller than our image and are convoluted all over the image.During these convolutions,\n  pixels of our image are modified so as to yield desired modifications in image.", "start_char_idx": 1091, "end_char_idx": 2195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "749d43b9-6f2c-40c4-b01b-7488c391c665": {"__data__": {"id_": "749d43b9-6f2c-40c4-b01b-7488c391c665", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0668c141264ac707b2fa6fe58b2f4af4eb0e4e95499e27502b00ce7c9698a0db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aeca82bf-6a35-48f0-a02b-258d6bd17270", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "fc4af7e366ecae1d4df37b50c83fd7db7d74d5b5b3c090bd82441bb212e55956", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03b8c283-b8fb-42cd-8552-25c3735b68c1", "node_type": "1", "metadata": {}, "hash": "4686ae2ecfa0fd09893556d97ced49a14184463b9bf23481dff8a70f3f1e060f", "class_name": "RelatedNodeInfo"}}, "text": "Let us make things more clearer with\n  help of an example:\n  Imagine you are given a 5X5 image and a 3X3 gaussian filter(A filter whose weights are sampled from a gaussian\n  distribution) and you want to apply the filter on the given image.Here \u201capplying filter\u201d is nothing but convolving filter\n  all over the image.", "start_char_idx": 2195, "end_char_idx": 2512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "03b8c283-b8fb-42cd-8552-25c3735b68c1": {"__data__": {"id_": "03b8c283-b8fb-42cd-8552-25c3735b68c1", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0668c141264ac707b2fa6fe58b2f4af4eb0e4e95499e27502b00ce7c9698a0db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "749d43b9-6f2c-40c4-b01b-7488c391c665", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "bae01dcc6056371339070d439567e9f1518f07be875e39941c79d64029a3e83a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eef4c5d2-65ad-468d-adf6-c7ebb1553ed0", "node_type": "1", "metadata": {}, "hash": "8332a2da7aaf99add51d0aa1ec5aee099fbbc82635f8c2cc3957aa12f053e369", "class_name": "RelatedNodeInfo"}}, "text": "50         46         100         47        45\n                  48         50        105         48        43\n                 46         48         101          49       50\n                 49         47         100          50        52\n                 50         48         110         47        48\n  Above figure shows how filter is used to modify pixels of our image.Imagine the filter being placed over our image in\n  such a way that the like coloured numbers in our filter and image coincide.This would mean that (50,green) of our\n  image would coincide with (4,green) of the filter.An operation of weighted mean is carried out as shown in figure.The\n  central pixel(50,green) is modified with the obtained mean value(62).Similarly this operation is carried out by moving\n  the filter such that every pixel of our image gets to be the central pixel(for now neglect the edges when filter would\n  overshoot our image).\n  How does the filter a\u25afects our image?", "start_char_idx": 2530, "end_char_idx": 3495, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "eef4c5d2-65ad-468d-adf6-c7ebb1553ed0": {"__data__": {"id_": "eef4c5d2-65ad-468d-adf6-c7ebb1553ed0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0668c141264ac707b2fa6fe58b2f4af4eb0e4e95499e27502b00ce7c9698a0db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03b8c283-b8fb-42cd-8552-25c3735b68c1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "c5c520e1394ad2950d415c833e3d84a9d94a434de6af34e0ccca1a1e8232343b", "class_name": "RelatedNodeInfo"}}, "text": "Di\u25aferent filters have di\u25aferent e\u25afect on the image.The gaussian filter is used as\n  preprocessing step in image processing and is responsible for smoothing and removing noise.It leaves a blurring\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                            5/144/5/24, 8:30 PM                                                       Intuitions in CNNs\n e\u25afect.Filters like sobel are used to highlight edges.Thus depending upon their weights,di\u25aferent filters\n emphasize/remove di\u25aferent features.\n CNNs,Finally!!\n As the input to a CNN is an image,therefore it operates on volume.The input pixels are stacked in form of volume axbxc\n where axb is the resolution of the image and c is number of channels(which is 3 for RGB).This is called as input\n layer.The input layer is subjected to a number of filters to generate activations.These activations along with filters form\n the second layer of our network(or the first convolution layer).The number of channels in each filter is equal to number\n of channels in our image.", "start_char_idx": 3496, "end_char_idx": 4574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "793dedf2-504a-46b1-ab83-6f89f3a26eca": {"__data__": {"id_": "793dedf2-504a-46b1-ab83-6f89f3a26eca", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0668c141264ac707b2fa6fe58b2f4af4eb0e4e95499e27502b00ce7c9698a0db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63b94462-c2c4-4a8e-b971-4567d3da6dfb", "node_type": "1", "metadata": {}, "hash": "ec89e93aa0d4cc1fe0194999c6dd6468983b3b3c7c078c12ae18e85b933850cf", "class_name": "RelatedNodeInfo"}}, "text": ").Thus our input images have around 1\n        million pixels and same will be number of input nodes of our neural network.If we assume that our hidden layer\n        has 10,000 nodes(resonable assumption),the size of our weight matrix will shoot up to 1 millionX10,000 trainable\n        elements!!This is just the first hidden layer!This is pure crazy!!!Thus for even lesser resolution images using a\n        vanilla neural network is computationally too expensive.\n        Another approach which can eliminate the problem of expensive computations is if somehow we are able to\n        decrease number of pixels from input image.If somehow we are able to eliminate the noisy pixels from our image\n        and keep only those that are essential for image recognition.We are talking about manual feature\n        selection.Although manual feature selection may prove to be handy in designing of systems such as anomaly\n        detection but while dealing with images it does not help much.You would not know how a network distinguishes\n        between an image of your and your friend\u02bcs face.", "start_char_idx": 0, "end_char_idx": 1088, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "63b94462-c2c4-4a8e-b971-4567d3da6dfb": {"__data__": {"id_": "63b94462-c2c4-4a8e-b971-4567d3da6dfb", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0668c141264ac707b2fa6fe58b2f4af4eb0e4e95499e27502b00ce7c9698a0db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "793dedf2-504a-46b1-ab83-6f89f3a26eca", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0ac46702fc039507153fdaa79c15752c08d91a39fd306b53a1b2ca306881b6a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c79751eb-158f-468c-bbc3-a397833cebe7", "node_type": "1", "metadata": {}, "hash": "ca4a4e18847618556b9961f268d94c5e4cfa473a5a837abab37ffc7ba30f8ee5", "class_name": "RelatedNodeInfo"}}, "text": "Thus we can now say that we need a network arrangement that limits the number of parameters and also learns the\n  necessary features by itself.Enter CNNs\n  Convolutional neural network\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                          4/144/5/24, 8:30 PM                                                                Intuitions in CNNs\n  Working of a filter\n  The basic structure of a CNN is a little di\u25aferent from vanilla feed forward neural network.Instead of consecutive layers\n  being fully connected,we introduce the concept of \u201cfilters\u201d.When we talk about \u201cfiltering\u201d in image processing we are\n  talking about emphasizing or removing certain features.A filter can be visualized as multidimensional matrix of\n  weights.These weights are usually governed by some pattern or principle suitably selected to emphasize or remove\n  features.Filters are usually smaller than our image and are convoluted all over the image.During these convolutions,\n  pixels of our image are modified so as to yield desired modifications in image.Let us make things more clearer with\n  help of an example:\n  Imagine you are given a 5X5 image and a 3X3 gaussian filter(A filter whose weights are sampled from a gaussian\n  distribution) and you want to apply the filter on the given image.Here \u201capplying filter\u201d is nothing but convolving filter\n  all over the image.", "start_char_idx": 1091, "end_char_idx": 2512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c79751eb-158f-468c-bbc3-a397833cebe7": {"__data__": {"id_": "c79751eb-158f-468c-bbc3-a397833cebe7", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0668c141264ac707b2fa6fe58b2f4af4eb0e4e95499e27502b00ce7c9698a0db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63b94462-c2c4-4a8e-b971-4567d3da6dfb", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "e16c54213999edd113c3eb0c4323beb30bac510c654e82f595122943eee39ce7", "class_name": "RelatedNodeInfo"}}, "text": "50         46         100         47        45\n                  48         50        105         48        43\n                 46         48         101          49       50\n                 49         47         100          50        52\n                 50         48         110         47        48\n  Above figure shows how filter is used to modify pixels of our image.Imagine the filter being placed over our image in\n  such a way that the like coloured numbers in our filter and image coincide.This would mean that (50,green) of our\n  image would coincide with (4,green) of the filter.An operation of weighted mean is carried out as shown in figure.The\n  central pixel(50,green) is modified with the obtained mean value(62).Similarly this operation is carried out by moving\n  the filter such that every pixel of our image gets to be the central pixel(for now neglect the edges when filter would\n  overshoot our image).\n  How does the filter a\u25afects our image? Di\u25aferent filters have di\u25aferent e\u25afect on the image.The gaussian filter is used as\n  preprocessing step in image processing and is responsible for smoothing and removing noise.It leaves a blurring\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                            5/144/5/24, 8:30 PM                                                       Intuitions in CNNs\n e\u25afect.Filters like sobel are used to highlight edges.Thus depending upon their weights,di\u25aferent filters\n emphasize/remove di\u25aferent features.\n CNNs,Finally!!\n As the input to a CNN is an image,therefore it operates on volume.The input pixels are stacked in form of volume axbxc\n where axb is the resolution of the image and c is number of channels(which is 3 for RGB).This is called as input\n layer.The input layer is subjected to a number of filters to generate activations.These activations along with filters form\n the second layer of our network(or the first convolution layer).The number of channels in each filter is equal to number\n of channels in our image.", "start_char_idx": 2530, "end_char_idx": 4574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-1": {"__data__": {"id_": "node-1", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a8a2edd-962c-4b7d-8b73-4124f977c048", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "23b398f45bbc207b8ed8902a254925923e0496b13660344ec8b7dede767ad4b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e95c174b-3aa6-43da-8cbf-180effdb0958", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "b9621e3975a7486da56c848f3400831dcffea2c83ff89c69c89b214a0c1a70c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c510c8d6-d457-45f1-8201-a30f1a121df9", "node_type": "1", "metadata": {}, "hash": "e23bcd3e0b771c0506f21f056d3b97cde9787fca096d006d5f56afea8e9fbe82", "class_name": "RelatedNodeInfo"}}, "text": ").Thus our input images have around 1\n        million pixels and same will be number of input nodes of our neural network.If we assume that our hidden layer\n        has 10,000 nodes(resonable assumption),the size of our weight matrix will shoot up to 1 millionX10,000 trainable\n        elements!!This is just the first hidden layer!This is pure crazy!!!Thus for even lesser resolution images using a\n        vanilla neural network is computationally too expensive.\n        Another approach which can eliminate the problem of expensive computations is if somehow we are able to\n        decrease number of pixels from input image.If somehow we are able to eliminate the noisy pixels from our image\n        and keep only those that are essential for image recognition.We are talking about manual feature\n        selection.Although manual feature selection may prove to be handy in designing of systems such as anomaly\n        detection but while dealing with images it does not help much.You would not know how a network distinguishes\n        between an image of your and your friend\u02bcs face.\n  Thus we can now say that we need a network arrangement that limits the number of parameters and also learns the\n  necessary features by itself.Enter CNNs\n  Convolutional neural network\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                          4/144/5/24, 8:30 PM                                                                Intuitions in CNNs\n  Working of a filter\n  The basic structure of a CNN is a little di\u25aferent from vanilla feed forward neural network.Instead of consecutive layers\n  being fully connected,we introduce the concept of \u201cfilters\u201d.When we talk about \u201cfiltering\u201d in image processing we are\n  talking about emphasizing or removing certain features.A filter can be visualized as multidimensional matrix of\n  weights.These weights are usually governed by some pattern or principle suitably selected to emphasize or remove\n  features.Filters are usually smaller than our image and are convoluted all over the image.During these convolutions,\n  pixels of our image are modified so as to yield desired modifications in image.Let us make things more clearer with\n  help of an example:\n  Imagine you are given a 5X5 image and a 3X3 gaussian filter(A filter whose weights are sampled from a gaussian\n  distribution) and you want to apply the filter on the given image.Here \u201capplying filter\u201d is nothing but convolving filter\n  all over the image.\n                 50         46         100         47        45\n                  48         50        105         48        43\n                 46         48         101          49       50\n                 49         47         100          50        52\n                 50         48         110         47        48\n  Above figure shows how filter is used to modify pixels of our image.Imagine the filter being placed over our image in\n  such a way that the like coloured numbers in our filter and image coincide.This would mean that (50,green) of our\n  image would coincide with (4,green) of the filter.An operation of weighted mean is carried out as shown in figure.The\n  central pixel(50,green) is modified with the obtained mean value(62).Similarly this operation is carried out by moving\n  the filter such that every pixel of our image gets to be the central pixel(for now neglect the edges when filter would\n  overshoot our image).\n  How does the filter a\u25afects our image? Di\u25aferent filters have di\u25aferent e\u25afect on the image.The gaussian filter is used as\n  preprocessing step in image processing and is responsible for smoothing and removing noise.It leaves a blurring\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                            5/144/5/24, 8:30 PM                                                       Intuitions in CNNs\n e\u25afect.Filters like sobel are used to highlight edges.Thus depending upon their weights,di\u25aferent filters\n emphasize/remove di\u25aferent features.\n CNNs,Finally!!\n As the input to a CNN is an image,therefore it operates on volume.The input pixels are stacked in form of volume axbxc\n where axb is the resolution of the image and c is number of channels(which is 3 for RGB).This is called as input\n layer.The input layer is subjected to a number of filters to generate activations.These activations along with filters form\n the second layer of our network(or the first convolution layer).The number of channels in each filter is equal to number\n of channels in our image.", "start_char_idx": 4727, "end_char_idx": 9301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c07a6867-57ea-421d-b6bd-569c214b2a54": {"__data__": {"id_": "c07a6867-57ea-421d-b6bd-569c214b2a54", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0bb1b5c8a8003abab24a4f654463a479212f52ce87cf8c4a31dfa16184502bbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e11b3a6d-e17c-49a1-b15a-9b017a80c498", "node_type": "1", "metadata": {}, "hash": "418a9f75d59433caf27650ca2ba5d9fcdf866afeacfbf8228df5245f945d9976", "class_name": "RelatedNodeInfo"}}, "text": "https://jasdeep06.github.io/posts/basics-of-cnns/                                                                             6/144/5/24, 8:30 PM                Intuitions in CNNs\nhttps://jasdeep06.github.io/posts/basics-of-cnns/  7/144/5/24, 8:30 PM                                                          Intuitions in CNNs\n  The first of the above two figures shows convolution of 5X5X3 filter on 32X32X3 image.Note that the spatial dimensions\n  of activation map is 28X28.This spatial dimension is smaller because we are only sliding the filter over the image such\n  that it does not overshoots the image.(We will see a concrete mathematical formula to reach to smaller spatial\n  dimension) The second figure shows four 5X5X3 filters stacked to produce four 28X28X1 activations thus giving rise to\n  28X28X4 activation map.Note that each of the four filters would have access to the input layer and operate directly on\n  our image.", "start_char_idx": 0, "end_char_idx": 936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e11b3a6d-e17c-49a1-b15a-9b017a80c498": {"__data__": {"id_": "e11b3a6d-e17c-49a1-b15a-9b017a80c498", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0bb1b5c8a8003abab24a4f654463a479212f52ce87cf8c4a31dfa16184502bbb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c07a6867-57ea-421d-b6bd-569c214b2a54", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "7fc5412af920893eabdf996c0e3a2e68013a5e4fdbdacaea6aa401a786b09b76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8117b684-370c-4cf5-98d0-218e3c91d4c3", "node_type": "1", "metadata": {}, "hash": "805be029927ad7efdf2bb320bb772f1f64163a4ed070ea79182973eff1ae350a", "class_name": "RelatedNodeInfo"}}, "text": "This activation map is passed through a non-linearity(most commonly ReLU).The output is then further exposed to set\n  of filters which will have access to our ReLU applied activation map(and not the input image).To summarize,filters\n  would be applied on the activation of previous layer.We can stack up as many layers as required.Thus the network\n  looks like:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                          8/144/5/24, 8:30 PM                                                           Intuitions in CNNs\n                  32                                   28                                       24\n                       CONV,                                    CONV,                                 CONV,\n                        ReLU                                    ReLU                                   ReLU\n                       e.g: 6                                   e.g: 10\n             32        5x5x3                      28            Sx5x6                      24\n                       filters                                  filters         10\n  A\u25afer stacking suitable numbers of convolution layers,we finally connect the activation map to fully connected layer\n  which gives us scores for di\u25aferent classes to be classified.", "start_char_idx": 939, "end_char_idx": 2265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "8117b684-370c-4cf5-98d0-218e3c91d4c3": {"__data__": {"id_": "8117b684-370c-4cf5-98d0-218e3c91d4c3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0bb1b5c8a8003abab24a4f654463a479212f52ce87cf8c4a31dfa16184502bbb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e11b3a6d-e17c-49a1-b15a-9b017a80c498", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "d3abf02d426eb518f48af3946054943228a1059697f84b3e558863296bbd17e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfbfa998-b3a4-43f9-a7be-90bb65e7c14c", "node_type": "1", "metadata": {}, "hash": "3c5ed7913a37cfe2807f4f1b148a042252d5a04938757f0150799936322318a4", "class_name": "RelatedNodeInfo"}}, "text": "Trainable parameters\n  The weights of the filters are randomly initialized.We let our network learn these weights during training.This captures\n  the fact that we let the network decide which features are important for identification of a particular image.Also as\n  filter weights(and biases) and the weights of the last fully connected layer are the only trainable parameters,CNNs\n  have satisfied both the motivations(to decrease number of parameters and feature learning).\n  Padding\n  While applying filter to our images,we noticed that their spatial dimensions diminished.This happened because we\n  moved filter only as long as they reached the edge of our image. Thus few pixels at the edges did not get the chance to\n  be at the center of the filter.To visualize this have a look at the figure below:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                           9/144/5/24, 8:30 PM                                                          Intuitions in CNNs\n  In the above filter convolution as we reach the next state of filter,we move one step towards right.This step is called as\n  stride.In above case stride=1.", "start_char_idx": 2268, "end_char_idx": 3452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "dfbfa998-b3a4-43f9-a7be-90bb65e7c14c": {"__data__": {"id_": "dfbfa998-b3a4-43f9-a7be-90bb65e7c14c", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0bb1b5c8a8003abab24a4f654463a479212f52ce87cf8c4a31dfa16184502bbb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8117b684-370c-4cf5-98d0-218e3c91d4c3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "3668730b07579909489a9f63c75bd19197a7a0e324f14de8595b202f567f9f02", "class_name": "RelatedNodeInfo"}}, "text": "Although reaching to diminished dimension can easily be visualized in case of small\n  images but for larger ones we need to have a mathematical formula(or not!).The output size is given by:\n  Output     size=(N-F)/stride          +1   where    N  is   dimension      of  input image,F         is  dimension       of  filter.\n  In this case,N=7 and F=3 so, output size=(7-3)/1 +1=5.\n  This diminishing of spatial dimensions is undesirable as when our networks gets deep with increase in number of\n  convolution layers,the decrease in spatial dimension would be a problem as it will take away significant chunks of\n  information.To counter this problem,Padding is introduced.", "start_char_idx": 3453, "end_char_idx": 4126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7fe7c3f6-4908-49bc-8c08-68ab91d6ab20": {"__data__": {"id_": "7fe7c3f6-4908-49bc-8c08-68ab91d6ab20", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0bb1b5c8a8003abab24a4f654463a479212f52ce87cf8c4a31dfa16184502bbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c80c275-b436-4b9a-b633-b514c3c73e42", "node_type": "1", "metadata": {}, "hash": "1473841a8f41dbeb3f38fdaa6e3509e3fcb1da9b3522b85308fb834802d57773", "class_name": "RelatedNodeInfo"}}, "text": "https://jasdeep06.github.io/posts/basics-of-cnns/                                                                             6/144/5/24, 8:30 PM                Intuitions in CNNs\nhttps://jasdeep06.github.io/posts/basics-of-cnns/  7/144/5/24, 8:30 PM                                                          Intuitions in CNNs\n  The first of the above two figures shows convolution of 5X5X3 filter on 32X32X3 image.Note that the spatial dimensions\n  of activation map is 28X28.This spatial dimension is smaller because we are only sliding the filter over the image such\n  that it does not overshoots the image.(We will see a concrete mathematical formula to reach to smaller spatial\n  dimension) The second figure shows four 5X5X3 filters stacked to produce four 28X28X1 activations thus giving rise to\n  28X28X4 activation map.Note that each of the four filters would have access to the input layer and operate directly on\n  our image.\n  This activation map is passed through a non-linearity(most commonly ReLU).The output is then further exposed to set\n  of filters which will have access to our ReLU applied activation map(and not the input image).To summarize,filters\n  would be applied on the activation of previous layer.We can stack up as many layers as required.Thus the network\n  looks like:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                          8/144/5/24, 8:30 PM                                                           Intuitions in CNNs\n                  32                                   28                                       24\n                       CONV,                                    CONV,                                 CONV,\n                        ReLU                                    ReLU                                   ReLU\n                       e.g: 6                                   e.g: 10\n             32        5x5x3                      28            Sx5x6                      24\n                       filters                                  filters         10\n  A\u25afer stacking suitable numbers of convolution layers,we finally connect the activation map to fully connected layer\n  which gives us scores for di\u25aferent classes to be classified.", "start_char_idx": 0, "end_char_idx": 2265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5c80c275-b436-4b9a-b633-b514c3c73e42": {"__data__": {"id_": "5c80c275-b436-4b9a-b633-b514c3c73e42", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0bb1b5c8a8003abab24a4f654463a479212f52ce87cf8c4a31dfa16184502bbb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fe7c3f6-4908-49bc-8c08-68ab91d6ab20", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "238fa268668ae873f5756d8d3bb90a322cf1085171176e95a6b3b26bf0ffda40", "class_name": "RelatedNodeInfo"}}, "text": "Trainable parameters\n  The weights of the filters are randomly initialized.We let our network learn these weights during training.This captures\n  the fact that we let the network decide which features are important for identification of a particular image.Also as\n  filter weights(and biases) and the weights of the last fully connected layer are the only trainable parameters,CNNs\n  have satisfied both the motivations(to decrease number of parameters and feature learning).\n  Padding\n  While applying filter to our images,we noticed that their spatial dimensions diminished.This happened because we\n  moved filter only as long as they reached the edge of our image. Thus few pixels at the edges did not get the chance to\n  be at the center of the filter.To visualize this have a look at the figure below:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                           9/144/5/24, 8:30 PM                                                          Intuitions in CNNs\n  In the above filter convolution as we reach the next state of filter,we move one step towards right.This step is called as\n  stride.In above case stride=1. Although reaching to diminished dimension can easily be visualized in case of small\n  images but for larger ones we need to have a mathematical formula(or not!).The output size is given by:\n  Output     size=(N-F)/stride          +1   where    N  is   dimension      of  input image,F         is  dimension       of  filter.\n  In this case,N=7 and F=3 so, output size=(7-3)/1 +1=5.\n  This diminishing of spatial dimensions is undesirable as when our networks gets deep with increase in number of\n  convolution layers,the decrease in spatial dimension would be a problem as it will take away significant chunks of\n  information.To counter this problem,Padding is introduced.", "start_char_idx": 2268, "end_char_idx": 4126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-2": {"__data__": {"id_": "node-2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a8a2edd-962c-4b7d-8b73-4124f977c048", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "23b398f45bbc207b8ed8902a254925923e0496b13660344ec8b7dede767ad4b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43f53caf-aead-4853-b0d0-15103bb0ccab", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0668c141264ac707b2fa6fe58b2f4af4eb0e4e95499e27502b00ce7c9698a0db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca6777ae-dbaf-4f21-b581-59de92e14bd3", "node_type": "1", "metadata": {}, "hash": "a8822c59380ddf657272c30462f1dc0c6311516cf2e9add61233e2985c19a81c", "class_name": "RelatedNodeInfo"}}, "text": "https://jasdeep06.github.io/posts/basics-of-cnns/                                                                             6/144/5/24, 8:30 PM                Intuitions in CNNs\nhttps://jasdeep06.github.io/posts/basics-of-cnns/  7/144/5/24, 8:30 PM                                                          Intuitions in CNNs\n  The first of the above two figures shows convolution of 5X5X3 filter on 32X32X3 image.Note that the spatial dimensions\n  of activation map is 28X28.This spatial dimension is smaller because we are only sliding the filter over the image such\n  that it does not overshoots the image.(We will see a concrete mathematical formula to reach to smaller spatial\n  dimension) The second figure shows four 5X5X3 filters stacked to produce four 28X28X1 activations thus giving rise to\n  28X28X4 activation map.Note that each of the four filters would have access to the input layer and operate directly on\n  our image.\n  This activation map is passed through a non-linearity(most commonly ReLU).The output is then further exposed to set\n  of filters which will have access to our ReLU applied activation map(and not the input image).To summarize,filters\n  would be applied on the activation of previous layer.We can stack up as many layers as required.Thus the network\n  looks like:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                          8/144/5/24, 8:30 PM                                                           Intuitions in CNNs\n                  32                                   28                                       24\n                       CONV,                                    CONV,                                 CONV,\n                        ReLU                                    ReLU                                   ReLU\n                       e.g: 6                                   e.g: 10\n             32        5x5x3                      28            Sx5x6                      24\n                       filters                                  filters         10\n  A\u25afer stacking suitable numbers of convolution layers,we finally connect the activation map to fully connected layer\n  which gives us scores for di\u25aferent classes to be classified.\n  Trainable parameters\n  The weights of the filters are randomly initialized.We let our network learn these weights during training.This captures\n  the fact that we let the network decide which features are important for identification of a particular image.Also as\n  filter weights(and biases) and the weights of the last fully connected layer are the only trainable parameters,CNNs\n  have satisfied both the motivations(to decrease number of parameters and feature learning).\n  Padding\n  While applying filter to our images,we noticed that their spatial dimensions diminished.This happened because we\n  moved filter only as long as they reached the edge of our image. Thus few pixels at the edges did not get the chance to\n  be at the center of the filter.To visualize this have a look at the figure below:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                           9/144/5/24, 8:30 PM                                                          Intuitions in CNNs\n  In the above filter convolution as we reach the next state of filter,we move one step towards right.This step is called as\n  stride.In above case stride=1. Although reaching to diminished dimension can easily be visualized in case of small\n  images but for larger ones we need to have a mathematical formula(or not!).The output size is given by:\n  Output     size=(N-F)/stride          +1   where    N  is   dimension      of  input image,F         is  dimension       of  filter.\n  In this case,N=7 and F=3 so, output size=(7-3)/1 +1=5.\n  This diminishing of spatial dimensions is undesirable as when our networks gets deep with increase in number of\n  convolution layers,the decrease in spatial dimension would be a problem as it will take away significant chunks of\n  information.To counter this problem,Padding is introduced.", "start_char_idx": 9302, "end_char_idx": 13428, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f36980b8-fa61-4c7f-8484-0901f58b2240": {"__data__": {"id_": "f36980b8-fa61-4c7f-8484-0901f58b2240", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4bb8e1f-fdee-4e75-be66-b6d3b5dd4d74", "node_type": "1", "metadata": {}, "hash": "3eca05943f1dd8df102121887320a8edaa2ddef299480bda81ae5b34037689fc", "class_name": "RelatedNodeInfo"}}, "text": "As the name suggests,padding is nothing but padding of images.Our basic problem was that the pixels that were on\n  the edge of our image were le\u25afout by the filter.What if they are no more at the edge?This can be made possible by\n  padding our image with zero pixels.This would allow our filter to traverse along the image beyond the edges and thus\n  solve the problem.If we apply zero pad border to our 7X7 example above:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                                      10/144/5/24, 8:30 PM                                                         Intuitions in CNNs\n For stride=1,the general formula for number of padding borders required to preserve spatial size of our activation is:\n  borders=(F-1)/2\n where F is size of our filter.In this case F=3,so borders=(3-1)/2=1.", "start_char_idx": 0, "end_char_idx": 860, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f4bb8e1f-fdee-4e75-be66-b6d3b5dd4d74": {"__data__": {"id_": "f4bb8e1f-fdee-4e75-be66-b6d3b5dd4d74", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f36980b8-fa61-4c7f-8484-0901f58b2240", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "86d4a8b31993b1042ddaac9047c14409603088714cb502a3f90405858c7a87a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d13cc652-b494-4d83-8501-03e3bccda374", "node_type": "1", "metadata": {}, "hash": "4664b0caadb68c9be5fbd3d1a04476cde11767573d86661047f3d13d0e6c707a", "class_name": "RelatedNodeInfo"}}, "text": "Pooling\n Now that we have all the components of our network,we can suggest some modifications to make our network\n better.To further decrease the number of parameters and to prevent overfitting,we introduce the concept of\n pooling.Pooling involves downsampling of our activation map.The most common form of pooling used is called max-\n pooling. Remember that our activation map consisted of individual activations from di\u25aferent filters.Pooling operation\n is done individually on these activations i.e.", "start_char_idx": 862, "end_char_idx": 1363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d13cc652-b494-4d83-8501-03e3bccda374": {"__data__": {"id_": "d13cc652-b494-4d83-8501-03e3bccda374", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4bb8e1f-fdee-4e75-be66-b6d3b5dd4d74", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0e7a737835ffe4f98186b3a2a9365149ecaedfc03ed0601dc385f9c5be5fa015", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1bcd914-4234-4cc4-9826-1df66c9ce382", "node_type": "1", "metadata": {}, "hash": "35c7c45ee15791ce09d7f37d4b02244aba0dc324c033cd6739df1325f567daeb", "class_name": "RelatedNodeInfo"}}, "text": "pooling operation only modifies the spatial dimension of our activations\n and leaves the number of activations una\u25afected.Let us see max-pooling with help of an example:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                         11/144/5/24, 8:30 PM                                                   Intuitions in CNNs\n  Max pooling is pretty straightforward as shown in above figure.Our 4X4 activation would be traversed by a 2X2 max-\n  pool filter with stride 2.This traversal can be visualized by dividing our 4X4 activation in 4 quadrants.During filter\n  traversal,maximum of each quadrant is preserved and rest of the pixel values are neglected.Thus we get a\n  downsampled max pooled activation.", "start_char_idx": 1364, "end_char_idx": 2126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f1bcd914-4234-4cc4-9826-1df66c9ce382": {"__data__": {"id_": "f1bcd914-4234-4cc4-9826-1df66c9ce382", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d13cc652-b494-4d83-8501-03e3bccda374", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "1da7a662680e151253ceb0b3cf9959ec255abb323cc58d8794edf9bc93f916ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e588bce0-f222-44c2-bb30-e00e21b22dc3", "node_type": "1", "metadata": {}, "hash": "cce265e472fdb5434ffb2c4950a633a6db5e0946742965dfa770be6631a31fc0", "class_name": "RelatedNodeInfo"}}, "text": "This pooling operation is done a\u25afer a few convolution layers.In essence the\n  netwok can be viewed as:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                         12/144/5/24, 8:30 PM                                                        Intuitions in CNNs\n                              CONV RELUCONV RELU POOLCONV RELU CONV RELUPOOLCONV  RELUCONV RELU POOL     FC\n                                                                                                       car\n                                                                                                      ltruck\n                                                                                                      [airplane\n                                                                                                      [ship\n                                                                                                       norse\n Note that the pooling layer is a\u25afer every second convolution layer.It id just matter of whatever works for you!!!Try\n di\u25aferent combinations and pick the best for your network.\n So this post was all about getting a intuitive feel of CNNs.In the next post we will apply CNNs for image classification\n task in Tensorflow.Stay tuned!!                     Posted on 07 February,2017\n  ALSO ON JASDEEP06\n    Further-into                Getting", "start_char_idx": 2127, "end_char_idx": 3538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e588bce0-f222-44c2-bb30-e00e21b22dc3": {"__data__": {"id_": "e588bce0-f222-44c2-bb30-e00e21b22dc3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1bcd914-4234-4cc4-9826-1df66c9ce382", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "e163695ba1d1e80a1ad84efbada2bc08211c99a1df2fcb5f1789f3cde946fd11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7071038b-dc6a-49d7-8ad7-66c43881a6da", "node_type": "1", "metadata": {}, "hash": "0992888cba0713d119da7eafc2f3eb5ee8ecf96005fc07ad8c0e2e564f28b1b5", "class_name": "RelatedNodeInfo"}}, "text": "started with        Lets-Practice-               Understanding LSTM in       Variable-sharing |\n    Further-into-\n    Further-into-               Getting started with\n                                Getting started with        Lets-Practice-\n                                                            Lets-Practice-               Understanding LSTM in", "start_char_idx": 3539, "end_char_idx": 3891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7071038b-dc6a-49d7-8ad7-66c43881a6da": {"__data__": {"id_": "7071038b-dc6a-49d7-8ad7-66c43881a6da", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e588bce0-f222-44c2-bb30-e00e21b22dc3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "49f7a4ebc3bc8173695105d8b0d282610f90578fb488e61a226d4688c40f4645", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1364f3c5-91e1-496d-9fb7-29bfd78f1788", "node_type": "1", "metadata": {}, "hash": "02ddd3d1da08c13286df8e7d7dfb58d3285a3866894b2f3516b0989d4f9a9164", "class_name": "RelatedNodeInfo"}}, "text": "Understanding LSTM in       Variable-sharing-\n    backpropagation                                         Backpropagation              Tensorflow                  Variable-sharing-\n    backpropagation             Tensorflow                                                                           Tensorflow", "start_char_idx": 3981, "end_char_idx": 4289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1364f3c5-91e1-496d-9fb7-29bfd78f1788": {"__data__": {"id_": "1364f3c5-91e1-496d-9fb7-29bfd78f1788", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7071038b-dc6a-49d7-8ad7-66c43881a6da", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "edc90be6fe572decf51350252ef62f3cb5a5d0970eddd07be4a0990d520df1d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30129102-cdab-4014-aeb5-e0bd2b4b04ef", "node_type": "1", "metadata": {}, "hash": "0f8899c652a1b5ca3bb03e395f40ae67f014d6224e1a761a5b8ddf18c7289bf6", "class_name": "RelatedNodeInfo"}}, "text": "backpropagation             Tensor\u25afow\n                                Tensor\u25afow                   Backpropagation\n                                                            Backpropagation              Tensor\u25afow", "start_char_idx": 4294, "end_char_idx": 4506, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "30129102-cdab-4014-aeb5-e0bd2b4b04ef": {"__data__": {"id_": "30129102-cdab-4014-aeb5-e0bd2b4b04ef", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1364f3c5-91e1-496d-9fb7-29bfd78f1788", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "a05f2ad758cb43fe48ea0f6cfe79273e286c8903d113c167b4612bbe1f879ae4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a074e433-a46a-491d-acad-ce6d09ebfb3f", "node_type": "1", "metadata": {}, "hash": "f17ccf249cf296daab01dadf280990be706566bbf48931db92f6d9b927651414", "class_name": "RelatedNodeInfo"}}, "text": "Tensor\u25afow                   Tensor\u25afow\n    7 years ago2 comments                                                                                            Tensor\u25afow\n             \u2022                  7 years ago3 comments\n                                         \u2022                  7 years", "start_char_idx": 4596, "end_char_idx": 4882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a074e433-a46a-491d-acad-ce6d09ebfb3f": {"__data__": {"id_": "a074e433-a46a-491d-acad-ce6d09ebfb3f", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30129102-cdab-4014-aeb5-e0bd2b4b04ef", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "75c667cb52a4548f78eafd107180a536be3cf3a9ca1d13ab8de4b06786ceaa09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "139feaf9-2f7e-45cb-9337-3d870cb056e7", "node_type": "1", "metadata": {}, "hash": "b13c56130a19541c58d120bd307da9c78823444ab3a248683aed5e6bb04ea28d", "class_name": "RelatedNodeInfo"}}, "text": "ago4 comments\n                                                                      \u2022                  7 years ago32 comments\n                                                                                                  \u2022                  7 years ago14 comm", "start_char_idx": 4883, "end_char_idx": 5144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "139feaf9-2f7e-45cb-9337-3d870cb056e7": {"__data__": {"id_": "139feaf9-2f7e-45cb-9337-3d870cb056e7", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a074e433-a46a-491d-acad-ce6d09ebfb3f", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "033994a79fd0fe3746468d06d5815a7f2c2ecbfd12deee77c1dd64ef7d9485f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4e2f371-9eea-4d23-aa2d-9b793569df21", "node_type": "1", "metadata": {}, "hash": "871a1ef701e4f5c883e871e6ab5ba26c5435a4978507cd49bafc8bc541153dce", "class_name": "RelatedNodeInfo"}}, "text": "\u2022\n    Backpropagation : Further   Tensorflow : Getting StartedLets-practice-               CNNs in Tensorflow(cifar-   Tensorflow: Variable\n    into Backpropagation        with Tensorflow             backpropagation              10)                         in Tensorflow\nhttps://jasdeep06.", "start_char_idx": 5271, "end_char_idx": 5560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c4e2f371-9eea-4d23-aa2d-9b793569df21": {"__data__": {"id_": "c4e2f371-9eea-4d23-aa2d-9b793569df21", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "139feaf9-2f7e-45cb-9337-3d870cb056e7", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "40767b3883dd0ff14d86da97b94ab7053f64f8962685a2d5c97ee9f31fb4d1fc", "class_name": "RelatedNodeInfo"}}, "text": "github.io/posts/basics-of-cnns/                                                                                   13/144/5/24, 8:30 PM                Intuitions in CNNs\n 0 Comments                                   \ue603Jasdeep Singh Chhabra\n (QiNG\n      Start the discussion\u2026\n  \uf1091  Share                                      BestNewestOldest\n                         Be thefirst to comment.", "start_char_idx": 5560, "end_char_idx": 5947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f74d5fbd-e1a9-4306-b605-3f7b0120c9e8": {"__data__": {"id_": "f74d5fbd-e1a9-4306-b605-3f7b0120c9e8", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d38ab7c-e7be-4ba9-afd3-f74f1a3fd72e", "node_type": "1", "metadata": {}, "hash": "68177dd2b0f56efafd9f4ee04bef175d9c627faf81d1dd87d644233404b160fe", "class_name": "RelatedNodeInfo"}}, "text": "As the name suggests,padding is nothing but padding of images.Our basic problem was that the pixels that were on\n  the edge of our image were le\u25afout by the filter.What if they are no more at the edge?This can be made possible by\n  padding our image with zero pixels.This would allow our filter to traverse along the image beyond the edges and thus\n  solve the problem.If we apply zero pad border to our 7X7 example above:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                                      10/144/5/24, 8:30 PM                                                         Intuitions in CNNs\n For stride=1,the general formula for number of padding borders required to preserve spatial size of our activation is:\n  borders=(F-1)/2\n where F is size of our filter.In this case F=3,so borders=(3-1)/2=1.\n Pooling\n Now that we have all the components of our network,we can suggest some modifications to make our network\n better.To further decrease the number of parameters and to prevent overfitting,we introduce the concept of\n pooling.Pooling involves downsampling of our activation map.The most common form of pooling used is called max-\n pooling. Remember that our activation map consisted of individual activations from di\u25aferent filters.Pooling operation\n is done individually on these activations i.e. pooling operation only modifies the spatial dimension of our activations\n and leaves the number of activations una\u25afected.Let us see max-pooling with help of an example:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                         11/144/5/24, 8:30 PM                                                   Intuitions in CNNs\n  Max pooling is pretty straightforward as shown in above figure.Our 4X4 activation would be traversed by a 2X2 max-\n  pool filter with stride 2.This traversal can be visualized by dividing our 4X4 activation in 4 quadrants.During filter\n  traversal,maximum of each quadrant is preserved and rest of the pixel values are neglected.Thus we get a\n  downsampled max pooled activation.", "start_char_idx": 0, "end_char_idx": 2126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6d38ab7c-e7be-4ba9-afd3-f74f1a3fd72e": {"__data__": {"id_": "6d38ab7c-e7be-4ba9-afd3-f74f1a3fd72e", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f74d5fbd-e1a9-4306-b605-3f7b0120c9e8", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "6f809cbe75446dcfbc92cce9fa0dbe4872a6d271104a23798529dc75c0faf284", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "080e86c9-6259-4139-982a-b2b0a765e1a2", "node_type": "1", "metadata": {}, "hash": "e1fa70b05e1909d27bdc3227a47d2eee06cabd1456402eb2e55f64763bc65819", "class_name": "RelatedNodeInfo"}}, "text": "This pooling operation is done a\u25afer a few convolution layers.In essence the\n  netwok can be viewed as:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                         12/144/5/24, 8:30 PM                                                        Intuitions in CNNs\n                              CONV RELUCONV RELU POOLCONV RELU CONV RELUPOOLCONV  RELUCONV RELU POOL     FC\n                                                                                                       car\n                                                                                                      ltruck\n                                                                                                      [airplane\n                                                                                                      [ship\n                                                                                                       norse\n Note that the pooling layer is a\u25afer every second convolution layer.It id just matter of whatever works for you!!!Try\n di\u25aferent combinations and pick the best for your network.\n So this post was all about getting a intuitive feel of CNNs.In the next post we will apply CNNs for image classification\n task in Tensorflow.Stay tuned!!", "start_char_idx": 2127, "end_char_idx": 3431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "080e86c9-6259-4139-982a-b2b0a765e1a2": {"__data__": {"id_": "080e86c9-6259-4139-982a-b2b0a765e1a2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d38ab7c-e7be-4ba9-afd3-f74f1a3fd72e", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "60f15906f2e55ea4a0db62c455d7220b0aaf47232db1b294a1c15aff38704617", "class_name": "RelatedNodeInfo"}}, "text": "Posted on 07 February,2017\n  ALSO ON JASDEEP06\n    Further-into                Getting started with        Lets-Practice-               Understanding LSTM in       Variable-sharing |\n    Further-into-\n    Further-into-               Getting started with\n                                Getting started with        Lets-Practice-\n                                                            Lets-Practice-               Understanding LSTM in\n                                                                                         Understanding LSTM in       Variable-sharing-\n    backpropagation                                         Backpropagation              Tensorflow                  Variable-sharing-\n    backpropagation             Tensorflow                                                                           Tensorflow\n    backpropagation             Tensor\u25afow\n                                Tensor\u25afow                   Backpropagation\n                                                            Backpropagation              Tensor\u25afow\n                                                                                         Tensor\u25afow                   Tensor\u25afow\n    7 years ago2 comments                                                                                            Tensor\u25afow\n             \u2022                  7 years ago3 comments\n                                         \u2022                  7 years ago4 comments\n                                                                      \u2022                  7 years ago32 comments\n                                                                                                  \u2022                  7 years ago14 comm\n                                                                                                                              \u2022\n    Backpropagation : Further   Tensorflow : Getting StartedLets-practice-               CNNs in Tensorflow(cifar-   Tensorflow: Variable\n    into Backpropagation        with Tensorflow             backpropagation              10)                         in Tensorflow\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                                   13/144/5/24, 8:30 PM                Intuitions in CNNs\n 0 Comments                                   \ue603Jasdeep Singh Chhabra\n (QiNG\n      Start the discussion\u2026\n  \uf1091  Share                                      BestNewestOldest\n                         Be thefirst to comment.", "start_char_idx": 3452, "end_char_idx": 5947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-3": {"__data__": {"id_": "node-3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a8a2edd-962c-4b7d-8b73-4124f977c048", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "23b398f45bbc207b8ed8902a254925923e0496b13660344ec8b7dede767ad4b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c510c8d6-d457-45f1-8201-a30f1a121df9", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0bb1b5c8a8003abab24a4f654463a479212f52ce87cf8c4a31dfa16184502bbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4053bcd4-8000-45db-badf-475930e481d9", "node_type": "1", "metadata": {}, "hash": "41df44ac5f12e96bf9f920a0b1185419895426449297ddecbd6e92b131a9d581", "class_name": "RelatedNodeInfo"}}, "text": "As the name suggests,padding is nothing but padding of images.Our basic problem was that the pixels that were on\n  the edge of our image were le\u25afout by the filter.What if they are no more at the edge?This can be made possible by\n  padding our image with zero pixels.This would allow our filter to traverse along the image beyond the edges and thus\n  solve the problem.If we apply zero pad border to our 7X7 example above:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                                      10/144/5/24, 8:30 PM                                                         Intuitions in CNNs\n For stride=1,the general formula for number of padding borders required to preserve spatial size of our activation is:\n  borders=(F-1)/2\n where F is size of our filter.In this case F=3,so borders=(3-1)/2=1.\n Pooling\n Now that we have all the components of our network,we can suggest some modifications to make our network\n better.To further decrease the number of parameters and to prevent overfitting,we introduce the concept of\n pooling.Pooling involves downsampling of our activation map.The most common form of pooling used is called max-\n pooling. Remember that our activation map consisted of individual activations from di\u25aferent filters.Pooling operation\n is done individually on these activations i.e. pooling operation only modifies the spatial dimension of our activations\n and leaves the number of activations una\u25afected.Let us see max-pooling with help of an example:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                         11/144/5/24, 8:30 PM                                                   Intuitions in CNNs\n  Max pooling is pretty straightforward as shown in above figure.Our 4X4 activation would be traversed by a 2X2 max-\n  pool filter with stride 2.This traversal can be visualized by dividing our 4X4 activation in 4 quadrants.During filter\n  traversal,maximum of each quadrant is preserved and rest of the pixel values are neglected.Thus we get a\n  downsampled max pooled activation. This pooling operation is done a\u25afer a few convolution layers.In essence the\n  netwok can be viewed as:\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                         12/144/5/24, 8:30 PM                                                        Intuitions in CNNs\n                              CONV RELUCONV RELU POOLCONV RELU CONV RELUPOOLCONV  RELUCONV RELU POOL     FC\n                                                                                                       car\n                                                                                                      ltruck\n                                                                                                      [airplane\n                                                                                                      [ship\n                                                                                                       norse\n Note that the pooling layer is a\u25afer every second convolution layer.It id just matter of whatever works for you!!!Try\n di\u25aferent combinations and pick the best for your network.\n So this post was all about getting a intuitive feel of CNNs.In the next post we will apply CNNs for image classification\n task in Tensorflow.Stay tuned!!                     Posted on 07 February,2017\n  ALSO ON JASDEEP06\n    Further-into                Getting started with        Lets-Practice-               Understanding LSTM in       Variable-sharing |\n    Further-into-\n    Further-into-               Getting started with\n                                Getting started with        Lets-Practice-\n                                                            Lets-Practice-               Understanding LSTM in\n                                                                                         Understanding LSTM in       Variable-sharing-\n    backpropagation                                         Backpropagation              Tensorflow                  Variable-sharing-\n    backpropagation             Tensorflow                                                                           Tensorflow\n    backpropagation             Tensor\u25afow\n                                Tensor\u25afow                   Backpropagation\n                                                            Backpropagation              Tensor\u25afow\n                                                                                         Tensor\u25afow                   Tensor\u25afow\n    7 years ago2 comments                                                                                            Tensor\u25afow\n             \u2022                  7 years ago3 comments\n                                         \u2022                  7 years ago4 comments\n                                                                      \u2022                  7 years ago32 comments\n                                                                                                  \u2022                  7 years ago14 comm\n                                                                                                                              \u2022\n    Backpropagation : Further   Tensorflow : Getting StartedLets-practice-               CNNs in Tensorflow(cifar-   Tensorflow: Variable\n    into Backpropagation        with Tensorflow             backpropagation              10)                         in Tensorflow\nhttps://jasdeep06.github.io/posts/basics-of-cnns/                                                                                   13/144/5/24, 8:30 PM                Intuitions in CNNs\n 0 Comments                                   \ue603Jasdeep Singh Chhabra\n (QiNG\n      Start the discussion\u2026\n  \uf1091  Share                                      BestNewestOldest\n                         Be thefirst to comment.", "start_char_idx": 13431, "end_char_idx": 19378, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0055471e-1034-48ff-93d8-0dac473ade3a": {"__data__": {"id_": "0055471e-1034-48ff-93d8-0dac473ade3a", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0552b07a31629dcd40e6d3a2763d7c3c119535e1f862a3340be35756f69abb19", "class_name": "RelatedNodeInfo"}}, "text": "SubscribePrivacyDo Not Sell My Data\n Tensorflow maintained byjasdeep06\nhttps://jasdeep06.github.io/posts/basics-of-cnns/        14/14", "start_char_idx": 0, "end_char_idx": 133, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-4", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "559d7be6-4816-4688-b8b7-4447362cf45b": {"__data__": {"id_": "559d7be6-4816-4688-b8b7-4447362cf45b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "0552b07a31629dcd40e6d3a2763d7c3c119535e1f862a3340be35756f69abb19", "class_name": "RelatedNodeInfo"}}, "text": "SubscribePrivacyDo Not Sell My Data\n Tensorflow maintained byjasdeep06\nhttps://jasdeep06.github.io/posts/basics-of-cnns/        14/14", "start_char_idx": 0, "end_char_idx": 133, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-4", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-4": {"__data__": {"id_": "node-4", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a8a2edd-962c-4b7d-8b73-4124f977c048", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "23b398f45bbc207b8ed8902a254925923e0496b13660344ec8b7dede767ad4b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca6777ae-dbaf-4f21-b581-59de92e14bd3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}, "hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "class_name": "RelatedNodeInfo"}}, "text": "SubscribePrivacyDo Not Sell My Data\n Tensorflow maintained byjasdeep06\nhttps://jasdeep06.github.io/posts/basics-of-cnns/        14/14", "start_char_idx": 19381, "end_char_idx": 19514, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-4", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "24efd9de-3836-4f35-b929-edcf1e80d238": {"__data__": {"id_": "24efd9de-3836-4f35-b929-edcf1e80d238", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Intuitions in CNNs", "path": "cache\\blogposts\\Intuitions-in-CNNs\\parsed\\images\\f24c1d96-55c8-4899-b16f-2bc4292e3567-img_p0_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being discussed here revolves around the digital representation of images, specifically through the use of pixels. Pixels are essentially the smallest units of a digital image, often likened to tiny blocks or dots that come together to form the complete picture. The resolution of an image is determined by the number of pixels it contains, with higher resolutions indicating a greater number of pixels and, consequently, more detail.\n\nTo illustrate this concept, consider an example where an image's resolution is described as 2048 by 1536. This means the image is composed of 2048 pixels in width and 1536 pixels in height, totaling approximately 3.1 million pixels, or 3.1 megapixels. A much simpler image, say one that is 2 by 2, would consist of just 4 pixels.\n\nUnderstanding how pixels contribute to the formation of an image is crucial when applying convolutional neural networks (CNNs) to image processing tasks. CNNs utilize filters to scan through an image's pixels, extracting features and patterns that are essential for tasks such as image recognition. This process is fundamental to the effectiveness of CNNs in various applications, including computer vision and machine learning.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76d3a260-510e-4788-93dd-4cd302244a2a": {"__data__": {"id_": "76d3a260-510e-4788-93dd-4cd302244a2a", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Intuitions in CNNs", "path": "cache\\blogposts\\Intuitions-in-CNNs\\parsed\\images\\f24c1d96-55c8-4899-b16f-2bc4292e3567-img_p1_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being discussed pertains to the fundamental structure of digital color representation, particularly in the context of RGB images. RGB stands for Red, Green, and Blue, which are the primary colors used in digital displays and image sensors to create a wide spectrum of colors. Each pixel in an RGB image is composed of three separate channels corresponding to these colors. By varying the intensity of each channel, a vast array of colors can be produced.\n\nThe intensity of each color channel is determined by its bit depth, which is commonly set at 8 bits. This means that each channel can have 256 different intensity levels, ranging from 0 to 255. The value 0 represents no intensity, while 255 represents full intensity. When these values are combined across the three channels, they define the color of a single pixel. For instance, a pixel with the channel values of (64, 224, 208) would appear as the color turquoise.\n\nThe arrangement of these pixels in a grid forms the complete image, with each pixel contributing to the overall picture by its specific color. The way these pixels are organized and how their colors are combined result in the visual representation of an image as perceived by the human eye. The manipulation and understanding of these pixel values are crucial in the field of computer vision and image processing, particularly when using Convolutional Neural Networks (CNNs) for tasks such as image recognition and classification.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82b90091-f51b-4a80-aab5-ad778cd98cd4": {"__data__": {"id_": "82b90091-f51b-4a80-aab5-ad778cd98cd4", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Intuitions in CNNs", "path": "cache\\blogposts\\Intuitions-in-CNNs\\parsed\\images\\f24c1d96-55c8-4899-b16f-2bc4292e3567-img_p2_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being discussed here is related to the structure of an RGB image in the context of Convolutional Neural Networks (CNNs). An RGB image with a resolution of 200x200 is essentially a grid that is 200 pixels wide and 200 pixels tall. Each pixel in this grid has three color channels: Red, Green, and Blue. These channels are combined to represent the full spectrum of colors in each pixel.\n\nIn a visual representation of this concept, you might see a single pixel enlarged to show its individual color components. For instance, a pixel could be depicted as a small square, and next to it, there could be an arrow pointing to three circles or squares, each one representing one of the RGB channels. These three elements might be colored accordingly: red, green, and blue. If the pixel in question is turquoise, the corresponding circles or squares for the RGB channels would be colored to reflect the mixture of blue and green with a lesser contribution from the red channel that combines to form the turquoise color.\n\nThis kind of visual aid helps to illustrate how an image is composed of numerous pixels, each with its own set of RGB values that determine its final color when viewed on a screen. The representation of the image in three dimensions as AXBX3 emphasizes the layering of the color channels across the entire image, which is a fundamental concept in image processing and CNNs.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6203a449-4022-4088-a5c9-733417324aca": {"__data__": {"id_": "6203a449-4022-4088-a5c9-733417324aca", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Intuitions in CNNs", "path": "cache\\blogposts\\Intuitions-in-CNNs\\parsed\\images\\f24c1d96-55c8-4899-b16f-2bc4292e3567-img_p3_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being discussed here is the motivation behind using Convolutional Neural Networks (CNNs) for image classification tasks. The text explains the limitations of using a vanilla feedforward network for image classification due to the high number of input nodes that would be required for high-resolution images. This would result in a massive number of trainable parameters, making the process computationally expensive and inefficient.\n\nTo address this issue, the idea is to reduce the number of input pixels by focusing on the essential features necessary for image recognition, rather than manually selecting features, which is not practical for complex image data. This is where CNNs come into play, as they are designed to automatically learn and identify important features from the images, significantly reducing the number of parameters and making the process more efficient.\n\nThe concept is illustrated by showing how an image can be transformed into different representations that highlight various features or aspects, which could be analogous to how a CNN might process and analyze different features within an image to aid in classification. This transformation emphasizes the importance of feature learning in CNNs, which allows the network to focus on the most informative parts of the image for the task at hand.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6fce714-84bd-4abf-a60d-70968423bcfc": {"__data__": {"id_": "a6fce714-84bd-4abf-a60d-70968423bcfc", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Intuitions in CNNs", "path": "cache\\blogposts\\Intuitions-in-CNNs\\parsed\\images\\f24c1d96-55c8-4899-b16f-2bc4292e3567-img_p4_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "In the context of Convolutional Neural Networks (CNNs), when discussing the working of a filter, it's important to understand how it interacts with an image. A filter, which can be thought of as a small matrix of weights, is applied to an image to modify its pixels and extract or emphasize certain features.\n\nConsider a scenario where you have a grid representing pixel values of an image. This grid is a 5x5 matrix with numbers such as 50, 46, 100, and so on, arranged in rows and columns. Each number represents the intensity of a pixel in the image.\n\nNow, imagine another smaller grid, a 3x3 matrix, which is the filter. This filter has its own set of numbers, and it's designed based on a specific pattern or distribution, such as a Gaussian distribution. The numbers in this filter might be arranged in a way that emphasizes the center value, with the surrounding values gradually decreasing.\n\nThe process of applying the filter to the image involves overlaying the filter on top of the image grid and aligning the central value of the filter with each pixel of the image grid in turn. When the filter is placed over the image, a calculation is performed where each overlapping pixel value is multiplied by the corresponding filter weight, and the results are summed up to produce a new value for the central pixel.\n\nFor example, if the central pixel of the image grid is 50 and the corresponding filter value is 4, and the surrounding", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51a0218f-7c11-4676-884e-b156bddde0af": {"__data__": {"id_": "51a0218f-7c11-4676-884e-b156bddde0af", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Intuitions in CNNs", "path": "cache\\blogposts\\Intuitions-in-CNNs\\parsed\\images\\f24c1d96-55c8-4899-b16f-2bc4292e3567-img_p5_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being discussed here relates to the way Convolutional Neural Networks (CNNs) process visual information. When a CNN is presented with visual data, it interprets this data in a structured manner. The data is organized into a three-dimensional array, with the two spatial dimensions representing the resolution of the visual content and the third dimension representing the color channels, typically three for a standard RGB (red, green, blue) color model.\n\nThe initial layer of the CNN, known as the input layer, receives this raw visual data. The network then applies various filters to this layer to extract features. These filters are designed to detect specific types of patterns or features within the visual data, such as edges, textures, or shapes. Each filter produces a different representation of the input data, highlighting certain aspects while de-emphasizing others.\n\nAs the filters are applied, they generate a set of activations, which are essentially the filtered versions of the input data. These activations reveal how strongly the features detected by each filter are present in the original visual data. The collection of these activations forms the next layer in the CNN, often referred to as the first convolutional layer.\n\nThe process of applying filters and generating activations is a fundamental operation in CNNs, allowing the network to build a complex hierarchy of features at various levels of abstraction. This hierarchical feature extraction is what enables CNNs to perform tasks such as image recognition with high accuracy, as they can learn to recognize patterns that are indicative of specific objects or categories", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45fa6121-03e6-409f-9ae3-7270519deb2c": {"__data__": {"id_": "45fa6121-03e6-409f-9ae3-7270519deb2c", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Intuitions in CNNs", "path": "cache\\blogposts\\Intuitions-in-CNNs\\parsed\\images\\f24c1d96-55c8-4899-b16f-2bc4292e3567-img_p6_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being discussed here is likely related to the workings of Convolutional Neural Networks (CNNs). The visual representation you're referring to seems to illustrate the process of applying a filter or kernel to an input to produce an output feature map. This is a fundamental operation in CNNs where the filter slides over the input data to extract features, which are then used for tasks such as image recognition. The process involves element-wise multiplication of the filter values with the original input values, followed by a summation to form a single value in the output feature map. This operation is repeated across the entire input to form a complete feature map that represents certain characteristics of the input, such as edges or textures, depending on the filter applied.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7cab28c9-6735-462b-aa6d-d223ecf16c4f": {"__data__": {"id_": "7cab28c9-6735-462b-aa6d-d223ecf16c4f", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Intuitions in CNNs", "path": "cache\\blogposts\\Intuitions-in-CNNs\\parsed\\images\\f24c1d96-55c8-4899-b16f-2bc4292e3567-img_p7_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "In the context of convolutional neural networks (CNNs), when a filter is applied to an image, it is used to detect specific features or patterns. The process involves sliding the filter across the image, which results in an activation map that highlights areas where features of interest are detected. The size of the activation map is typically smaller than the original image because the filter only moves within the bounds of the image, ensuring it doesn't extend beyond the image's edges.\n\nWhen multiple filters are used, each one can detect different features, and their outputs are stacked together to form a multi-dimensional activation map. This map then undergoes a non-linear transformation, such as the application of a ReLU function, which introduces non-linearity into the model and helps with learning complex patterns.\n\nThe transformed activation map is not the end of the process. It can be further processed by additional layers of filters, each building on the activations from the previous layer. This stacking of layers allows the network to learn hierarchies of features, with higher layers learning more complex and abstract representations of the data. The architecture of a CNN is designed to take advantage of this hierarchical feature learning to perform tasks such as image recognition and classification.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c4b0070-ef03-4f36-9cbc-dc59b60745cd": {"__data__": {"id_": "9c4b0070-ef03-4f36-9cbc-dc59b60745cd", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Intuitions in CNNs", "path": "cache\\blogposts\\Intuitions-in-CNNs\\parsed\\images\\f24c1d96-55c8-4899-b16f-2bc4292e3567-img_p8_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being discussed here relates to the structure and functioning of Convolutional Neural Networks (CNNs), which are a class of deep neural networks widely used in image recognition and processing tasks. The text describes a sequence of operations typically found in CNNs, including convolution layers followed by activation functions like ReLU (Rectified Linear Unit).\n\nIn a CNN, an image is passed through a series of convolutional layers with filters that are designed to detect specific features such as edges, textures, or patterns. Each convolutional layer applies a set of filters to the input, and the result of this convolution is then passed through an activation function to introduce non-linearity into the model.\n\nThe table in the text outlines a progression of convolutional layers with different numbers of filters and their respective sizes. For example, the first layer uses 32 filters of size 5x5x3, where 5x5 represents the spatial dimensions of the filter and 3 represents the depth, corresponding to the color channels of the input image. The subsequent layers use filters with different dimensions and quantities, such as 28 filters of size 5x5x6, indicating an increase in the depth dimension due to the output of the previous layer.\n\nThe text also mentions the concept of trainable parameters, which are the weights of the filters and biases that the network learns during the training process. These parameters are adjusted through a process of optimization to minimize the error in the network's predictions.\n\nPadding is another important concept in CNN", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9043b9a6-e2a8-4ffd-a2d7-1526f9a32bea": {"__data__": {"id_": "9043b9a6-e2a8-4ffd-a2d7-1526f9a32bea", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Intuitions in CNNs", "path": "cache\\blogposts\\Intuitions-in-CNNs\\parsed\\images\\f24c1d96-55c8-4899-b16f-2bc4292e3567-img_p9_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being discussed pertains to the process of applying a filter to an input during convolution in Convolutional Neural Networks (CNNs). The stride, which is the step size with which the filter moves across the input, is set to 1, meaning the filter moves one unit at a time to the right. When the filter, which has a certain dimension, is applied to the input image with its own dimension, the resulting output size can be calculated using the formula provided. In the specific example given, the input image has a dimension of 7 and the filter has a dimension of 3, resulting in an output size of 5.\n\nThe text also addresses the issue of spatial dimension reduction, which can lead to loss of information, especially as the network becomes deeper with more convolution layers. To mitigate this, padding is introduced, which involves adding extra pixels around the edge of the input image, typically with a value of zero. This allows the filter to cover the edge pixels of the original input image, ensuring that they are not excluded from the convolution process. By applying zero padding to the 7x7 input image, the filter can effectively process the entire image, including the edges, without reducing the spatial dimensions of the output.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "090e58ac-3301-470e-bb3c-b81fa5c0a729": {"__data__": {"id_": "090e58ac-3301-470e-bb3c-b81fa5c0a729", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Intuitions in CNNs", "path": "cache\\blogposts\\Intuitions-in-CNNs\\parsed\\images\\f24c1d96-55c8-4899-b16f-2bc4292e3567-img_p10_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being discussed here involves a grid-like structure that represents an activation map from a convolutional neural network (CNN). This grid is composed of cells, each potentially representing an activation value resulting from the application of a filter to an input image. The grid shows a highlighted area, which could be illustrating the application of a filter or a pooling operation, such as max-pooling.\n\nIn the context of max-pooling, the highlighted area might be showing how a specific region of the grid is being considered for downsampling, where the maximum value within this region would be selected to represent the entire area in a reduced form. This process effectively reduces the spatial dimensions of the activation map while retaining the most significant features, which helps in reducing the number of parameters and preventing overfitting in the neural network.\n\nThe grid is typically bordered by additional cells that might represent padding, which is added to preserve the spatial dimensions of the activation map after the convolution operation. Padding allows the filter to be applied to the bordering elements of the input image, ensuring that the output size can be maintained. The formula provided suggests that for a filter of size 3 (F=3), a single border of padding (borders = 1) is necessary to keep the size of the activation map unchanged when the stride is set to 1.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebc084d5-1e28-4ed3-a4ff-694427b848dd": {"__data__": {"id_": "ebc084d5-1e28-4ed3-a4ff-694427b848dd", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Intuitions in CNNs", "path": "cache\\blogposts\\Intuitions-in-CNNs\\parsed\\images\\f24c1d96-55c8-4899-b16f-2bc4292e3567-img_p11_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being explained here is a process in Convolutional Neural Networks (CNNs) known as max pooling. Imagine a grid, specifically a 4x4 grid, where each cell contains a numerical value. This grid represents an activation map that is a result of previous convolution operations in a CNN. To perform max pooling, a smaller grid, say a 2x2 grid, is used to scan over the larger grid. The smaller grid moves across the larger one in steps, referred to as strides, and at each step, it selects the maximum value from the cells it covers.\n\nIn this scenario, the stride is 2, which means the smaller grid moves two cells at a time. As it scans the larger grid, it effectively divides it into four equal parts, or quadrants. From each quadrant, the maximum value is taken and the other values are disregarded. This results in a new, smaller grid that is a downsampled version of the original, containing only the maximum values from each quadrant. This new grid has fewer cells, which in this case would be a 2x2 grid, reflecting the downsampling effect of the max pooling operation.\n\nThis max pooling step is typically performed after several convolutional layers within the CNN, and it serves to reduce the spatial dimensions of the activation map. This downsampling is beneficial because it reduces the computational load for the network, helps to make the network more robust to variations in the input, and can help to prevent over", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f43d5e59-d624-482b-8e98-59e842a4ab38": {"__data__": {"id_": "f43d5e59-d624-482b-8e98-59e842a4ab38", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Intuitions in CNNs", "path": "cache\\blogposts\\Intuitions-in-CNNs\\parsed\\images\\f24c1d96-55c8-4899-b16f-2bc4292e3567-img_p12_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you're referring to seems to be discussing the architecture of a Convolutional Neural Network (CNN) and its application in image classification tasks. CNNs are a class of deep neural networks commonly used in analyzing visual imagery.\n\nThe architecture mentioned includes several layers: convolutional layers (CONV), rectified linear units (RELU), pooling layers (POOL), and a fully connected layer (FC). The convolutional layers are responsible for extracting features from the input images through filters that detect edges, textures, and other visual elements. The RELU layers introduce non-linearity to the network, allowing it to learn complex patterns. Pooling layers reduce the spatial size of the representation, which decreases the number of parameters and computation in the network, and also helps to make the detection of features invariant to scale and orientation.\n\nThe sequence of layers typically alternates between convolutional layers and RELU layers, with pooling layers inserted after every second convolutional layer. This pattern is repeated multiple times, and the network ends with a fully connected layer that integrates the learned features to perform classification.\n\nThe text suggests experimenting with different combinations of these layers to determine the best architecture for a specific network. It also indicates that the next post will cover the practical application of CNNs for image classification using TensorFlow, which is a popular open-source software library for machine learning.\n\nThe content also lists various categories such as cars, trucks, airplanes, ships, and horses, which are likely the classes used in an image classification task.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"9c5fc5ac-bdc9-438b-abc3-6b88e77ef6d9": {"doc_hash": "58c7120e2448d3c9f9b4cb2eb3fd59c908aa5f06dd41663407e87af32a52f2d8", "ref_doc_id": "node-0"}, "03a86b98-d016-4644-ba9a-4638013c5652": {"doc_hash": "09c9ddcd22762559649cc22c9503bdc341370c963f227ca8db6960d0c1e03539", "ref_doc_id": "node-0"}, "f7a673c8-95b4-4f87-a6c1-09415685ca33": {"doc_hash": "757c738a1a1a935323da675eb499c3c660a7c08ae35523797ad7081a44553af3", "ref_doc_id": "node-0"}, "0f20ee78-5713-488c-a266-562a3aa8cd71": {"doc_hash": "a3ebb8dc489a7f7498ebd60594fb314bd14df562d9c2e561f364201885c12081", "ref_doc_id": "node-0"}, "b4cd6b04-1898-4615-86c6-93e94f6cfa75": {"doc_hash": "54a406eeea5e1d449bfd572bc965bcafe22e91072f5d35d60d5d6bd97318330a", "ref_doc_id": "node-0"}, "f11fa4a8-26ce-4bd5-95f1-ab050005ebae": {"doc_hash": "94e37474e121171d518c6b203e7a452a98f514c6c609e87fa2a00a298e910b54", "ref_doc_id": "node-0"}, "1e0a623c-91ea-4081-93c4-64408078ad28": {"doc_hash": "9d44d5439bc1ac9aaaf4909ab796abebdc7396e7d9e3051ede1fd27409e6f2e9", "ref_doc_id": "node-0"}, "ab746730-5830-43b7-a4ac-74e7b7c407a7": {"doc_hash": "650c0c4ed30a943fe388089e2db980d95189e3e9069794d8f3ca4abfd9432b5b", "ref_doc_id": "node-0"}, "node-0": {"doc_hash": "b9621e3975a7486da56c848f3400831dcffea2c83ff89c69c89b214a0c1a70c7", "ref_doc_id": "5a8a2edd-962c-4b7d-8b73-4124f977c048"}, "a966622b-0d36-436f-b849-f6bc2e884851": {"doc_hash": "0ac46702fc039507153fdaa79c15752c08d91a39fd306b53a1b2ca306881b6a1", "ref_doc_id": "node-1"}, "aeca82bf-6a35-48f0-a02b-258d6bd17270": {"doc_hash": "fc4af7e366ecae1d4df37b50c83fd7db7d74d5b5b3c090bd82441bb212e55956", "ref_doc_id": "node-1"}, "749d43b9-6f2c-40c4-b01b-7488c391c665": {"doc_hash": "bae01dcc6056371339070d439567e9f1518f07be875e39941c79d64029a3e83a", "ref_doc_id": "node-1"}, "03b8c283-b8fb-42cd-8552-25c3735b68c1": {"doc_hash": "c5c520e1394ad2950d415c833e3d84a9d94a434de6af34e0ccca1a1e8232343b", "ref_doc_id": "node-1"}, "eef4c5d2-65ad-468d-adf6-c7ebb1553ed0": {"doc_hash": "a1e8fa4788f502615d996907d7bdc0bd0b4f6df07890d83d538edf4581d58bf0", "ref_doc_id": "node-1"}, "793dedf2-504a-46b1-ab83-6f89f3a26eca": {"doc_hash": "0ac46702fc039507153fdaa79c15752c08d91a39fd306b53a1b2ca306881b6a1", "ref_doc_id": "node-1"}, "63b94462-c2c4-4a8e-b971-4567d3da6dfb": {"doc_hash": "e16c54213999edd113c3eb0c4323beb30bac510c654e82f595122943eee39ce7", "ref_doc_id": "node-1"}, "c79751eb-158f-468c-bbc3-a397833cebe7": {"doc_hash": "9646ba2bfd0dad32b7dd1cd106c3bde297a89d0253b16549c1f938d3d8ff5fb8", "ref_doc_id": "node-1"}, "node-1": {"doc_hash": "0668c141264ac707b2fa6fe58b2f4af4eb0e4e95499e27502b00ce7c9698a0db", "ref_doc_id": "5a8a2edd-962c-4b7d-8b73-4124f977c048"}, "c07a6867-57ea-421d-b6bd-569c214b2a54": {"doc_hash": "7fc5412af920893eabdf996c0e3a2e68013a5e4fdbdacaea6aa401a786b09b76", "ref_doc_id": "node-2"}, "e11b3a6d-e17c-49a1-b15a-9b017a80c498": {"doc_hash": "d3abf02d426eb518f48af3946054943228a1059697f84b3e558863296bbd17e2", "ref_doc_id": "node-2"}, "8117b684-370c-4cf5-98d0-218e3c91d4c3": {"doc_hash": "3668730b07579909489a9f63c75bd19197a7a0e324f14de8595b202f567f9f02", "ref_doc_id": "node-2"}, "dfbfa998-b3a4-43f9-a7be-90bb65e7c14c": {"doc_hash": "c4777d3bcdebc286a5f0b2adfbd74420c96c2fd34c251f3220466f4e666585f3", "ref_doc_id": "node-2"}, "7fe7c3f6-4908-49bc-8c08-68ab91d6ab20": {"doc_hash": "238fa268668ae873f5756d8d3bb90a322cf1085171176e95a6b3b26bf0ffda40", "ref_doc_id": "node-2"}, "5c80c275-b436-4b9a-b633-b514c3c73e42": {"doc_hash": "4de38daa2046a7eed3abf42813b20ab02ae9b57febdcf020cf6e08b1c7ecbfae", "ref_doc_id": "node-2"}, "node-2": {"doc_hash": "0bb1b5c8a8003abab24a4f654463a479212f52ce87cf8c4a31dfa16184502bbb", "ref_doc_id": "5a8a2edd-962c-4b7d-8b73-4124f977c048"}, "f36980b8-fa61-4c7f-8484-0901f58b2240": {"doc_hash": "86d4a8b31993b1042ddaac9047c14409603088714cb502a3f90405858c7a87a2", "ref_doc_id": "node-3"}, "f4bb8e1f-fdee-4e75-be66-b6d3b5dd4d74": {"doc_hash": "0e7a737835ffe4f98186b3a2a9365149ecaedfc03ed0601dc385f9c5be5fa015", "ref_doc_id": "node-3"}, "d13cc652-b494-4d83-8501-03e3bccda374": {"doc_hash": "1da7a662680e151253ceb0b3cf9959ec255abb323cc58d8794edf9bc93f916ca", "ref_doc_id": "node-3"}, "f1bcd914-4234-4cc4-9826-1df66c9ce382": {"doc_hash": "e163695ba1d1e80a1ad84efbada2bc08211c99a1df2fcb5f1789f3cde946fd11", "ref_doc_id": "node-3"}, "e588bce0-f222-44c2-bb30-e00e21b22dc3": {"doc_hash": "49f7a4ebc3bc8173695105d8b0d282610f90578fb488e61a226d4688c40f4645", "ref_doc_id": "node-3"}, "7071038b-dc6a-49d7-8ad7-66c43881a6da": {"doc_hash": "edc90be6fe572decf51350252ef62f3cb5a5d0970eddd07be4a0990d520df1d4", "ref_doc_id": "node-3"}, "1364f3c5-91e1-496d-9fb7-29bfd78f1788": {"doc_hash": "a05f2ad758cb43fe48ea0f6cfe79273e286c8903d113c167b4612bbe1f879ae4", "ref_doc_id": "node-3"}, "30129102-cdab-4014-aeb5-e0bd2b4b04ef": {"doc_hash": "75c667cb52a4548f78eafd107180a536be3cf3a9ca1d13ab8de4b06786ceaa09", "ref_doc_id": "node-3"}, "a074e433-a46a-491d-acad-ce6d09ebfb3f": {"doc_hash": "033994a79fd0fe3746468d06d5815a7f2c2ecbfd12deee77c1dd64ef7d9485f5", "ref_doc_id": "node-3"}, "139feaf9-2f7e-45cb-9337-3d870cb056e7": {"doc_hash": "40767b3883dd0ff14d86da97b94ab7053f64f8962685a2d5c97ee9f31fb4d1fc", "ref_doc_id": "node-3"}, "c4e2f371-9eea-4d23-aa2d-9b793569df21": {"doc_hash": "8d408748ddfc379570643c1c38b1d7d36ad5cf5bfa171fbb0e6968c7cb046ca7", "ref_doc_id": "node-3"}, "f74d5fbd-e1a9-4306-b605-3f7b0120c9e8": {"doc_hash": "6f809cbe75446dcfbc92cce9fa0dbe4872a6d271104a23798529dc75c0faf284", "ref_doc_id": "node-3"}, "6d38ab7c-e7be-4ba9-afd3-f74f1a3fd72e": {"doc_hash": "60f15906f2e55ea4a0db62c455d7220b0aaf47232db1b294a1c15aff38704617", "ref_doc_id": "node-3"}, "080e86c9-6259-4139-982a-b2b0a765e1a2": {"doc_hash": "797fc542946c6f94320b520d345c360aab7e384189bbae9516846278c223c4db", "ref_doc_id": "node-3"}, "node-3": {"doc_hash": "f2b2159ad86b8e124b5ac38813b1d476dc6d8545b68d1d30de4a170ea56d35e1", "ref_doc_id": "5a8a2edd-962c-4b7d-8b73-4124f977c048"}, "0055471e-1034-48ff-93d8-0dac473ade3a": {"doc_hash": "0552b07a31629dcd40e6d3a2763d7c3c119535e1f862a3340be35756f69abb19", "ref_doc_id": "node-4"}, "559d7be6-4816-4688-b8b7-4447362cf45b": {"doc_hash": "0552b07a31629dcd40e6d3a2763d7c3c119535e1f862a3340be35756f69abb19", "ref_doc_id": "node-4"}, "node-4": {"doc_hash": "0552b07a31629dcd40e6d3a2763d7c3c119535e1f862a3340be35756f69abb19", "ref_doc_id": "5a8a2edd-962c-4b7d-8b73-4124f977c048"}, "24efd9de-3836-4f35-b929-edcf1e80d238": {"doc_hash": "7f3c03faf1f554b50b347051d39ddac47856aecc988970e644525df77221e6cc"}, "76d3a260-510e-4788-93dd-4cd302244a2a": {"doc_hash": "66ca4e53df416395d0c5c0da7d1076d2a294583482bc3de4e4023d8580d62400"}, "82b90091-f51b-4a80-aab5-ad778cd98cd4": {"doc_hash": "cae72f155b18cf7a82720d37ba0a8c8909fb3361c618a98be8a88b5ed3a9ae4f"}, "6203a449-4022-4088-a5c9-733417324aca": {"doc_hash": "17ef329906f12f287cf99af3ea33431084badb0ebb6d8d3ddae3765cfaf50d55"}, "a6fce714-84bd-4abf-a60d-70968423bcfc": {"doc_hash": "65c405a2b66228c7a05a1f33edba53eb1f5392dcdf7975425317c513d7b0cd47"}, "51a0218f-7c11-4676-884e-b156bddde0af": {"doc_hash": "4c0d46beb5702326c7e8e98031b16fcf38619810549faaf2dd6150fdbad03498"}, "45fa6121-03e6-409f-9ae3-7270519deb2c": {"doc_hash": "611f2f78c9e780a2d2636e5d670d304ae393b32f34063ae5030d05a6760f8ebf"}, "7cab28c9-6735-462b-aa6d-d223ecf16c4f": {"doc_hash": "ebacb3510ef5c0c32523da59b6558bc8e6f3caac2dfbcfa687baaa57119e4b71"}, "9c4b0070-ef03-4f36-9cbc-dc59b60745cd": {"doc_hash": "6139b07baeda49695f9fe656f3ada3fa633b8514a8e955a3dc3bfc262e51826b"}, "9043b9a6-e2a8-4ffd-a2d7-1526f9a32bea": {"doc_hash": "5e06c1e31358c173fa09f6dbcaa4f8edacdc63ee00894f47b50eb1adc8e05e33"}, "090e58ac-3301-470e-bb3c-b81fa5c0a729": {"doc_hash": "ad8d9fe07d310c9727f44f56ddc707032ec452fbf4b9a7fa1877165ba5c89981"}, "ebc084d5-1e28-4ed3-a4ff-694427b848dd": {"doc_hash": "534ef90926fffc643da2affb0aeaa74905bb7eb7f78acf6bf63fd934065be3aa"}, "f43d5e59-d624-482b-8e98-59e842a4ab38": {"doc_hash": "523970ccc6b54f083733538aadd7afb8ac669b7058657010c29edfdca48a7aaf"}}, "docstore/ref_doc_info": {"node-0": {"node_ids": ["9c5fc5ac-bdc9-438b-abc3-6b88e77ef6d9", "03a86b98-d016-4644-ba9a-4638013c5652", "f7a673c8-95b4-4f87-a6c1-09415685ca33", "0f20ee78-5713-488c-a266-562a3aa8cd71", "b4cd6b04-1898-4615-86c6-93e94f6cfa75", "f11fa4a8-26ce-4bd5-95f1-ab050005ebae", "1e0a623c-91ea-4081-93c4-64408078ad28", "ab746730-5830-43b7-a4ac-74e7b7c407a7"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}}, "5a8a2edd-962c-4b7d-8b73-4124f977c048": {"node_ids": ["node-0", "node-1", "node-2", "node-3", "node-4"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}}, "node-1": {"node_ids": ["a966622b-0d36-436f-b849-f6bc2e884851", "aeca82bf-6a35-48f0-a02b-258d6bd17270", "749d43b9-6f2c-40c4-b01b-7488c391c665", "03b8c283-b8fb-42cd-8552-25c3735b68c1", "eef4c5d2-65ad-468d-adf6-c7ebb1553ed0", "793dedf2-504a-46b1-ab83-6f89f3a26eca", "63b94462-c2c4-4a8e-b971-4567d3da6dfb", "c79751eb-158f-468c-bbc3-a397833cebe7"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}}, "node-2": {"node_ids": ["c07a6867-57ea-421d-b6bd-569c214b2a54", "e11b3a6d-e17c-49a1-b15a-9b017a80c498", "8117b684-370c-4cf5-98d0-218e3c91d4c3", "dfbfa998-b3a4-43f9-a7be-90bb65e7c14c", "7fe7c3f6-4908-49bc-8c08-68ab91d6ab20", "5c80c275-b436-4b9a-b633-b514c3c73e42"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}}, "node-3": {"node_ids": ["f36980b8-fa61-4c7f-8484-0901f58b2240", "f4bb8e1f-fdee-4e75-be66-b6d3b5dd4d74", "d13cc652-b494-4d83-8501-03e3bccda374", "f1bcd914-4234-4cc4-9826-1df66c9ce382", "e588bce0-f222-44c2-bb30-e00e21b22dc3", "7071038b-dc6a-49d7-8ad7-66c43881a6da", "1364f3c5-91e1-496d-9fb7-29bfd78f1788", "30129102-cdab-4014-aeb5-e0bd2b4b04ef", "a074e433-a46a-491d-acad-ce6d09ebfb3f", "139feaf9-2f7e-45cb-9337-3d870cb056e7", "c4e2f371-9eea-4d23-aa2d-9b793569df21", "f74d5fbd-e1a9-4306-b605-3f7b0120c9e8", "6d38ab7c-e7be-4ba9-afd3-f74f1a3fd72e", "080e86c9-6259-4139-982a-b2b0a765e1a2"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}}, "node-4": {"node_ids": ["0055471e-1034-48ff-93d8-0dac473ade3a", "559d7be6-4816-4688-b8b7-4447362cf45b"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Intuitions in CNNs"}}}}