{"docstore/data": {"2296bbf5-f88f-41ef-bb25-74e4ac9c857b": {"__data__": {"id_": "2296bbf5-f88f-41ef-bb25-74e4ac9c857b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "86c921b374ef22d0e0c0824c5aa1a11872e09dba34cd35555562304b2c10bc2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a897be8-d3d3-4214-ac7a-3a17454687dc", "node_type": "1", "metadata": {}, "hash": "adf89932b89cb798924bf4d628ca6a9916d542b1985d2b8741d24907b0a8e312", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:27 PM                                                                   Neural Stacks-An Explaination\n  Neural Stacks                                                                                                                                HOME\n  Neural Stacks-An Explanation\n  Recently I stumbled upon         this paper by Google Deepmind titled \u201cLearning to transduce with unbounded memory\u201d.I think it will be\n  pretty fascinating to implement this paper.The implementation will be based on neural stacks accomplishing the sequence reversal\n  task.We will first implement scaled down version of neural stacks using python and numpy and then look to implement it in\n  tensorflow.As I would be implementing and posting subsequently as I get time,this section of blog may be updated less\n  frequently(still pretty frequently!).There may be some mistakes in implementations and I may have to disregard my previous\n  implementations to correct those mistakes. I suggest you to follow this portion of blog with open mind as I will post here open to\n  suggestions and criticism.\n  In this post I will first draw an outline of the task of sequence reversal using the neural stack.Then we will look at the design of neural\n  stack and would implement the forward propagation through the stack.\n  Lets get started!!!", "start_char_idx": 0, "end_char_idx": 1324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7a897be8-d3d3-4214-ac7a-3a17454687dc": {"__data__": {"id_": "7a897be8-d3d3-4214-ac7a-3a17454687dc", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "86c921b374ef22d0e0c0824c5aa1a11872e09dba34cd35555562304b2c10bc2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2296bbf5-f88f-41ef-bb25-74e4ac9c857b", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "825e96b6547b1c5a9e4f68308fd84b7b2eb6aee4d25e841a1d0168b3648b0d7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "978a7dbc-4966-4261-a6d6-a8ed7a8811f4", "node_type": "1", "metadata": {}, "hash": "76be1abc47d32234b6b080d2181f8518d605133d9d523628d0682b1efffbf5d6", "class_name": "RelatedNodeInfo"}}, "text": "Outline\n  We know that Recurrent neural networks(RNNs) can use hidden layers as memory to process arbitrary sequences of inputs.But as\n  the length of input sequence increase,the hidden layers have to increase in order to capture long term dependencies.This makes the\n  net deeper.A deeper net should work well(at least in theory) but that is not the case.Various problems such as exploding and dying\n  gradients,increase in number of parameters due to deeper nets etc. make the learning di\u25aficult and ine\u25aficient. To counter these\n  di\u25aficulties,LSTM(Long short term memory) cells were introduced.Although they work really well as compared to vanilla RNNs,still\n  they fail to generalize for longer strings in context of transduction tasks like sequence reversal etc. The main problem,according to\n  me,this paper wants to tackle is of generalization of transduction tasks for larger strings as compared to the training data(because\n  larger strings would require larger computational resources during training).The paper does this by introducing an extensible\n  memory which is logically unbounded(i.e.", "start_char_idx": 1327, "end_char_idx": 2428, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "978a7dbc-4966-4261-a6d6-a8ed7a8811f4": {"__data__": {"id_": "978a7dbc-4966-4261-a6d6-a8ed7a8811f4", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "86c921b374ef22d0e0c0824c5aa1a11872e09dba34cd35555562304b2c10bc2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a897be8-d3d3-4214-ac7a-3a17454687dc", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "30a09dcc76addc3de9c2e763b3ee86ecb9694e04b538e9a90988a9cfc3fe19c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef91eb73-c227-4352-9f42-1a51c6eceb3a", "node_type": "1", "metadata": {}, "hash": "be7391901b69c7d1233cc969c0dd0276dd755d656b08a91b769cb020a1d5cbb5", "class_name": "RelatedNodeInfo"}}, "text": "infinite capacity in theory but definitely would be limited due to machine\n  constraints).This memory structure(e.g. a neural stack) is controlled with the help of a Recurrent neural network.The benefit that we\n  get from such arrangement is we get logically unbounded memory for our network which is independent from the parameter and\n  nature of our RNN(unlike previously when we had to increase the depth of network to increase the memory capacity of our\n  network).\n  I hope this outline gave you an idea of what the author wants to achieve with this paper.According to me,during analysis of any\n  research paper,outline is the most important tool.It gives us an idea of what is coming and also aligns our direction of thinking to\n  that of the author\u02bcs.\n  What is a Stack?", "start_char_idx": 2429, "end_char_idx": 3206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ef91eb73-c227-4352-9f42-1a51c6eceb3a": {"__data__": {"id_": "ef91eb73-c227-4352-9f42-1a51c6eceb3a", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "86c921b374ef22d0e0c0824c5aa1a11872e09dba34cd35555562304b2c10bc2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "978a7dbc-4966-4261-a6d6-a8ed7a8811f4", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "2aa9ab6722b3eebe87fa2ba2636c7e774deb8832977977767182458ee7a07eac", "class_name": "RelatedNodeInfo"}}, "text": "Neural stack is inspired from one of the traditional data structure \u201cStack\u201d.A stack is a linear data structure that works on the\n  principle of LIFO(Last-in-first-out).Just imagine a stack of books.You would stack them up one over another and at the time of\n  retrival you will pick a book from top of the stack i.e. the one that was stacked last(in) would be retrived(out) first. The act of\n  stacking up objects(here books or else numbers!) is called \u201cpush\u201d operation while the act of retrieval is called \u201cpop\u201d\n  operation.Another operation is possible in which we can read the object on top of the stack.This is called \u201cpeek/read\u201d operation.For\n  this paper,we will use the \u201cread\u201d term. Thus it is pretty straightforward to reverse a sequence using a traditional stack.Just push the\n  string that we want to reverse into the stack(character-by-character/number-by-number you get that!)", "start_char_idx": 3209, "end_char_idx": 4097, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1220b428-98a1-4e23-b709-a49669611e83": {"__data__": {"id_": "1220b428-98a1-4e23-b709-a49669611e83", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "86c921b374ef22d0e0c0824c5aa1a11872e09dba34cd35555562304b2c10bc2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2814c4e-f579-422a-9516-62122f2721b4", "node_type": "1", "metadata": {}, "hash": "d2d5bfeac941c3a9a11c7d814654a537715cfb267f06c050606bbe619e9ccb01", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:27 PM                                                                   Neural Stacks-An Explaination\n  Neural Stacks                                                                                                                                HOME\n  Neural Stacks-An Explanation\n  Recently I stumbled upon         this paper by Google Deepmind titled \u201cLearning to transduce with unbounded memory\u201d.I think it will be\n  pretty fascinating to implement this paper.The implementation will be based on neural stacks accomplishing the sequence reversal\n  task.We will first implement scaled down version of neural stacks using python and numpy and then look to implement it in\n  tensorflow.As I would be implementing and posting subsequently as I get time,this section of blog may be updated less\n  frequently(still pretty frequently!).There may be some mistakes in implementations and I may have to disregard my previous\n  implementations to correct those mistakes. I suggest you to follow this portion of blog with open mind as I will post here open to\n  suggestions and criticism.\n  In this post I will first draw an outline of the task of sequence reversal using the neural stack.Then we will look at the design of neural\n  stack and would implement the forward propagation through the stack.\n  Lets get started!!!\n  Outline\n  We know that Recurrent neural networks(RNNs) can use hidden layers as memory to process arbitrary sequences of inputs.But as\n  the length of input sequence increase,the hidden layers have to increase in order to capture long term dependencies.This makes the\n  net deeper.A deeper net should work well(at least in theory) but that is not the case.Various problems such as exploding and dying\n  gradients,increase in number of parameters due to deeper nets etc. make the learning di\u25aficult and ine\u25aficient. To counter these\n  di\u25aficulties,LSTM(Long short term memory) cells were introduced.Although they work really well as compared to vanilla RNNs,still\n  they fail to generalize for longer strings in context of transduction tasks like sequence reversal etc. The main problem,according to\n  me,this paper wants to tackle is of generalization of transduction tasks for larger strings as compared to the training data(because\n  larger strings would require larger computational resources during training).The paper does this by introducing an extensible\n  memory which is logically unbounded(i.e. infinite capacity in theory but definitely would be limited due to machine\n  constraints).This memory structure(e.g.", "start_char_idx": 0, "end_char_idx": 2545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d2814c4e-f579-422a-9516-62122f2721b4": {"__data__": {"id_": "d2814c4e-f579-422a-9516-62122f2721b4", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "86c921b374ef22d0e0c0824c5aa1a11872e09dba34cd35555562304b2c10bc2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1220b428-98a1-4e23-b709-a49669611e83", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "8b6e32bbc1858b00a70aa9379f14990f34732546e853d14df0930f8333263c8e", "class_name": "RelatedNodeInfo"}}, "text": "a neural stack) is controlled with the help of a Recurrent neural network.The benefit that we\n  get from such arrangement is we get logically unbounded memory for our network which is independent from the parameter and\n  nature of our RNN(unlike previously when we had to increase the depth of network to increase the memory capacity of our\n  network).\n  I hope this outline gave you an idea of what the author wants to achieve with this paper.According to me,during analysis of any\n  research paper,outline is the most important tool.It gives us an idea of what is coming and also aligns our direction of thinking to\n  that of the author\u02bcs.\n  What is a Stack?\n  Neural stack is inspired from one of the traditional data structure \u201cStack\u201d.A stack is a linear data structure that works on the\n  principle of LIFO(Last-in-first-out).Just imagine a stack of books.You would stack them up one over another and at the time of\n  retrival you will pick a book from top of the stack i.e. the one that was stacked last(in) would be retrived(out) first. The act of\n  stacking up objects(here books or else numbers!) is called \u201cpush\u201d operation while the act of retrieval is called \u201cpop\u201d\n  operation.Another operation is possible in which we can read the object on top of the stack.This is called \u201cpeek/read\u201d operation.For\n  this paper,we will use the \u201cread\u201d term. Thus it is pretty straightforward to reverse a sequence using a traditional stack.Just push the\n  string that we want to reverse into the stack(character-by-character/number-by-number you get that!)", "start_char_idx": 2546, "end_char_idx": 4097, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-0": {"__data__": {"id_": "node-0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3426a652-a9c3-4d57-b4c5-0fdded7e2626", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "d34af1791075116b0eac2e790399faa9d6c28ad7e10c3f346cd10e1d0de7ab96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1876448-0e8a-4387-bce8-42ee30ccfaaa", "node_type": "1", "metadata": {}, "hash": "6de6a979b907d712ba9cafdcd7509874e186db1b6f350fcf7be2b6cb2a55ddfb", "class_name": "RelatedNodeInfo"}}, "text": "4/5/24, 8:27 PM                                                                   Neural Stacks-An Explaination\n  Neural Stacks                                                                                                                                HOME\n  Neural Stacks-An Explanation\n  Recently I stumbled upon         this paper by Google Deepmind titled \u201cLearning to transduce with unbounded memory\u201d.I think it will be\n  pretty fascinating to implement this paper.The implementation will be based on neural stacks accomplishing the sequence reversal\n  task.We will first implement scaled down version of neural stacks using python and numpy and then look to implement it in\n  tensorflow.As I would be implementing and posting subsequently as I get time,this section of blog may be updated less\n  frequently(still pretty frequently!).There may be some mistakes in implementations and I may have to disregard my previous\n  implementations to correct those mistakes. I suggest you to follow this portion of blog with open mind as I will post here open to\n  suggestions and criticism.\n  In this post I will first draw an outline of the task of sequence reversal using the neural stack.Then we will look at the design of neural\n  stack and would implement the forward propagation through the stack.\n  Lets get started!!!\n  Outline\n  We know that Recurrent neural networks(RNNs) can use hidden layers as memory to process arbitrary sequences of inputs.But as\n  the length of input sequence increase,the hidden layers have to increase in order to capture long term dependencies.This makes the\n  net deeper.A deeper net should work well(at least in theory) but that is not the case.Various problems such as exploding and dying\n  gradients,increase in number of parameters due to deeper nets etc. make the learning di\u25aficult and ine\u25aficient. To counter these\n  di\u25aficulties,LSTM(Long short term memory) cells were introduced.Although they work really well as compared to vanilla RNNs,still\n  they fail to generalize for longer strings in context of transduction tasks like sequence reversal etc. The main problem,according to\n  me,this paper wants to tackle is of generalization of transduction tasks for larger strings as compared to the training data(because\n  larger strings would require larger computational resources during training).The paper does this by introducing an extensible\n  memory which is logically unbounded(i.e. infinite capacity in theory but definitely would be limited due to machine\n  constraints).This memory structure(e.g. a neural stack) is controlled with the help of a Recurrent neural network.The benefit that we\n  get from such arrangement is we get logically unbounded memory for our network which is independent from the parameter and\n  nature of our RNN(unlike previously when we had to increase the depth of network to increase the memory capacity of our\n  network).\n  I hope this outline gave you an idea of what the author wants to achieve with this paper.According to me,during analysis of any\n  research paper,outline is the most important tool.It gives us an idea of what is coming and also aligns our direction of thinking to\n  that of the author\u02bcs.\n  What is a Stack?\n  Neural stack is inspired from one of the traditional data structure \u201cStack\u201d.A stack is a linear data structure that works on the\n  principle of LIFO(Last-in-first-out).Just imagine a stack of books.You would stack them up one over another and at the time of\n  retrival you will pick a book from top of the stack i.e. the one that was stacked last(in) would be retrived(out) first. The act of\n  stacking up objects(here books or else numbers!) is called \u201cpush\u201d operation while the act of retrieval is called \u201cpop\u201d\n  operation.Another operation is possible in which we can read the object on top of the stack.This is called \u201cpeek/read\u201d operation.For\n  this paper,we will use the \u201cread\u201d term. Thus it is pretty straightforward to reverse a sequence using a traditional stack.Just push the\n  string that we want to reverse into the stack(character-by-character/number-by-number you get that!)", "start_char_idx": 0, "end_char_idx": 4097, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-0", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0901ab43-9e94-408e-9684-754121d833d3": {"__data__": {"id_": "0901ab43-9e94-408e-9684-754121d833d3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "12cc2f5fecb0d01a6e26ebaa8943bd93983c18e6f20a61c946cf5939a0305bcf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ec0f783-b0b9-480d-a68f-514a9df1cfdf", "node_type": "1", "metadata": {}, "hash": "20d09e13b139b96de0f3cc782c8cc27f87fab8ee47b0d21883e2d2c0089a8121", "class_name": "RelatedNodeInfo"}}, "text": "and just pop those entities\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                                                     1/74/5/24, 8:27 PM                                                           Neural Stacks-An Explaination\n  out.Refer the figure:\n                                                                 E\n                                                                 H\n  Neural Stack\n  Traditional stacks are fine but if we want to connect a stack with a recurrent network or in fact any network then it should be\n  di\u25aferentiable.Networks learn using backpropagation and for backpropagation of error every part of our network should be\n  di\u25aferentiable.Thus the mantra is\n       \u201cIf it is di\u25aferentiable,it is trainable\u201d\n  As our stack will be connected to a RNN,it should be di\u25aferentiable as well.In order to render these stacks di\u25aferentiable,the paper\n  comes up with rendering the discrete operations push and pop continuous by representing them with a value in interval (0,", "start_char_idx": 0, "end_char_idx": 1048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1ec0f783-b0b9-480d-a68f-514a9df1cfdf": {"__data__": {"id_": "1ec0f783-b0b9-480d-a68f-514a9df1cfdf", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "12cc2f5fecb0d01a6e26ebaa8943bd93983c18e6f20a61c946cf5939a0305bcf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0901ab43-9e94-408e-9684-754121d833d3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "e7eeccd9e200df1f56b8352a604f1328eb2dd097e7442ba3e97c1f00d3e81406", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3292ec7-b59c-4e85-8039-ff925739ec6f", "node_type": "1", "metadata": {}, "hash": "9a0b1769bd0023ce64df684dd061655215c9038edff2b19c16bece01e290f659", "class_name": "RelatedNodeInfo"}}, "text": "1) which\n  represent the degree of certainity with which the controller(RNN) wishes to push a vector            v  onto the stack or pop the top of the\n  stack.\n  Implementation of Neural Stack-\n  The author implements the neural stacks by using a value matrix           V  which acts as a expandable stack for storing the vectors as they\n  are pushed.Each vector in the value matrix has a strength value which is stored in another vector called strength vector               s .The\n  strength values of vectors in value matrix can be seen as certainty by which the vector is in the value matrix.A zero strength value\n  would signify the absence of corresponding vector from the value matrix. Both strength vector and value matrix expand with time as\n  new values are pushed into the value matrix.\n  The implementation of neural stack is based on three key formulas.I will explain the significance of these formulas and their\n  respective python implementation.", "start_char_idx": 1048, "end_char_idx": 2009, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c3292ec7-b59c-4e85-8039-ff925739ec6f": {"__data__": {"id_": "c3292ec7-b59c-4e85-8039-ff925739ec6f", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "12cc2f5fecb0d01a6e26ebaa8943bd93983c18e6f20a61c946cf5939a0305bcf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ec0f783-b0b9-480d-a68f-514a9df1cfdf", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "9bbcd7b1211250c34a9003ddae0bdb92552c0ce7f8c991eb9ae0a3743c9ce346", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2169e9ec-3406-4a7a-85f9-ff20a71eeb5a", "node_type": "1", "metadata": {}, "hash": "0714fdf0818565416f0c31acce46dfd726214cbbc76c97270749173dc68c2bbd", "class_name": "RelatedNodeInfo"}}, "text": "The first formula gives description of our value matrix:\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                                                 2/74/5/24, 8:27 PM                                        Neural Stacks-An Explaination\n  Vtli]               Vt-1[i]          if 1 < i < t                      (Note that Vt[i]                 Vi   for all i < t)\n                      Vt               if i =t\n The above formula represents the e\u25afect of push operation on our value matrix,.Assuming on every time step we push into our\n                                    th                              Vt\n value matrix,the indexrepresents the  entry in our value matrix. Thus at any time instant,our value matrix would be\n                    i              i                                              t\n comprised of all the vectors pushed until timeand the vector pushed at the timei.e.. One of the important things to take\n                                         t                              t   vt\n note here is that,once a vector is added to our value matrix,it is never modified.For modifications during pop operations we modify\n strength vector rather than vectors in our value matrix.", "start_char_idx": 2012, "end_char_idx": 3262, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2169e9ec-3406-4a7a-85f9-ff20a71eeb5a": {"__data__": {"id_": "2169e9ec-3406-4a7a-85f9-ff20a71eeb5a", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "12cc2f5fecb0d01a6e26ebaa8943bd93983c18e6f20a61c946cf5939a0305bcf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3292ec7-b59c-4e85-8039-ff925739ec6f", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "25a6185df88203b1943eb2a579b8d8edcfdf0f7ca5df4c15872868ff4756bb7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48bdcafc-3b54-48e8-8ef7-63e2330de903", "node_type": "1", "metadata": {}, "hash": "6a55d554d7d6f36af8267a77021ce25ea8c4d4e1000cd830c553e554adca90d5", "class_name": "RelatedNodeInfo"}}, "text": "The second formula describes our strength vector:\n             max  (0,st-1 [i]~mac   (0, &t5  t-12 st-1[j]))  ifl<i <t\n  St[i] =    dt                             j-itl             if i =t\n The above formula represents the e\u25afect of push and pop operations on our strength vector.is the pop signal andis the push\n                                                                                ut                   d t\n signal.Both the value lie in the range (0,1).denotes the strength vector at timewhileis the strength vector at time\n                                     st                               t      s t\u22121\n        . When we receive a pop signal(), we traverse down the strength vector from highest index to lowest index repeatedly\n t \u2212 1                            ut\n subtracting the scalars of  from   .If   is greater than the next scalar then that scalar is set to zero(of course a\u25afer\n                       st\u22121      u t  u t\n subtraction!) and the traversal continues.Ifis less than the next scalar thenis subtracted from that scalar and traversal stops.", "start_char_idx": 3264, "end_char_idx": 4336, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "48bdcafc-3b54-48e8-8ef7-63e2330de903": {"__data__": {"id_": "48bdcafc-3b54-48e8-8ef7-63e2330de903", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "12cc2f5fecb0d01a6e26ebaa8943bd93983c18e6f20a61c946cf5939a0305bcf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2169e9ec-3406-4a7a-85f9-ff20a71eeb5a", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "112af6c8ca98fd55d816b43f917dcac7f4b8edcd69f5bf96fc710576d65695d3", "class_name": "RelatedNodeInfo"}}, "text": "u t                           u t\n When we receive a push signal(),theth entry of the strength vector is modified as. When push and pop signals are received\n                            dt     t                                       dt\n first pop takes place which is followed by push.", "start_char_idx": 4375, "end_char_idx": 4659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0178f562-2ba6-4989-8399-161766d95c84": {"__data__": {"id_": "0178f562-2ba6-4989-8399-161766d95c84", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "12cc2f5fecb0d01a6e26ebaa8943bd93983c18e6f20a61c946cf5939a0305bcf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d961e957-a136-486d-9e3c-c9a5accc3660", "node_type": "1", "metadata": {}, "hash": "05b611919b257d5154f1b24f55842b0a20fae1fe4d88ed2befa608f37c8494aa", "class_name": "RelatedNodeInfo"}}, "text": "and just pop those entities\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                                                     1/74/5/24, 8:27 PM                                                           Neural Stacks-An Explaination\n  out.Refer the figure:\n                                                                 E\n                                                                 H\n  Neural Stack\n  Traditional stacks are fine but if we want to connect a stack with a recurrent network or in fact any network then it should be\n  di\u25aferentiable.Networks learn using backpropagation and for backpropagation of error every part of our network should be\n  di\u25aferentiable.Thus the mantra is\n       \u201cIf it is di\u25aferentiable,it is trainable\u201d\n  As our stack will be connected to a RNN,it should be di\u25aferentiable as well.In order to render these stacks di\u25aferentiable,the paper\n  comes up with rendering the discrete operations push and pop continuous by representing them with a value in interval (0,1) which\n  represent the degree of certainity with which the controller(RNN) wishes to push a vector            v  onto the stack or pop the top of the\n  stack.\n  Implementation of Neural Stack-\n  The author implements the neural stacks by using a value matrix           V  which acts as a expandable stack for storing the vectors as they\n  are pushed.Each vector in the value matrix has a strength value which is stored in another vector called strength vector               s .The\n  strength values of vectors in value matrix can be seen as certainty by which the vector is in the value matrix.A zero strength value\n  would signify the absence of corresponding vector from the value matrix. Both strength vector and value matrix expand with time as\n  new values are pushed into the value matrix.\n  The implementation of neural stack is based on three key formulas.I will explain the significance of these formulas and their\n  respective python implementation.", "start_char_idx": 0, "end_char_idx": 2009, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d961e957-a136-486d-9e3c-c9a5accc3660": {"__data__": {"id_": "d961e957-a136-486d-9e3c-c9a5accc3660", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "12cc2f5fecb0d01a6e26ebaa8943bd93983c18e6f20a61c946cf5939a0305bcf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0178f562-2ba6-4989-8399-161766d95c84", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "8c6cd287a94a2a488d11822a56ab7ce38d8a051b635784073a1ca9bcca6cfc8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "229a0077-6117-455f-b064-061466e010dc", "node_type": "1", "metadata": {}, "hash": "608faaa5db715904b7ed8d60249f7e38c0214bb9a5ac8b3ab2c3063fe5d369e3", "class_name": "RelatedNodeInfo"}}, "text": "The first formula gives description of our value matrix:\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                                                 2/74/5/24, 8:27 PM                                        Neural Stacks-An Explaination\n  Vtli]               Vt-1[i]          if 1 < i < t                      (Note that Vt[i]                 Vi   for all i < t)\n                      Vt               if i =t\n The above formula represents the e\u25afect of push operation on our value matrix,.Assuming on every time step we push into our\n                                    th                              Vt\n value matrix,the indexrepresents the  entry in our value matrix. Thus at any time instant,our value matrix would be\n                    i              i                                              t\n comprised of all the vectors pushed until timeand the vector pushed at the timei.e.. One of the important things to take\n                                         t                              t   vt\n note here is that,once a vector is added to our value matrix,it is never modified.For modifications during pop operations we modify\n strength vector rather than vectors in our value matrix.\n The second formula describes our strength vector:\n             max  (0,st-1 [i]~mac   (0, &t5  t-12 st-1[j]))  ifl<i <t\n  St[i] =    dt                             j-itl             if i =t\n The above formula represents the e\u25afect of push and pop operations on our strength vector.is the pop signal andis the push\n                                                                                ut                   d t\n signal.Both the value lie in the range (0,1).denotes the strength vector at timewhileis the strength vector at time\n                                     st                               t      s t\u22121\n        . When we receive a pop signal(), we traverse down the strength vector from highest index to lowest index repeatedly\n t \u2212 1                            ut\n subtracting the scalars of  from   .If   is greater than the next scalar then that scalar is set to zero(of course a\u25afer\n                       st\u22121      u t  u t\n subtraction!) and the traversal continues.Ifis less than the next scalar thenis subtracted from that scalar and traversal stops.\n                                      u t                           u t\n When we receive a push signal(),theth entry of the strength vector is modified as.", "start_char_idx": 2012, "end_char_idx": 4492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "229a0077-6117-455f-b064-061466e010dc": {"__data__": {"id_": "229a0077-6117-455f-b064-061466e010dc", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-1", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "12cc2f5fecb0d01a6e26ebaa8943bd93983c18e6f20a61c946cf5939a0305bcf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d961e957-a136-486d-9e3c-c9a5accc3660", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "32e32b239d9436c5af2ea699fb2f52adcfe7d9f9696e7952ee47ec352ad6196a", "class_name": "RelatedNodeInfo"}}, "text": "When push and pop signals are received\n                            dt     t                                       dt\n first pop takes place which is followed by push.", "start_char_idx": 4493, "end_char_idx": 4659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-1": {"__data__": {"id_": "node-1", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3426a652-a9c3-4d57-b4c5-0fdded7e2626", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "d34af1791075116b0eac2e790399faa9d6c28ad7e10c3f346cd10e1d0de7ab96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d0bf8cc-41a1-43d9-8fcf-efe4a9f8fb2c", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "86c921b374ef22d0e0c0824c5aa1a11872e09dba34cd35555562304b2c10bc2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6126bf1-cb26-4557-b727-43ea77dd18ea", "node_type": "1", "metadata": {}, "hash": "a55a2277cc4fa91cb3e9387a548463fcafe4a22b8f59022387f6cc61060b6d63", "class_name": "RelatedNodeInfo"}}, "text": "and just pop those entities\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                                                     1/74/5/24, 8:27 PM                                                           Neural Stacks-An Explaination\n  out.Refer the figure:\n                                                                 E\n                                                                 H\n  Neural Stack\n  Traditional stacks are fine but if we want to connect a stack with a recurrent network or in fact any network then it should be\n  di\u25aferentiable.Networks learn using backpropagation and for backpropagation of error every part of our network should be\n  di\u25aferentiable.Thus the mantra is\n       \u201cIf it is di\u25aferentiable,it is trainable\u201d\n  As our stack will be connected to a RNN,it should be di\u25aferentiable as well.In order to render these stacks di\u25aferentiable,the paper\n  comes up with rendering the discrete operations push and pop continuous by representing them with a value in interval (0,1) which\n  represent the degree of certainity with which the controller(RNN) wishes to push a vector            v  onto the stack or pop the top of the\n  stack.\n  Implementation of Neural Stack-\n  The author implements the neural stacks by using a value matrix           V  which acts as a expandable stack for storing the vectors as they\n  are pushed.Each vector in the value matrix has a strength value which is stored in another vector called strength vector               s .The\n  strength values of vectors in value matrix can be seen as certainty by which the vector is in the value matrix.A zero strength value\n  would signify the absence of corresponding vector from the value matrix. Both strength vector and value matrix expand with time as\n  new values are pushed into the value matrix.\n  The implementation of neural stack is based on three key formulas.I will explain the significance of these formulas and their\n  respective python implementation.\n  The first formula gives description of our value matrix:\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                                                 2/74/5/24, 8:27 PM                                        Neural Stacks-An Explaination\n  Vtli]               Vt-1[i]          if 1 < i < t                      (Note that Vt[i]                 Vi   for all i < t)\n                      Vt               if i =t\n The above formula represents the e\u25afect of push operation on our value matrix,.Assuming on every time step we push into our\n                                    th                              Vt\n value matrix,the indexrepresents the  entry in our value matrix. Thus at any time instant,our value matrix would be\n                    i              i                                              t\n comprised of all the vectors pushed until timeand the vector pushed at the timei.e.. One of the important things to take\n                                         t                              t   vt\n note here is that,once a vector is added to our value matrix,it is never modified.For modifications during pop operations we modify\n strength vector rather than vectors in our value matrix.\n The second formula describes our strength vector:\n             max  (0,st-1 [i]~mac   (0, &t5  t-12 st-1[j]))  ifl<i <t\n  St[i] =    dt                             j-itl             if i =t\n The above formula represents the e\u25afect of push and pop operations on our strength vector.is the pop signal andis the push\n                                                                                ut                   d t\n signal.Both the value lie in the range (0,1).denotes the strength vector at timewhileis the strength vector at time\n                                     st                               t      s t\u22121\n        . When we receive a pop signal(), we traverse down the strength vector from highest index to lowest index repeatedly\n t \u2212 1                            ut\n subtracting the scalars of  from   .If   is greater than the next scalar then that scalar is set to zero(of course a\u25afer\n                       st\u22121      u t  u t\n subtraction!) and the traversal continues.Ifis less than the next scalar thenis subtracted from that scalar and traversal stops.\n                                      u t                           u t\n When we receive a push signal(),theth entry of the strength vector is modified as. When push and pop signals are received\n                            dt     t                                       dt\n first pop takes place which is followed by push.", "start_char_idx": 4098, "end_char_idx": 8757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "bde88eb3-8d76-4fc3-a5e5-dc8f4b0c94e8": {"__data__": {"id_": "bde88eb3-8d76-4fc3-a5e5-dc8f4b0c94e8", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "5fc907c102fb1ea0bfd8842bba82e95e99b672b20fe5747fb728cba5b215ac5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78da8037-38dc-4af5-870a-fb4a2bea4e02", "node_type": "1", "metadata": {}, "hash": "740e1875d37c1e6ff72b5f701ca84494bb953da6ade1feb8eb516e7029d15a95", "class_name": "RelatedNodeInfo"}}, "text": "Have a look at figures below to clarify notations and pop operation:The first figure\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                 3/74/5/24, 8:27 PM                                      Neural Stacks-An Explaination\n                                                     V4                           0.4\n                                                     V3                           0.3\n                                                     Vz                           0.6\n                                                                                  0.5\n  will be referred to as \u201creference-figure\u201d.\n                                       V4\n                                      V3\n                                      Vz                            0.5\n                                       V,                           0.5\n  This process of modification of strength vector can be represented in python as:\n  #initializing strength dictionary\n  strength={}\n  def strength_time(time,push_certainity,pop_certainity):\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                       4/74/5/24,", "start_char_idx": 0, "end_char_idx": 1188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "78da8037-38dc-4af5-870a-fb4a2bea4e02": {"__data__": {"id_": "78da8037-38dc-4af5-870a-fb4a2bea4e02", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "5fc907c102fb1ea0bfd8842bba82e95e99b672b20fe5747fb728cba5b215ac5b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bde88eb3-8d76-4fc3-a5e5-dc8f4b0c94e8", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "084874a5019e24ca7ac5cb22852053d5b54c51f3c77b9506297cf10a6912e2c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4858240c-d645-454d-829f-1b67f52d632a", "node_type": "1", "metadata": {}, "hash": "7d72313e988d01763091e7b6ba04aa8f0f960991a0cb4adf3a06d36951bc0feb", "class_name": "RelatedNodeInfo"}}, "text": "8:27 PM                                                       Neural Stacks-An Explaination\n        for  var   in  reversed(range(time)):\n             if   strength[var]<       pop_certainity:\n                   pop_certainity-=strength[var]\n             else: strength[var]       =  0\n                   strength[var]-=pop_certainity\n                   pop_certainity       =  0\n                   break\n        strength[time]       =  push_certainity       Tt = =1 (min(st[i], maz(0,1    j-i+1St[j])))Vi[i]\n  The third formula describes the read operation\n  The above formula represents the read vector.While reading from our stack,we set a fixed initial read quantity of 1.A temporary copy\n  of strength vector is made.Similar to pop operation,the copy of strength vector is traversed from highest index to lowest.If the next\n  scalar is less than the read value then its value is preserved and is subtracted from the read quantity.", "start_char_idx": 1189, "end_char_idx": 2124, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4858240c-d645-454d-829f-1b67f52d632a": {"__data__": {"id_": "4858240c-d645-454d-829f-1b67f52d632a", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "5fc907c102fb1ea0bfd8842bba82e95e99b672b20fe5747fb728cba5b215ac5b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78da8037-38dc-4af5-870a-fb4a2bea4e02", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "2db3f778c8a1888229b131737174c5f455b8a1757dd8aa8e89b01cb6adde330e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4de0f240-e9ff-4ec3-8db6-f9aae51e35ff", "node_type": "1", "metadata": {}, "hash": "aff2a4648c73e5d17d631c4468264dd2b4a5eca18c817e6ad63c1dfabf8fdbda", "class_name": "RelatedNodeInfo"}}, "text": "If the next scalar is more than\n  read value then its value is made equal to remaining read quantity and rest all scalars are set to zero.This resulting copy of strength\n  values is then multiplied with corresponding vectors in value matrix and by adding these product values read vector is generated.\n  Have a look at figure below to make things clearer:\n                                 V4                                     0.4                            0.4\n                                                                        0.3                            0.3\n                                Vz                                      0.6                            0.3\n                                 V                                      0.5\n  Implementing this in python we get:\n  def   read_time(time):\n        #returns read vector at time 'time'\n        #initial read value of 1\n        read=1\n        read_vector=np.zeros(input_size)\n        #duplicate of strenth vector to modify it at time of read operation\n        temp_strength=copy.deepcopy(strength)\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                                           5/74/5/24,", "start_char_idx": 2124, "end_char_idx": 3344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4de0f240-e9ff-4ec3-8db6-f9aae51e35ff": {"__data__": {"id_": "4de0f240-e9ff-4ec3-8db6-f9aae51e35ff", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "5fc907c102fb1ea0bfd8842bba82e95e99b672b20fe5747fb728cba5b215ac5b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4858240c-d645-454d-829f-1b67f52d632a", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "77c84cdbb6edcb5e3b03c09550a3feabf469aaa6d9556ff42ee0a87168baf15e", "class_name": "RelatedNodeInfo"}}, "text": "8:27 PM                                            Neural Stacks-An Explaination\n       #traversing through strength vector from top\n       for var  in  reversed(range(time+1)):\n           if  temp_strength[var]<     read:\n           else:read-=temp_strength[var]\n                temp_strength[var]=read\n                unwanted=set(temp_strength.keys())-set(range(var,time+1))\n                for  keys  in  unwanted:\n                breaktemp_strength[keys]=0\n       for var  in  Value.keys():\n           read_vector+=(temp_strength[var]*Value[var])\n       return  read_vector\n Checking our implementation: Below code is consistent with our read figure.Four vectors are pushed into our value matrix with the\n help of pushPop function.", "start_char_idx": 3345, "end_char_idx": 4081, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b107496d-b6df-4dde-91c1-112e830cdac0": {"__data__": {"id_": "b107496d-b6df-4dde-91c1-112e830cdac0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "5fc907c102fb1ea0bfd8842bba82e95e99b672b20fe5747fb728cba5b215ac5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc617e32-b748-4caa-9819-fe7a4222781f", "node_type": "1", "metadata": {}, "hash": "080bca28b1784a2d0b32f843ba3472894aa7328de49f26038eab4f19875a0e73", "class_name": "RelatedNodeInfo"}}, "text": "Have a look at figures below to clarify notations and pop operation:The first figure\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                 3/74/5/24, 8:27 PM                                      Neural Stacks-An Explaination\n                                                     V4                           0.4\n                                                     V3                           0.3\n                                                     Vz                           0.6\n                                                                                  0.5\n  will be referred to as \u201creference-figure\u201d.\n                                       V4\n                                      V3\n                                      Vz                            0.5\n                                       V,                           0.5\n  This process of modification of strength vector can be represented in python as:\n  #initializing strength dictionary\n  strength={}\n  def strength_time(time,push_certainity,pop_certainity):\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                       4/74/5/24, 8:27 PM                                                       Neural Stacks-An Explaination\n        for  var   in  reversed(range(time)):\n             if   strength[var]<       pop_certainity:\n                   pop_certainity-=strength[var]\n             else: strength[var]       =  0\n                   strength[var]-=pop_certainity\n                   pop_certainity       =  0\n                   break\n        strength[time]       =  push_certainity       Tt = =1 (min(st[i], maz(0,1    j-i+1St[j])))Vi[i]\n  The third formula describes the read operation\n  The above formula represents the read vector.While reading from our stack,we set a fixed initial read quantity of 1.A temporary copy\n  of strength vector is made.Similar to pop operation,the copy of strength vector is traversed from highest index to lowest.If the next\n  scalar is less than the read value then its value is preserved and is subtracted from the read quantity.If the next scalar is more than\n  read value then its value is made equal to remaining read quantity and rest all scalars are set to zero.This resulting copy of strength\n  values is then multiplied with corresponding vectors in value matrix and by adding these product values read vector is generated.", "start_char_idx": 0, "end_char_idx": 2425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "bc617e32-b748-4caa-9819-fe7a4222781f": {"__data__": {"id_": "bc617e32-b748-4caa-9819-fe7a4222781f", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-2", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "5fc907c102fb1ea0bfd8842bba82e95e99b672b20fe5747fb728cba5b215ac5b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b107496d-b6df-4dde-91c1-112e830cdac0", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "c09348123731c7ea82809999fffb40bcae8e1e39c4ea4ee35706760e27a199f2", "class_name": "RelatedNodeInfo"}}, "text": "Have a look at figure below to make things clearer:\n                                 V4                                     0.4                            0.4\n                                                                        0.3                            0.3\n                                Vz                                      0.6                            0.3\n                                 V                                      0.5\n  Implementing this in python we get:\n  def   read_time(time):\n        #returns read vector at time 'time'\n        #initial read value of 1\n        read=1\n        read_vector=np.zeros(input_size)\n        #duplicate of strenth vector to modify it at time of read operation\n        temp_strength=copy.deepcopy(strength)\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                                           5/74/5/24, 8:27 PM                                            Neural Stacks-An Explaination\n       #traversing through strength vector from top\n       for var  in  reversed(range(time+1)):\n           if  temp_strength[var]<     read:\n           else:read-=temp_strength[var]\n                temp_strength[var]=read\n                unwanted=set(temp_strength.keys())-set(range(var,time+1))\n                for  keys  in  unwanted:\n                breaktemp_strength[keys]=0\n       for var  in  Value.keys():\n           read_vector+=(temp_strength[var]*Value[var])\n       return  read_vector\n Checking our implementation: Below code is consistent with our read figure.Four vectors are pushed into our value matrix with the\n help of pushPop function.", "start_char_idx": 2428, "end_char_idx": 4081, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-2": {"__data__": {"id_": "node-2", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3426a652-a9c3-4d57-b4c5-0fdded7e2626", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "d34af1791075116b0eac2e790399faa9d6c28ad7e10c3f346cd10e1d0de7ab96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1876448-0e8a-4387-bce8-42ee30ccfaaa", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "12cc2f5fecb0d01a6e26ebaa8943bd93983c18e6f20a61c946cf5939a0305bcf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20496028-0820-43a0-8fcd-acd605f37ba8", "node_type": "1", "metadata": {}, "hash": "53bf7dcbccfcfb893b020edb445c1d7fd92548b13ccab09173cefd372445abc3", "class_name": "RelatedNodeInfo"}}, "text": "Have a look at figures below to clarify notations and pop operation:The first figure\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                 3/74/5/24, 8:27 PM                                      Neural Stacks-An Explaination\n                                                     V4                           0.4\n                                                     V3                           0.3\n                                                     Vz                           0.6\n                                                                                  0.5\n  will be referred to as \u201creference-figure\u201d.\n                                       V4\n                                      V3\n                                      Vz                            0.5\n                                       V,                           0.5\n  This process of modification of strength vector can be represented in python as:\n  #initializing strength dictionary\n  strength={}\n  def strength_time(time,push_certainity,pop_certainity):\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                       4/74/5/24, 8:27 PM                                                       Neural Stacks-An Explaination\n        for  var   in  reversed(range(time)):\n             if   strength[var]<       pop_certainity:\n                   pop_certainity-=strength[var]\n             else: strength[var]       =  0\n                   strength[var]-=pop_certainity\n                   pop_certainity       =  0\n                   break\n        strength[time]       =  push_certainity       Tt = =1 (min(st[i], maz(0,1    j-i+1St[j])))Vi[i]\n  The third formula describes the read operation\n  The above formula represents the read vector.While reading from our stack,we set a fixed initial read quantity of 1.A temporary copy\n  of strength vector is made.Similar to pop operation,the copy of strength vector is traversed from highest index to lowest.If the next\n  scalar is less than the read value then its value is preserved and is subtracted from the read quantity.If the next scalar is more than\n  read value then its value is made equal to remaining read quantity and rest all scalars are set to zero.This resulting copy of strength\n  values is then multiplied with corresponding vectors in value matrix and by adding these product values read vector is generated.\n  Have a look at figure below to make things clearer:\n                                 V4                                     0.4                            0.4\n                                                                        0.3                            0.3\n                                Vz                                      0.6                            0.3\n                                 V                                      0.5\n  Implementing this in python we get:\n  def   read_time(time):\n        #returns read vector at time 'time'\n        #initial read value of 1\n        read=1\n        read_vector=np.zeros(input_size)\n        #duplicate of strenth vector to modify it at time of read operation\n        temp_strength=copy.deepcopy(strength)\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                                           5/74/5/24, 8:27 PM                                            Neural Stacks-An Explaination\n       #traversing through strength vector from top\n       for var  in  reversed(range(time+1)):\n           if  temp_strength[var]<     read:\n           else:read-=temp_strength[var]\n                temp_strength[var]=read\n                unwanted=set(temp_strength.keys())-set(range(var,time+1))\n                for  keys  in  unwanted:\n                breaktemp_strength[keys]=0\n       for var  in  Value.keys():\n           read_vector+=(temp_strength[var]*Value[var])\n       return  read_vector\n Checking our implementation: Below code is consistent with our read figure.Four vectors are pushed into our value matrix with the\n help of pushPop function.", "start_char_idx": 8758, "end_char_idx": 12839, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "24a424a7-ca76-4902-8da2-6b2a50566e8b": {"__data__": {"id_": "24a424a7-ca76-4902-8da2-6b2a50566e8b", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "3791cada2500d6d61c51e1c59b0e34d4e4091e5908c2de2f9042eb6277e45865", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "847abd84-6ad2-4849-8a9f-0199ad305257", "node_type": "1", "metadata": {}, "hash": "88da0c5e38a35eb428344613d33c311da3f1c9271923b00bb8c4c109ef2b092a", "class_name": "RelatedNodeInfo"}}, "text": "import  numpy  as  np\n  import  copy\n  Value={}\n  strength={}\n  input_size=4\n  value_1=np.zeros(input_size)\n  value_1[0]=1            #[1 0 0 0]\n  value_2=np.zeros(input_size)\n  value_2[1]=1            #[0 1 0 0]\n  value_3=np.zeros(input_size)\n  value_3[2]=1            #[0 0 1 0]\n  value_4=np.zeros(input_size)\n  value_4[3]=1            #[0 0 0 1]\n  def  read_time(time):\n       #returns read vector at time 'time'\n       #initial read value of 1\n       read=1\n       read_vector=np.zeros(input_size)\n       #duplicate of strenth vector to modify it at time of read operation\n       temp_strength=copy.", "start_char_idx": 0, "end_char_idx": 603, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "847abd84-6ad2-4849-8a9f-0199ad305257": {"__data__": {"id_": "847abd84-6ad2-4849-8a9f-0199ad305257", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "3791cada2500d6d61c51e1c59b0e34d4e4091e5908c2de2f9042eb6277e45865", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24a424a7-ca76-4902-8da2-6b2a50566e8b", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "4263b1fb7de34bed6486a97f9f999cb721d490526f00d75aa61600810fe0c09b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b7716d7-6842-43e9-89e2-6d96eb028925", "node_type": "1", "metadata": {}, "hash": "98c361e49123b56d5b869f6414cd0d19325b89f22276752160b756c6628a34ac", "class_name": "RelatedNodeInfo"}}, "text": "deepcopy(strength)\n       #traversing through strength vector from top\n       for var  in  reversed(range(time+1)):\n           if  temp_strength[var]<     read:\n           else:read-=temp_strength[var]\n                temp_strength[var]=read\n                unwanted=set(temp_strength.keys())-set(range(var,time+1))\n                for  keys  in  unwanted:\n                breaktemp_strength[keys]=0\n       for var  in  Value.keys():\n           read_vector+=(temp_strength[var]*Value[var])\n       return  read_vector\n  def  strength_time(time,push_certainity,pop_certainity):\n       for var  in  reversed(range(time)):\n           if  strength[var]    < pop_certainity:\n                pop_certainity-=strength[var]\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                        6/74/5/24,", "start_char_idx": 603, "end_char_idx": 1448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "9b7716d7-6842-43e9-89e2-6d96eb028925": {"__data__": {"id_": "9b7716d7-6842-43e9-89e2-6d96eb028925", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "3791cada2500d6d61c51e1c59b0e34d4e4091e5908c2de2f9042eb6277e45865", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "847abd84-6ad2-4849-8a9f-0199ad305257", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "276a073fc4fc2aa4762e7f958bee139bd61cd1f512c466a1fb744dd42e55e607", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50f06785-161c-49a5-be81-51181932982d", "node_type": "1", "metadata": {}, "hash": "b0d0d68b897bbac5a5035585d99163f7fd48c1460eb81f5c104500154f2f83e0", "class_name": "RelatedNodeInfo"}}, "text": "8:27 PM strength[var]    =  0                       Neural Stacks-An Explaination\n            else:\n                strength[var]-=pop_certainity\n                pop_certainity     = 0\n                break\n       strength[time]    = push_certainity\n       print(strength)\n  def  pushPop(push_value,push_certainity,pop_certainity,time):\n       strength_time(time,push_certainity,pop_certainity)\n       Value[time]=push_value\n       return  read_time(time)\n  pushPop(value_1,0.5,0,0)\n  pushPop(value_2,0.6,0,1)\n  pushPop(value_3,0.3,0,2)\n  print(pushPop(value_4,0.4,0,3))        #prints [0 .3 .3 .4]\n Above program outputs [0 .3 .3 .4] which is consistent with 0.4*v4+0.3*v3+0.3*v2+0*v1.", "start_char_idx": 1449, "end_char_idx": 2135, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "50f06785-161c-49a5-be81-51181932982d": {"__data__": {"id_": "50f06785-161c-49a5-be81-51181932982d", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "3791cada2500d6d61c51e1c59b0e34d4e4091e5908c2de2f9042eb6277e45865", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b7716d7-6842-43e9-89e2-6d96eb028925", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "cb7586666e179512bdddabac1fa3afdb45353b088185ec1cae083b1c4f4faebc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1376b5aa-097b-48a6-84d9-c2c18915394e", "node_type": "1", "metadata": {}, "hash": "8f48f418e20a2708a07a3b24277151330e556b664f991d01ef3fb0d994fb9cac", "class_name": "RelatedNodeInfo"}}, "text": "In the next post we will look to implement backpropagation through our neural stack.\n  ALSO ON JASDEEP06                               Posted on 22 January,", "start_char_idx": 2137, "end_char_idx": 2293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1376b5aa-097b-48a6-84d9-c2c18915394e": {"__data__": {"id_": "1376b5aa-097b-48a6-84d9-c2c18915394e", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "3791cada2500d6d61c51e1c59b0e34d4e4091e5908c2de2f9042eb6277e45865", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50f06785-161c-49a5-be81-51181932982d", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "dbaaf1450a9f892391c85298ff5f4fe5c8247bf51a158286c78b39c8ad304ef3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "993c3b2a-1f8f-4c50-8b0a-4a4854c4d203", "node_type": "1", "metadata": {}, "hash": "8071c095bcf31fae7d655fe9eaa813e1121138e3f10c68f6cd49455f0915e6d1", "class_name": "RelatedNodeInfo"}}, "text": "2017\n   Varlable-sharing-in-   Understanding LSTMin   Lets-Practice          Getting st\n    Variable-sharing-in-   Understanding LSTM in  Lets-Practice-         Getting sta\n    Variable-sharing-in-   Understanding LSTM in  Lets-Practice-         Getting sta\n   Tensorflov             ensorflov              Backpropagation        Tensorflov\n    Tensor\u25afow              Tensor\u25afow              Backpropagation        Tensor\u25afow\n    Tensor\u25afow              Tensor\u25afow              Backpropagation        Tensor\u25afow\n    7 years\u2022ago14 comments 7 years\u2022ago32 comments 7 years\u2022ago4 comments  7 years\u2022ago\n    Tensorflow: Variable sharingCNNs in Tensorflow(cifar-Lets-practice-  Tensorflow :\n    in Tensorflow          10)                    backpropagation        with Tensorfl\n 0 Comments                                               \ue603  Jasdeep Singh Chhabra\n           Start the discussion\u2026\n   \uf109     Share                                                   Best Newest Oldest\n                                 Be thefirst to comment.", "start_char_idx": 2293, "end_char_idx": 3314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "993c3b2a-1f8f-4c50-8b0a-4a4854c4d203": {"__data__": {"id_": "993c3b2a-1f8f-4c50-8b0a-4a4854c4d203", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "3791cada2500d6d61c51e1c59b0e34d4e4091e5908c2de2f9042eb6277e45865", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1376b5aa-097b-48a6-84d9-c2c18915394e", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "d4ac8512a7d9ada5b1addfa037fe743dedc809267721a2e67bca58156a58e013", "class_name": "RelatedNodeInfo"}}, "text": "Subscribe   Privacy    Do Not Sell My Data\n Neural Stacks maintained byjasdeep06\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                           7/7", "start_char_idx": 3319, "end_char_idx": 3494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f0b0c653-5a65-4518-b17d-337795635a9e": {"__data__": {"id_": "f0b0c653-5a65-4518-b17d-337795635a9e", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "3791cada2500d6d61c51e1c59b0e34d4e4091e5908c2de2f9042eb6277e45865", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1d1db5b-fa3f-4b1d-a548-2d50216d1c10", "node_type": "1", "metadata": {}, "hash": "b3579b8639eb4147609c8135e1aed9f0b87b9da8c965749833c413f2825cf8de", "class_name": "RelatedNodeInfo"}}, "text": "import  numpy  as  np\n  import  copy\n  Value={}\n  strength={}\n  input_size=4\n  value_1=np.zeros(input_size)\n  value_1[0]=1            #[1 0 0 0]\n  value_2=np.zeros(input_size)\n  value_2[1]=1            #[0 1 0 0]\n  value_3=np.zeros(input_size)\n  value_3[2]=1            #[0 0 1 0]\n  value_4=np.zeros(input_size)\n  value_4[3]=1            #[0 0 0 1]\n  def  read_time(time):\n       #returns read vector at time 'time'\n       #initial read value of 1\n       read=1\n       read_vector=np.zeros(input_size)\n       #duplicate of strenth vector to modify it at time of read operation\n       temp_strength=copy.deepcopy(strength)\n       #traversing through strength vector from top\n       for var  in  reversed(range(time+1)):\n           if  temp_strength[var]<     read:\n           else:read-=temp_strength[var]\n                temp_strength[var]=read\n                unwanted=set(temp_strength.keys())-set(range(var,time+1))\n                for  keys  in  unwanted:\n                breaktemp_strength[keys]=0\n       for var  in  Value.keys():\n           read_vector+=(temp_strength[var]*Value[var])\n       return  read_vector\n  def  strength_time(time,push_certainity,pop_certainity):\n       for var  in  reversed(range(time)):\n           if  strength[var]    < pop_certainity:\n                pop_certainity-=strength[var]\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                        6/74/5/24, 8:27 PM strength[var]    =  0                       Neural Stacks-An Explaination\n            else:\n                strength[var]-=pop_certainity\n                pop_certainity     = 0\n                break\n       strength[time]    = push_certainity\n       print(strength)\n  def  pushPop(push_value,push_certainity,pop_certainity,time):\n       strength_time(time,push_certainity,", "start_char_idx": 0, "end_char_idx": 1828, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d1d1db5b-fa3f-4b1d-a548-2d50216d1c10": {"__data__": {"id_": "d1d1db5b-fa3f-4b1d-a548-2d50216d1c10", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "node-3", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "3791cada2500d6d61c51e1c59b0e34d4e4091e5908c2de2f9042eb6277e45865", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0b0c653-5a65-4518-b17d-337795635a9e", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "dc73074fcdcdd879795ebc45196c88c25b9ae4def1a77f4ecf6de41b0924e7d5", "class_name": "RelatedNodeInfo"}}, "text": "pop_certainity)\n       Value[time]=push_value\n       return  read_time(time)\n  pushPop(value_1,0.5,0,0)\n  pushPop(value_2,0.6,0,1)\n  pushPop(value_3,0.3,0,2)\n  print(pushPop(value_4,0.4,0,3))        #prints [0 .3 .3 .4]\n Above program outputs [0 .3 .3 .4] which is consistent with 0.4*v4+0.3*v3+0.3*v2+0*v1.\n In the next post we will look to implement backpropagation through our neural stack.\n  ALSO ON JASDEEP06                               Posted on 22 January,2017\n   Varlable-sharing-in-   Understanding LSTMin   Lets-Practice          Getting st\n    Variable-sharing-in-   Understanding LSTM in  Lets-Practice-         Getting sta\n    Variable-sharing-in-   Understanding LSTM in  Lets-Practice-         Getting sta\n   Tensorflov             ensorflov              Backpropagation        Tensorflov\n    Tensor\u25afow              Tensor\u25afow              Backpropagation        Tensor\u25afow\n    Tensor\u25afow              Tensor\u25afow              Backpropagation        Tensor\u25afow\n    7 years\u2022ago14 comments 7 years\u2022ago32 comments 7 years\u2022ago4 comments  7 years\u2022ago\n    Tensorflow: Variable sharingCNNs in Tensorflow(cifar-Lets-practice-  Tensorflow :\n    in Tensorflow          10)                    backpropagation        with Tensorfl\n 0 Comments                                               \ue603  Jasdeep Singh Chhabra\n           Start the discussion\u2026\n   \uf109     Share                                                   Best Newest Oldest\n                                 Be thefirst to comment.\n    Subscribe   Privacy    Do Not Sell My Data\n Neural Stacks maintained byjasdeep06\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                           7/7", "start_char_idx": 1828, "end_char_idx": 3494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "node-3": {"__data__": {"id_": "node-3", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3426a652-a9c3-4d57-b4c5-0fdded7e2626", "node_type": "4", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "d34af1791075116b0eac2e790399faa9d6c28ad7e10c3f346cd10e1d0de7ab96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6126bf1-cb26-4557-b727-43ea77dd18ea", "node_type": "1", "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}, "hash": "5fc907c102fb1ea0bfd8842bba82e95e99b672b20fe5747fb728cba5b215ac5b", "class_name": "RelatedNodeInfo"}}, "text": "import  numpy  as  np\n  import  copy\n  Value={}\n  strength={}\n  input_size=4\n  value_1=np.zeros(input_size)\n  value_1[0]=1            #[1 0 0 0]\n  value_2=np.zeros(input_size)\n  value_2[1]=1            #[0 1 0 0]\n  value_3=np.zeros(input_size)\n  value_3[2]=1            #[0 0 1 0]\n  value_4=np.zeros(input_size)\n  value_4[3]=1            #[0 0 0 1]\n  def  read_time(time):\n       #returns read vector at time 'time'\n       #initial read value of 1\n       read=1\n       read_vector=np.zeros(input_size)\n       #duplicate of strenth vector to modify it at time of read operation\n       temp_strength=copy.deepcopy(strength)\n       #traversing through strength vector from top\n       for var  in  reversed(range(time+1)):\n           if  temp_strength[var]<     read:\n           else:read-=temp_strength[var]\n                temp_strength[var]=read\n                unwanted=set(temp_strength.keys())-set(range(var,time+1))\n                for  keys  in  unwanted:\n                breaktemp_strength[keys]=0\n       for var  in  Value.keys():\n           read_vector+=(temp_strength[var]*Value[var])\n       return  read_vector\n  def  strength_time(time,push_certainity,pop_certainity):\n       for var  in  reversed(range(time)):\n           if  strength[var]    < pop_certainity:\n                pop_certainity-=strength[var]\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                                                        6/74/5/24, 8:27 PM strength[var]    =  0                       Neural Stacks-An Explaination\n            else:\n                strength[var]-=pop_certainity\n                pop_certainity     = 0\n                break\n       strength[time]    = push_certainity\n       print(strength)\n  def  pushPop(push_value,push_certainity,pop_certainity,time):\n       strength_time(time,push_certainity,pop_certainity)\n       Value[time]=push_value\n       return  read_time(time)\n  pushPop(value_1,0.5,0,0)\n  pushPop(value_2,0.6,0,1)\n  pushPop(value_3,0.3,0,2)\n  print(pushPop(value_4,0.4,0,3))        #prints [0 .3 .3 .4]\n Above program outputs [0 .3 .3 .4] which is consistent with 0.4*v4+0.3*v3+0.3*v2+0*v1.\n In the next post we will look to implement backpropagation through our neural stack.\n  ALSO ON JASDEEP06                               Posted on 22 January,2017\n   Varlable-sharing-in-   Understanding LSTMin   Lets-Practice          Getting st\n    Variable-sharing-in-   Understanding LSTM in  Lets-Practice-         Getting sta\n    Variable-sharing-in-   Understanding LSTM in  Lets-Practice-         Getting sta\n   Tensorflov             ensorflov              Backpropagation        Tensorflov\n    Tensor\u25afow              Tensor\u25afow              Backpropagation        Tensor\u25afow\n    Tensor\u25afow              Tensor\u25afow              Backpropagation        Tensor\u25afow\n    7 years\u2022ago14 comments 7 years\u2022ago32 comments 7 years\u2022ago4 comments  7 years\u2022ago\n    Tensorflow: Variable sharingCNNs in Tensorflow(cifar-Lets-practice-  Tensorflow :\n    in Tensorflow          10)                    backpropagation        with Tensorfl\n 0 Comments                                               \ue603  Jasdeep Singh Chhabra\n           Start the discussion\u2026\n   \uf109     Share                                                   Best Newest Oldest\n                                 Be thefirst to comment.\n    Subscribe   Privacy    Do Not Sell My Data\n Neural Stacks maintained byjasdeep06\nhttps://jasdeep06.github.io/posts/Neural-Stacks/                                           7/7", "start_char_idx": 12842, "end_char_idx": 16336, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "node-3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "24f9521d-eada-4970-b200-abb7cca73267": {"__data__": {"id_": "24f9521d-eada-4970-b200-abb7cca73267", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Neural Stacks An Explaination", "path": "cache\\blogposts\\Neural Stacks-An Explaination\\parsed\\images\\4743aa52-96aa-4121-a5c5-54c33f694286-img_p1_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being discussed here revolves around a data structure known as a \"Neural Stack,\" which is designed to interface with neural networks, particularly recurrent neural networks (RNNs). The key aspect of this data structure is that it is differentiable, which means that it can be integrated into the learning process of neural networks through backpropagation. This is essential because, for a network to learn, all components must be able to adjust their parameters in response to the error signal propagated back through the network.\n\nThe neural stack is described as having a value matrix that serves as a dynamic repository for vectors. These vectors are pushed onto the stack and each one is associated with a strength value. The strength values are indicative of the certainty or confidence level of the vectors' presence in the stack. A strength value of zero would indicate that the corresponding vector is effectively not present in the value matrix.\n\nThe neural stack operates with continuous push and pop operations, as opposed to the discrete operations found in traditional stacks. This continuity is represented by values in the interval (0,1), which express the degree of certainty with which an operation is performed. This is crucial for the differentiability of the stack, allowing it to be trained alongside the RNN.\n\nThe implementation of the neural stack involves three key mathematical expressions that govern how vectors are added to the value matrix, how their strength values are determined, and how elements are retrieved or removed from the stack. These formulas are integral to the functioning of the neural stack and are", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfc9ad30-eaf9-4682-883c-7b2710de542f": {"__data__": {"id_": "bfc9ad30-eaf9-4682-883c-7b2710de542f", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Neural Stacks An Explaination", "path": "cache\\blogposts\\Neural Stacks-An Explaination\\parsed\\images\\4743aa52-96aa-4121-a5c5-54c33f694286-img_p2_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you're referring to seems to be explaining the concept of neural stacks, which are data structures used in neural networks to handle sequential data. The text describes the operations of pushing and popping vectors from a value matrix, which is a collection of vectors representing data at different time steps. The value matrix is updated by adding new vectors without modifying the existing ones.\n\nThe push operation is detailed with a formula that indicates how the value matrix is updated at each time step by adding a new vector. The index in the value matrix corresponds to the time step, and the matrix accumulates all vectors up to the current time.\n\nThe pop operation is explained through a second formula that pertains to the strength vector. This vector keeps track of the 'strength' or relevance of each entry in the value matrix. The pop operation involves traversing the strength vector and adjusting its values based on the pop signal, which is denoted by \u03b4t, and the push signal, denoted by ut. These signals control the addition and removal of strength from the vector entries, with the push signal also affecting the strength vector at the current time step.\n\nThe text suggests that the operations are sequential, with the pop operation occurring before the push operation when both signals are received. The explanation includes a reference to figures for further clarification of the notations and the pop operation, which would visually represent the described processes and help in understanding the concept of neural stacks.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c1562eb-5da1-4b68-a7f5-c46f1bb534e0": {"__data__": {"id_": "7c1562eb-5da1-4b68-a7f5-c46f1bb534e0", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Neural Stacks An Explaination", "path": "cache\\blogposts\\Neural Stacks-An Explaination\\parsed\\images\\4743aa52-96aa-4121-a5c5-54c33f694286-img_p2_2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you provided describes a mathematical model for a data structure known as a \"neural stack,\" which is used in machine learning and neural network architectures. The text explains how the push operation affects a value matrix by adding vectors to it at each time step, with the index representing the entry in the value matrix. It is noted that once a vector is added, it is not modified.\n\nThe second part of the text details the behavior of a strength vector, which is used to manage the push and pop operations within the neural stack. The strength vector is updated based on push and pop signals, denoted by \u03b4t (delta t) for pop and ut (u subscript t) for push, both of which are values between 0 and 1. The update rules for the strength vector are given by a formula that involves conditional operations based on the indices and the strength values at the previous time step.\n\nThe text also mentions that the push and pop operations are sequential, with the pop operation occurring before the push when both signals are received. To help readers understand the concepts better, they are directed to refer to figures that illustrate the notations and the pop operation, with a specific URL provided for the first figure. The figure at the given URL would visually represent the mathematical concepts described in the text, likely showing the structure of the neural stack and the effects of the push and pop operations on the value and strength matrices.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "266db264-23f5-4cf4-81c8-701b10ac089f": {"__data__": {"id_": "266db264-23f5-4cf4-81c8-701b10ac089f", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Neural Stacks An Explaination", "path": "cache\\blogposts\\Neural Stacks-An Explaination\\parsed\\images\\4743aa52-96aa-4121-a5c5-54c33f694286-img_p3_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The subject matter appears to be a conceptual representation of a data structure known as \"Neural Stacks.\" The concept is visualized as two sets of elements arranged side by side. On the left, there is a vertical stack with four compartments, each labeled from V1 at the bottom to V4 at the top, suggesting a sequence or hierarchy. On the right, there is a corresponding set of values aligned with each of the compartments from the left stack. These values are 0.5, 0.6, 0.3, and 0.4, respectively, from bottom to top. The values seem to represent a certain attribute or weight associated with each compartment, possibly indicating the strength or certainty of the elements within the stack. The overall layout is reminiscent of a data structure where each element has an associated numerical value, and the arrangement suggests a process of pushing and popping elements with their respective strengths in a last-in, first-out (LIFO) manner. This is further supported by the reference to Python code, which implies that this is a computational model or algorithm being described, likely used in the context of neural networks or machine learning.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4d5ab97-12ee-4062-a497-2e2faa029d44": {"__data__": {"id_": "a4d5ab97-12ee-4062-a497-2e2faa029d44", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Neural Stacks An Explaination", "path": "cache\\blogposts\\Neural Stacks-An Explaination\\parsed\\images\\4743aa52-96aa-4121-a5c5-54c33f694286-img_p3_2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The subject matter appears to be a conceptual representation of a data structure known as \"Neural Stacks.\" The description includes a tabular representation with columns and rows, where each row seems to represent a version (V4, V3, Vz) with associated numerical values (0.4, 0.3, 0.6, 0.5). This table is referred to as \"reference-figure.\"\n\nFollowing this, there is another tabular structure that seems to be a simplified or modified version of the first, with versions stacked vertically and a numerical value next to the lowest two entries (0.5).\n\nAccompanying this is a snippet of Python code that initializes a dictionary named 'strength' and defines a function 'strength_time' which takes parameters 'time,' 'push_certainity,' and 'pop_certainity.' This function likely manipulates the strength values in the context of a neural stack, although the specific operations of the function are not provided in the text.\n\nThe concept of neural stacks is likely related to machine learning or artificial intelligence, where such data structures are used to model complex behaviors or processes. The numerical values might represent the strength or certainty of elements within the stack, which can be modified over time or through certain operations, as suggested by the Python function. The reference to a URL indicates that there might be further explanation or a detailed post on the topic available online.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "673c26a2-8eed-4cb9-bdf0-71ec6ee28e3e": {"__data__": {"id_": "673c26a2-8eed-4cb9-bdf0-71ec6ee28e3e", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Neural Stacks An Explaination", "path": "cache\\blogposts\\Neural Stacks-An Explaination\\parsed\\images\\4743aa52-96aa-4121-a5c5-54c33f694286-img_p4_1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The content you provided explains a concept related to neural stacks and includes a snippet of Python code that demonstrates how to implement a read operation on a neural stack. The read operation is described mathematically and then translated into code. The mathematical formula provided is used to calculate the read vector, which involves a process of traversing a strength vector, modifying its values based on a certain read quantity, and then using these modified strength values to generate the read vector by multiplying them with corresponding vectors in a value matrix.\n\nThe table included in the explanation seems to represent a value matrix with columns and rows, where each cell contains a scalar value. These values are likely used in conjunction with the strength vector to compute the read vector as described in the text. The table is a visual aid to help understand how the values are organized and how they interact during the read operation.\n\nThe source link at the end indicates that this explanation and the code snippet are taken from a specific webpage that discusses neural stacks in more detail.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e7739bc-a48e-4dff-bb66-a93e45ca5869": {"__data__": {"id_": "5e7739bc-a48e-4dff-bb66-a93e45ca5869", "embedding": null, "metadata": {"author": "Jasdeep Singh Chhabra", "blog_title": "Neural Stacks An Explaination", "path": "cache\\blogposts\\Neural Stacks-An Explaination\\parsed\\images\\4743aa52-96aa-4121-a5c5-54c33f694286-img_p4_2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The concept being explained here is a data structure known as a \"neural stack,\" which is used in the context of neural networks and machine learning. The text describes a series of operations that can be performed on this stack, including pushing, popping, and reading elements based on certain strength values.\n\nThe operations are described using Python code snippets. The first snippet outlines a loop that reverses through a range of time steps, adjusting the strength values based on a variable called `pop_certainity`. The second snippet defines a function `read_time` that returns a read vector at a specified time, using an initial read value and a copy of the strength vector to determine the read vector.\n\nThe third formula mentioned in the text describes the read operation in a more mathematical form, explaining how the read vector is generated by traversing the copy of the strength vector and multiplying the resulting strength values with corresponding vectors in a value matrix.\n\nTo illustrate these operations, a visual representation is provided. It shows a stack with elements labeled V1 through V4, each associated with certain numerical values. These values represent the strength of each element in the stack. The visual aid helps to clarify how the read operation works by showing the relationship between the elements and their respective strengths, and how these strengths are adjusted during the read operation to produce the read vector. The values are organized in a tabular format, with columns indicating different states or steps in the process.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"2296bbf5-f88f-41ef-bb25-74e4ac9c857b": {"doc_hash": "825e96b6547b1c5a9e4f68308fd84b7b2eb6aee4d25e841a1d0168b3648b0d7e", "ref_doc_id": "node-0"}, "7a897be8-d3d3-4214-ac7a-3a17454687dc": {"doc_hash": "30a09dcc76addc3de9c2e763b3ee86ecb9694e04b538e9a90988a9cfc3fe19c3", "ref_doc_id": "node-0"}, "978a7dbc-4966-4261-a6d6-a8ed7a8811f4": {"doc_hash": "2aa9ab6722b3eebe87fa2ba2636c7e774deb8832977977767182458ee7a07eac", "ref_doc_id": "node-0"}, "ef91eb73-c227-4352-9f42-1a51c6eceb3a": {"doc_hash": "2f50194f6eeb6d0e10754c67ac2c8741e60c06b189a2923b89241119af2268cb", "ref_doc_id": "node-0"}, "1220b428-98a1-4e23-b709-a49669611e83": {"doc_hash": "8b6e32bbc1858b00a70aa9379f14990f34732546e853d14df0930f8333263c8e", "ref_doc_id": "node-0"}, "d2814c4e-f579-422a-9516-62122f2721b4": {"doc_hash": "8b580bc060a5760cc107cad4b985c9713624c5d06eb2ca31c881e723ab1cec93", "ref_doc_id": "node-0"}, "node-0": {"doc_hash": "86c921b374ef22d0e0c0824c5aa1a11872e09dba34cd35555562304b2c10bc2b", "ref_doc_id": "3426a652-a9c3-4d57-b4c5-0fdded7e2626"}, "0901ab43-9e94-408e-9684-754121d833d3": {"doc_hash": "e7eeccd9e200df1f56b8352a604f1328eb2dd097e7442ba3e97c1f00d3e81406", "ref_doc_id": "node-1"}, "1ec0f783-b0b9-480d-a68f-514a9df1cfdf": {"doc_hash": "9bbcd7b1211250c34a9003ddae0bdb92552c0ce7f8c991eb9ae0a3743c9ce346", "ref_doc_id": "node-1"}, "c3292ec7-b59c-4e85-8039-ff925739ec6f": {"doc_hash": "25a6185df88203b1943eb2a579b8d8edcfdf0f7ca5df4c15872868ff4756bb7d", "ref_doc_id": "node-1"}, "2169e9ec-3406-4a7a-85f9-ff20a71eeb5a": {"doc_hash": "112af6c8ca98fd55d816b43f917dcac7f4b8edcd69f5bf96fc710576d65695d3", "ref_doc_id": "node-1"}, "48bdcafc-3b54-48e8-8ef7-63e2330de903": {"doc_hash": "a0a7332d39e37c466bc34fda5b230d18786cf4d107b837cb9a59bfb3a1a5ea05", "ref_doc_id": "node-1"}, "0178f562-2ba6-4989-8399-161766d95c84": {"doc_hash": "8c6cd287a94a2a488d11822a56ab7ce38d8a051b635784073a1ca9bcca6cfc8c", "ref_doc_id": "node-1"}, "d961e957-a136-486d-9e3c-c9a5accc3660": {"doc_hash": "32e32b239d9436c5af2ea699fb2f52adcfe7d9f9696e7952ee47ec352ad6196a", "ref_doc_id": "node-1"}, "229a0077-6117-455f-b064-061466e010dc": {"doc_hash": "9fb59028f000fcaa0eb95e32d50bafb636acf0ae3a7bc133add0a73eb252fddc", "ref_doc_id": "node-1"}, "node-1": {"doc_hash": "12cc2f5fecb0d01a6e26ebaa8943bd93983c18e6f20a61c946cf5939a0305bcf", "ref_doc_id": "3426a652-a9c3-4d57-b4c5-0fdded7e2626"}, "bde88eb3-8d76-4fc3-a5e5-dc8f4b0c94e8": {"doc_hash": "084874a5019e24ca7ac5cb22852053d5b54c51f3c77b9506297cf10a6912e2c7", "ref_doc_id": "node-2"}, "78da8037-38dc-4af5-870a-fb4a2bea4e02": {"doc_hash": "2db3f778c8a1888229b131737174c5f455b8a1757dd8aa8e89b01cb6adde330e", "ref_doc_id": "node-2"}, "4858240c-d645-454d-829f-1b67f52d632a": {"doc_hash": "77c84cdbb6edcb5e3b03c09550a3feabf469aaa6d9556ff42ee0a87168baf15e", "ref_doc_id": "node-2"}, "4de0f240-e9ff-4ec3-8db6-f9aae51e35ff": {"doc_hash": "5bcbb3aefa5319fb693b38b1db6a85d42055a1733f1244e5b4ba5ec992f76ec6", "ref_doc_id": "node-2"}, "b107496d-b6df-4dde-91c1-112e830cdac0": {"doc_hash": "c09348123731c7ea82809999fffb40bcae8e1e39c4ea4ee35706760e27a199f2", "ref_doc_id": "node-2"}, "bc617e32-b748-4caa-9819-fe7a4222781f": {"doc_hash": "bf2cbb21985f0dc46a2b3e788b498d4d3a603751f3e69e1e488699cbafaedba3", "ref_doc_id": "node-2"}, "node-2": {"doc_hash": "5fc907c102fb1ea0bfd8842bba82e95e99b672b20fe5747fb728cba5b215ac5b", "ref_doc_id": "3426a652-a9c3-4d57-b4c5-0fdded7e2626"}, "24a424a7-ca76-4902-8da2-6b2a50566e8b": {"doc_hash": "4263b1fb7de34bed6486a97f9f999cb721d490526f00d75aa61600810fe0c09b", "ref_doc_id": "node-3"}, "847abd84-6ad2-4849-8a9f-0199ad305257": {"doc_hash": "276a073fc4fc2aa4762e7f958bee139bd61cd1f512c466a1fb744dd42e55e607", "ref_doc_id": "node-3"}, "9b7716d7-6842-43e9-89e2-6d96eb028925": {"doc_hash": "cb7586666e179512bdddabac1fa3afdb45353b088185ec1cae083b1c4f4faebc", "ref_doc_id": "node-3"}, "50f06785-161c-49a5-be81-51181932982d": {"doc_hash": "dbaaf1450a9f892391c85298ff5f4fe5c8247bf51a158286c78b39c8ad304ef3", "ref_doc_id": "node-3"}, "1376b5aa-097b-48a6-84d9-c2c18915394e": {"doc_hash": "d4ac8512a7d9ada5b1addfa037fe743dedc809267721a2e67bca58156a58e013", "ref_doc_id": "node-3"}, "993c3b2a-1f8f-4c50-8b0a-4a4854c4d203": {"doc_hash": "779abe40f0128f60852054ff58dd87f77853747bb413e577dab9acf7cd773210", "ref_doc_id": "node-3"}, "f0b0c653-5a65-4518-b17d-337795635a9e": {"doc_hash": "dc73074fcdcdd879795ebc45196c88c25b9ae4def1a77f4ecf6de41b0924e7d5", "ref_doc_id": "node-3"}, "d1d1db5b-fa3f-4b1d-a548-2d50216d1c10": {"doc_hash": "45c644323e232c5a026c7b9cf1f6a60ac66e5c4ef325cd829c2252014ab1e14f", "ref_doc_id": "node-3"}, "node-3": {"doc_hash": "3791cada2500d6d61c51e1c59b0e34d4e4091e5908c2de2f9042eb6277e45865", "ref_doc_id": "3426a652-a9c3-4d57-b4c5-0fdded7e2626"}, "24f9521d-eada-4970-b200-abb7cca73267": {"doc_hash": "9ad2a20b200bd1698995a79838d06d14630ff8c4390310aea494c7ed5a67478c"}, "bfc9ad30-eaf9-4682-883c-7b2710de542f": {"doc_hash": "84786a015afdd60aac31fd3208c397a9a0c66f06143316a83408562ad23e4072"}, "7c1562eb-5da1-4b68-a7f5-c46f1bb534e0": {"doc_hash": "53cd05499bf6280f9ddf38cbc54a76ab033caf4ad3c415a54f11a5d4009556c3"}, "266db264-23f5-4cf4-81c8-701b10ac089f": {"doc_hash": "ad5d23c6f9585b0da749e5a4f2233fa6d2ec91f72940366907b4f7eb436ac089"}, "a4d5ab97-12ee-4062-a497-2e2faa029d44": {"doc_hash": "b8235986854b8055d67224cc1ce74d141b03c46851915e04f59bb9ffecad154c"}, "673c26a2-8eed-4cb9-bdf0-71ec6ee28e3e": {"doc_hash": "7229eabc4f1f13659fa276a8c756985c8b409f2da86ae61031705eb57ac0471d"}, "5e7739bc-a48e-4dff-bb66-a93e45ca5869": {"doc_hash": "39672041d4e9a0210cf78387a643a231e69742a5f86f8269dc8be3bde00a2433"}}, "docstore/ref_doc_info": {"node-0": {"node_ids": ["2296bbf5-f88f-41ef-bb25-74e4ac9c857b", "7a897be8-d3d3-4214-ac7a-3a17454687dc", "978a7dbc-4966-4261-a6d6-a8ed7a8811f4", "ef91eb73-c227-4352-9f42-1a51c6eceb3a", "1220b428-98a1-4e23-b709-a49669611e83", "d2814c4e-f579-422a-9516-62122f2721b4"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}}, "3426a652-a9c3-4d57-b4c5-0fdded7e2626": {"node_ids": ["node-0", "node-1", "node-2", "node-3"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}}, "node-1": {"node_ids": ["0901ab43-9e94-408e-9684-754121d833d3", "1ec0f783-b0b9-480d-a68f-514a9df1cfdf", "c3292ec7-b59c-4e85-8039-ff925739ec6f", "2169e9ec-3406-4a7a-85f9-ff20a71eeb5a", "48bdcafc-3b54-48e8-8ef7-63e2330de903", "0178f562-2ba6-4989-8399-161766d95c84", "d961e957-a136-486d-9e3c-c9a5accc3660", "229a0077-6117-455f-b064-061466e010dc"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}}, "node-2": {"node_ids": ["bde88eb3-8d76-4fc3-a5e5-dc8f4b0c94e8", "78da8037-38dc-4af5-870a-fb4a2bea4e02", "4858240c-d645-454d-829f-1b67f52d632a", "4de0f240-e9ff-4ec3-8db6-f9aae51e35ff", "b107496d-b6df-4dde-91c1-112e830cdac0", "bc617e32-b748-4caa-9819-fe7a4222781f"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}}, "node-3": {"node_ids": ["24a424a7-ca76-4902-8da2-6b2a50566e8b", "847abd84-6ad2-4849-8a9f-0199ad305257", "9b7716d7-6842-43e9-89e2-6d96eb028925", "50f06785-161c-49a5-be81-51181932982d", "1376b5aa-097b-48a6-84d9-c2c18915394e", "993c3b2a-1f8f-4c50-8b0a-4a4854c4d203", "f0b0c653-5a65-4518-b17d-337795635a9e", "d1d1db5b-fa3f-4b1d-a548-2d50216d1c10"], "metadata": {"author": "Jasdeep Singh Chhabra", "title": "Neural Stacks An Explaination"}}}}